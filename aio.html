<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Probabilistic Programming in R: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Probabilistic Programming in R
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Probabilistic Programming in R
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Probabilistic Programming in R
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="bayesian-statistics.html">1. Short introduction to Bayesian statistics</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="stan.html">2. Stan</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="mcmc.html">3. MCMC</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="hierarchical-models.html">4. Hierarchical Models</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="model-critisism.html">5. Model checking</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="gaussian-processes.html">6. Gaussian processes</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="other-topics.html">7. Other topics</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="exercises.html">8. Exercises</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-bayesian-statistics"><p>Content from <a href="bayesian-statistics.html">Short introduction to Bayesian statistics</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/bayesian-statistics.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How are statistical models formulated and fitted within the Bayesian
framework?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Learn to: - Formulate prior, likelihood, posterior distributions -
fit a Bayesian model with the grid approximation - Communicate posterior
information - Work with with posterior samples</p>
</div>
</div>
</div>
</div>
</div>
<section id="bayes-formula"><h2 class="section-heading">Bayes’ formula<a class="anchor" aria-label="anchor" href="#bayes-formula"></a>
</h2>
<hr class="half-width">
<p>The starting point of Bayesian statistics and probabilistic thinking
is Bayes’ theorem, expressed as:</p>
<p><span class="math display">\[
  p(\theta | X) = \frac{p(\theta)  p(X | \theta)}{p(X)} \\
\]</span></p>
<p>When dealing with a statistical model, this theorem is employed to
infer probabilities of the values of the model parameters <span class="math inline">\((\theta)\)</span> given the available data <span class="math inline">\((X)\)</span>. These probabilities are quantified
by the <em>posterior distribution</em> <span class="math inline">\(p(\theta | X)\)</span>, which serves as the focal
point of probabilistic modeling. The <em>prior distribution</em> <span class="math inline">\(p(\theta)\)</span> is utilized to incorporate
beliefs about <span class="math inline">\(\theta\)</span> before
considering the data. The <em>likelihood function</em> <span class="math inline">\(p(X | \theta)\)</span> provides the probability of
the data given <span class="math inline">\(\theta\)</span> and
determines the impact of the data on the posterior. The denominator on
the right-hand side <span class="math inline">\(p(X)\)</span> is called
the marginal probability, often practically impossible to compute.
Typically, the proportional version of Bayes’ formula is employed:</p>
<p><span class="math display">\[
p(\theta | X) \propto p(\theta)  p(X | \theta).
\]</span></p>
<p>The proportional Bayes’ formula yields an unnormalized posterior
distribution, which can subsequently be normalized to obtain the
posterior.</p>
<div class="section level3">
<h3 id="example-handedness">Example: handedness<a class="anchor" aria-label="anchor" href="#example-handedness"></a>
</h3>
<p>Let’s illustrate the use of the Bayes’ theorem with an example.</p>
<p>Assume we are trying to estimate the prevalence of left-handedness in
humans, based on a sample of <span class="math inline">\(N=50\)</span>
students, out of which <span class="math inline">\(x=7\)</span> are
left-handed and 43 right-handed.</p>
<p>The outcome is binary and the students are assumed to be independent
(e.g. no twins), so the binomial distribution is the appropriate choice
for likelihood:</p>
<p><span class="math display">\[
p(X|\theta) = Bin(7 | 50, \theta).
\]</span></p>
<p>Without further justification, we’ll choose <span class="math inline">\(p(\theta) = Beta(\theta |1, 10)\)</span> as the
prior distribution, so the unnormalized posterior distribution is</p>
<p><span class="math display">\[
p(\theta | X) = Bin(7 | 50, \theta) \cdot Beta(\theta | 1, 10).
\]</span></p>
<p>Below, we’ll plot these functions. Likelihood has been normalized for
better illustration.</p>
<figure><img src="fig/bayesian-statistics-rendered-unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The figure reveals that the majority of the mass in the posterior
distribution is concentrated between 0 and 0.25, with a peak at
approximately 0.1. This suggests that, given the provided data and prior
information, the model is confident that the value of <span class="math inline">\(\theta\)</span>, representing left-handedness in
humans, is close to 0.1. This aligns well with intuitive expectations
about the matter.</p>

</div>
</section><section id="communicating-posterior-information"><h2 class="section-heading">Communicating posterior information<a class="anchor" aria-label="anchor" href="#communicating-posterior-information"></a>
</h2>
<hr class="half-width">
<p>The posterior distribution <span class="math inline">\(p(\theta |
X)\)</span> contains all the information about <span class="math inline">\(\theta\)</span> given the data, chosen model, and
the prior distribution. However, understanding a distribution in its
entirety can be challenging, especially if it has an even moderately
exotic analytic form or if it is multidimensional. Additionally, in
probabilistic modeling, it’s common for the analytic form to be
computationally intractable or impossible to solve.</p>
<p>To effectively communicate posterior information, we require methods
to quantify the information contained in the posterior. Two commonly
used types of estimates are point estimates, such as the posterior mean,
mode, and variance, and posterior intervals, which provide probabilities
for ranges of values.</p>
<p>Two specific types of posterior intervals are of interest in this
course:</p>
<ol style="list-style-type: decimal">
<li><p><em>Credible intervals</em> (CIs): These intervals leave equal
posterior mass below and above them, computed as posterior quantiles.
For instance, a 90% CI would span between the 5% and 95%
quantiles.</p></li>
<li><p><em>Defined boundary intervals</em>: Computed as the posterior
mass for specific parts of the parameter space, these intervals quantify
the probability for given parameter conditions. For example, we might be
interested in the posterior probability that <span class="math inline">\(\theta &gt; 0\)</span>, <span class="math inline">\(0&lt;\theta&lt;0.5\)</span>, or <span class="math inline">\(\theta&lt;0\)</span> or <span class="math inline">\(\theta &gt; 0.5\)</span>. These probabilities can
be computed by integrating the posterior over the corresponding
sets.</p></li>
</ol>
<p>The following figures illustrate selected posterior intervals.</p>
<figure><img src="fig/bayesian-statistics-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="grid-approximation"><h2 class="section-heading">Grid approximation<a class="anchor" aria-label="anchor" href="#grid-approximation"></a>
</h2>
<hr class="half-width">
<p>Specifying a probabilistic model can be simple, but a common
bottleneck in Bayesian data analysis is model fitting. Later in the
course, we will begin using Stan, a state-of-the-art method for
approximating the posterior. However, for now, we’ll use the grid
approximation. This approach involves computing the unnormalized
posterior distribution at a grid of evenly spaced values in the
parameter space and can be specified as follows:</p>
<ol style="list-style-type: decimal">
<li>Define a grid of parameter values.</li>
<li>Compute the prior and likelihood on the grid.</li>
<li>Multiply to get the unnormalized posterior.</li>
<li>Normalize.</li>
</ol>
<p>Now, we’ll implement the grid approximation for the handedness
example in R.</p>
<div class="section level3">
<h3 id="example-binomial-model">Example: Binomial model<a class="anchor" aria-label="anchor" href="#example-binomial-model"></a>
</h3>
<p>First, we’ll define the data variables and the grid of parameter
values</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span></span>
<span></span>
<span><span class="co"># 7/50 are left-handed</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">7</span></span>
<span></span>
<span><span class="co"># Define a grid of points in the interval [0, 1], with 0.01 interval</span></span>
<span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.01</span></span>
<span><span class="va">theta_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span></code></pre>
</div>
<p>Computing the values of the likelihood, prior, and unnormalized
posterior is straightforward. These commands leverage vectorization.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu">dbinom</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="va">N</span>, prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></span>
<span><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu">dbeta</span><span class="op">(</span><span class="va">theta_grid</span>, <span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">likelihood</span><span class="op">*</span><span class="va">prior</span></span></code></pre>
</div>
<p>Next, the posterior needs to be normalized. In practice, this means
dividing the values by the area under the unnormalized posterior. The
area is computed with the integral <span class="math display">\[\int_0^1
p(\theta | X)_{\text{unnormalized}}d\theta.\]</span> In the discrete
scenario, this is simply <span class="math display">\[\sum_{\text{grid}}
p(\theta | X)_{\text{unnormalized}} \cdot \delta,\]</span> where <span class="math inline">\(\delta\)</span> is the grid interval.</p>
<p>Notice that the likelihood function is not a distribution in terms of
the parameter <span class="math inline">\(\theta\)</span>, so it doesn’t
sum to one. However, below, we also normalize it for easier comparison
with the prior and posterior.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># normalize </span></span>
<span><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">likelihood</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">likelihood</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span></span></code></pre>
</div>
<p>Finally, we can plot these functions</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make data frame</span></span>
<span><span class="va">df_hand</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_grid</span>, <span class="va">likelihood</span>, <span class="va">prior</span>, <span class="va">posterior</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># wide to long format</span></span>
<span><span class="va">df_l</span> <span class="op">&lt;-</span> <span class="va">df_hand</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"Function"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">theta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="va">df_l</span>, </span>
<span>       <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">Function</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_grafify</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p1</span></span></code></pre>
</div>
<figure><img src="fig/bayesian-statistics-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The points in the figure represent the values of the functions
computed at the grid locations. The lines depict linear interpolations
between these points.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Experiment with different priors and examine their effects on the
posterior. You could try, for example, different Beta distributions, the
normal distribution, or the uniform distribution.</p>
<p>How does the shape of the prior impact the posterior?</p>
<p>Is it fair to say that the posterior is a compromise between data
(likelihood) and the prior?</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="grid-approximation-and-posterior-summaries">Grid approximation and posterior summaries<a class="anchor" aria-label="anchor" href="#grid-approximation-and-posterior-summaries"></a>
</h3>
<p>Next, we’ll learn how to compute point estimates and posterior
intervals based on the approximate posterior obtained with the grid
approximation.</p>
<p>Computing the posterior mean and variance is based on the definition
of these statistics for continuous variables. The mean is defined as
<span class="math display">\[\int \theta \cdot p(\theta | X)
d\theta\]</span> and can be computed using discrete integration: <span class="math display">\[\sum_{\text{grid}} \theta \cdot p(\theta | X)
\cdot \delta\]</span>. Similarly, the variance can be computed based on
the definition <span class="math display">\[\text{var}(\theta) = \int
(\theta - \text{mean}(\theta))^2p(\theta | X)d\theta\]</span>. Posterior
mode is simply the grid value where the posterior is maximized.</p>
<p>In R, these statistics can be computed as follows:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>Estimate <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"Mode"</span>, <span class="st">"Mean"</span>, <span class="st">"Variance"</span><span class="op">)</span>, </span>
<span>           Value <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df_hand</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span>, <span class="st">"theta"</span><span class="op">]</span>,</span>
<span>                     <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span>, </span>
<span>                     <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">^</span><span class="fl">2</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span> <span class="op">-</span></span>
<span>                       <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  Estimate       Value
1     Mode 0.120000000
2     Mean 0.131147540
3 Variance 0.001837869</code></pre>
</div>
<p>Posterior intervals are also relatively easy to compute.</p>
<p>Finding the quantiles used to determine CIs is based on the
cumulative distribution function of the posterior <span class="math inline">\(F(\theta) = \int_{\infty}^{\theta}p(y | X)
dy\)</span>. The locations where the <span class="math inline">\(F(\theta) = 0.05\)</span> and <span class="math inline">\(F(\theta) = 0.95\)</span> define the 90% CIs.</p>
<p>Probabilities for certain parameter ranges are computed simply by
integrating over the appropriate set. For instance, <span class="math inline">\(Pr(\theta &lt; 0.1) = \int_0^{0.1} p(\theta | X)
d\theta\)</span></p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Compute the 90% CIs and the probability <span class="math inline">\(Pr(\theta &lt; 0.1)\)</span> for the handedness
example.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Quantiles</span></span>
<span><span class="va">q5</span> <span class="op">&lt;-</span> <span class="va">theta_grid</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="fu">cumsum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span> <span class="op">&gt;</span> <span class="fl">0.05</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">q95</span> <span class="op">&lt;-</span> <span class="va">theta_grid</span><span class="op">[</span><span class="fu">which.min</span><span class="op">(</span><span class="fu">cumsum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span> <span class="op">&lt;</span> <span class="fl">0.95</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Pr(theta &lt; 0.1)</span></span>
<span><span class="va">Pr_theta_under_0.1</span> <span class="op">&lt;-</span> <span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">[</span><span class="va">theta_grid</span> <span class="op">&lt;</span> <span class="fl">0.1</span><span class="op">]</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="fu">paste0</span><span class="op">(</span><span class="st">"90% CI = ("</span>, <span class="va">q5</span>,<span class="st">","</span>, <span class="va">q95</span>,<span class="st">")"</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "90% CI = (0.07,0.21)"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">print</span><span class="op">(</span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Pr(theta &lt; 0.1) = "</span>, <span class="fu">round</span><span class="op">(</span><span class="va">Pr_theta_under_0.1</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Pr(theta &lt; 0.1) = 0.20659"</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="example-gamma-model">Example: Gamma model<a class="anchor" aria-label="anchor" href="#example-gamma-model"></a>
</h3>
<p>The gamma distribution arises, for example, in applications that
model the waiting time between consecutive events. Let’s assume the
following data points were generated independently from a <span class="math inline">\(\Gamma(\alpha, \beta)\)</span> distribution with
unknown shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0.34</span>, <span class="fl">0.2</span>, <span class="fl">0.22</span>, <span class="fl">0.77</span>, <span class="fl">0.46</span>, <span class="fl">0.73</span>, <span class="fl">0.24</span>, <span class="fl">0.66</span>, <span class="fl">0.64</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> using the grid approximation.</p>
<p>Similarly as before, we’ll define a grid of points for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Now, since our parameter space is
2-dimensional, the grid need to be defined at all pairwise combinations
of the points of the individual grids.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.1</span></span>
<span><span class="va">alpha_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.01</span>, to <span class="op">=</span> <span class="fl">15</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span>
<span><span class="va">beta_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.01</span>, to <span class="op">=</span> <span class="fl">25</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get pairwise combinations</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">expand.grid</span><span class="op">(</span>alpha <span class="op">=</span> <span class="va">alpha_grid</span>, beta <span class="op">=</span> <span class="va">beta_grid</span><span class="op">)</span></span></code></pre>
</div>
<p>Next, we’ll compute the likelihood. As we assumed the data points to
have arisen independently from the gamma distribution, the likelihood is
the product of the likelihoods of individual observations.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Loop over all alpha, beta combinations</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">df</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"likelihood"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">prod</span><span class="op">(</span></span>
<span>    <span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span>,</span>
<span>           shape <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"alpha"</span><span class="op">]</span>,</span>
<span>           rate <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"beta"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Next, we’ll add priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Non-positive values are not
allowed, which should be reflected in the prior. The conjugate prior for
the Gamma likelihood <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Bayesian_inference" class="external-link">exists</a>,
which means there is a prior that causes the posterior to be of the same
shape. However, for simplicity, we’ll use <span class="math inline">\(\Gamma\)</span> priors with large variance.</p>
<p>Notice, that normalizing the posterior now requires integrating over
both dimensions, hence the <span class="math inline">\(\delta^2\)</span>
below.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Priors: alpha, beta ~ Gamma(2, .1)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>prior <span class="op">=</span> <span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, <span class="fl">2</span>, <span class="fl">0.1</span><span class="op">)</span><span class="op">*</span><span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">beta</span>, <span class="fl">2</span>, <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Posterior</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">prior</span><span class="op">*</span><span class="va">likelihood</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># normalize</span></span>
<span></span>
<span></span>
<span><span class="co"># Plot</span></span>
<span><span class="va">p_joint_posterior</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_tile</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, y <span class="op">=</span> <span class="va">beta</span>, fill <span class="op">=</span> <span class="va">posterior</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">scale_fill_gradientn</span><span class="op">(</span>colours <span class="op">=</span> <span class="fu">rainbow</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span>, y <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_joint_posterior</span></span></code></pre>
</div>
<figure><img src="fig/bayesian-statistics-rendered-unnamed-chunk-13-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Next, we’ll compute the posterior mode, which is a point in the
2-dimensional parameter space.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"alpha"</span>, <span class="st">"beta"</span><span class="op">)</span><span class="op">]</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      alpha beta
14898  4.71 9.91</code></pre>
</div>
<p>Often, in addition to the parameters of interest, the model contains
parameters we are not interested in. For instance, we could only need
information regarding <span class="math inline">\(\alpha\)</span>, in
which case <span class="math inline">\(\beta\)</span> would be a
‘nuisance’ parameter. Nuisance parameters are part of the full (‘joint’)
posterior, but they can be discarded by integrating the joint posterior
over these parameters. A posterior integrated over some parameters is
called a marginal posterior.</p>
<p>Let’s now compute the marginal posterior for <span class="math inline">\(\alpha\)</span> by integrating over <span class="math inline">\(\beta\)</span>. Intuitively, it can be helpful to
think of marginalization as a process where all of the joint posterior
mass is drawn towards the <span class="math inline">\(\alpha\)</span>
axis, as if drawn by a gravitational force.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get marginal posterior for alpha</span></span>
<span><span class="va">alpha_posterior</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>posterior <span class="op">=</span> <span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p_alpha_posterior</span> <span class="op">&lt;-</span> <span class="va">alpha_posterior</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, y <span class="op">=</span> <span class="va">posterior</span><span class="op">)</span>, </span>
<span>            color <span class="op">=</span> <span class="va">posterior_color</span>, </span>
<span>            linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_alpha_posterior</span></span></code></pre>
</div>
<figure><img src="fig/bayesian-statistics-rendered-unnamed-chunk-15-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Does the MAP of the joint posterior of <span class="math inline">\(\theta = (\alpha, \beta)\)</span> correspond to
the MAPs of the marginal posteriors of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>No. Why?</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="working-with-samples"><h2 class="section-heading">Working with samples<a class="anchor" aria-label="anchor" href="#working-with-samples"></a>
</h2>
<hr class="half-width">
<p>The main limitation of the grid approximation is that it becomes
impractical for models with even a moderate number of parameters. The
reason is that the number of computations grows as <span class="math inline">\(O \{ \Delta^p \}\)</span> where <span class="math inline">\(\Delta\)</span> is the number of grid points per
model parameter and <span class="math inline">\(p\)</span> the number of
parameters. This quickly becomes prohibitive, and the grid approximation
is seldom used in practice. The standard approach to fitting Bayesian
models is to draw samples from the posterior with Markov chain Monte
Carlo (MCMC) methods. These methods are the topic of a later episode but
we’ll anticipate this now by studying how posterior summaries can be
computed based on samples.</p>
<p>Let’s take the Beta-binomial model (beta prior, binomial likelihood)
of the handedness analysis as our example. It is an instance of a model
for which the posterior can be computed analytically. Given a prior
<span class="math inline">\(Beta(\alpha, \beta)\)</span> and likelihood
<span class="math inline">\(Bin(x | N, \theta)\)</span>, the posterior
is <span class="math display">\[p(\theta | X) = Beta(\alpha + x, \beta +
N - x).\]</span> Let’s simulate <span class="math inline">\(n =
1000\)</span> samples from this posterior using the handedness data:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">theta_samples</span> <span class="op">&lt;-</span> <span class="fu">rbeta</span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span> <span class="op">+</span> <span class="fl">7</span>, <span class="fl">10</span> <span class="op">+</span> <span class="fl">50</span> <span class="op">-</span> <span class="fl">7</span><span class="op">)</span></span></code></pre>
</div>
<p>Plotting a histogram of these samples against the grid approximation
displays that these are indeed approximating the same distribution
<img src="fig/bayesian-statistics-rendered-unnamed-chunk-17-1.png" style="display: block; margin: auto;" class="figure"></p>
<p>Computing posterior summaries from samples is easy. The posterior
mean and variance are computed simply by taking the mean and variance of
the samples, respectively. Posterior intervals are equally easy to
compute: 90% CI is recovered from the appropriate quantiles and the
probabilities of certain intervals are simply the proportion of samples
in the interval.</p>
<div id="discussion2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion2"></a>
</h3>
<div class="callout-content">
<p>Compute the posterior mean, variance, 90% CI and <span class="math inline">\(Pr(\theta &gt; 0.1)\)</span> using the generated
samples.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Likelihood determines the probability of data conditional on the
model parameters.</li>
<li>Prior encodes beliefs about the model parameters without considering
data.</li>
<li>Posterior quantifies the probability of parameter values conditional
on the data.</li>
<li>The posterior is a compromise between the data and prior. The less
data available, the greater the impact of the prior.</li>
<li>The grid approximation is a method for inferring the (approximate)
posterior distribution.</li>
<li>Posterior information can be summarized with point estimates and
posterior intervals.</li>
<li>The marginal posterior is accessed by integrating over nuisance
parameters.</li>
<li>Usually, Bayesian models are fitted using methods that generate
samples from the posterior.</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Bayesian Data Analysis (3rd ed.): Ch. 1-3</li>
<li>Statistical Rethinking (2nd ed.): Ch. 1-3</li>
<li>Bayes Rules!: Ch. 1-6</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-stan"><p>Content from <a href="stan.html">Stan</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/stan.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is Stan?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Learn how to: - implement statistical models in Stan - generate
posterior samples with Stan - extract and process samples generated with
Stan</p>
</div>
</div>
</div>
</div>
</div>
<p>Stan, a programming language, is as a tool for generating samples
from the posterior distribution. It achieves this by applying a Markov
Chain Monte Carlo (MCMC) algorithm, specifically a variant known as
Hamiltonian Monte Carlo. In the next episode, we will delve into MCMC,
but for now, our focus is on understanding how to execute it using
Stan.</p>
<p>To get started, follow the instructions provided at <a href="https://mc-stan.org/users/interfaces/" class="external-link uri">https://mc-stan.org/users/interfaces/</a> to install Stan on
your local computer.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>With Stan, you can fit model that have continuous parameters. Models
with discrete parameters such as most classification models are
typically impossible to fit, although some workarounds have been
implemented.</p>
</div>
</div>
</div>
<section id="basic-program-structure"><h2 class="section-heading">Basic program structure<a class="anchor" aria-label="anchor" href="#basic-program-structure"></a>
</h2>
<hr class="half-width">
<p>A Stan program is organized into several blocks that collectively
define the model. Typically, a Stan program includes at least the
following blocks:</p>
<ol style="list-style-type: decimal">
<li><p>Data: This block is used to declare the input data provided to
the model. It specifies the types and dimensions of the data variables
incorporated into the model.</p></li>
<li><p>Parameters: In this block, the model parameters are
declared.</p></li>
<li><p>Model: The likelihood and prior distributions are included here
through sampling statements.</p></li>
</ol>
<p>For best practices, it is recommended to specify Stan programs in
separate text files with a .stan extension, which can then be called
from R or other supported interfaces.</p>
<div class="section level3">
<h3 id="example-1-beta-binomial-model">Example 1: Beta-binomial model<a class="anchor" aria-label="anchor" href="#example-1-beta-binomial-model"></a>
</h3>
<p>The following Stan program specifies the Beta-binomial model, and
consists of data, parameters, and model blocks.</p>
<p>The data variables are the total sample size <span class="math inline">\(N\)</span> and the outcome of a binary variable
(coin flip, handedness etc.). The declared data type is <code>int</code>
for integer, and the variables have a lower bound 1 and 0 for <span class="math inline">\(N\)</span> and <span class="math inline">\(x\)</span>, respectively. Notice that each line
ends with a semicolon.</p>
<p>In the parameters block we declare <span class="math inline">\(\theta\)</span>, the probability for a success.
Since this parameter is a probability, it is a real number restricted
between 0 and 1.</p>
<p>In the model block, the likelihood is specified with the sampling
statement <code>x ~ binomial(N, theta)</code>. This line includes the
binomial distribution <span class="math inline">\(Bin(x | N,
theta)\)</span> in the target distribution. The prior is set similarly,
and omitting the prior implies a uniform prior.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>  <span class="kw">data</span>{</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N; </span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; x; </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>  }</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>  <span class="kw">parameters</span>{</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; theta;</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>  }</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>  </span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>  <span class="kw">model</span>{</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    <span class="co">// Likelihood</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    x ~ binomial(N, theta);</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>    <span class="co">// Prior is uniform</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>  }</span></code></pre>
</div>
<p>Once the Stan program has been saved we need to compile it. In R,
this is done by running the following line, where
<code>"binomial_model.stan"</code> is the path of the program.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binomial_model</span> <span class="op">&lt;-</span> <span class="fu">stan_model</span><span class="op">(</span><span class="st">"binomial_model.stan"</span><span class="op">)</span></span></code></pre>
</div>
<p>Once the program has been compiled, it can be used to generate the
posterior samples by calling the function <code>sampling()</code>. The
data needs to be defined as a list.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binom_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">50</span>, x <span class="op">=</span> <span class="fl">7</span><span class="op">)</span></span>
<span></span>
<span><span class="va">binom_samples</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span>object <span class="op">=</span> <span class="va">binomial_model</span>,</span>
<span>                          data <span class="op">=</span> <span class="va">binom_data</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 5e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 1:                0.004 seconds (Sampling)
Chain 1:                0.008 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 2:                0.004 seconds (Sampling)
Chain 2:                0.008 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 3:                0.003 seconds (Sampling)
Chain 3:                0.007 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 1e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 4:                0.004 seconds (Sampling)
Chain 4:                0.008 seconds (Total)
Chain 4: </code></pre>
</div>
<p>With the default settings, Stan executes 4 MCMC chains, each with
2000 iterations (more about this in the next episode on MCMC). During
the run, Stan provides progress information, aiding in estimating the
running time, particularly for complex models or extensive datasets. In
this case the sampling took only a fraction of a second.</p>
<p>When running <code>binom_samples</code>, a summary for the model
parameter <span class="math inline">\(p\)</span> is printed,
facilitating a quick review of the results.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binom_samples</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Inference for Stan model: anon_model.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
theta   0.15    0.00 0.05   0.07   0.12   0.15   0.18   0.27  1546    1
lp__  -22.86    0.02 0.73 -24.96 -23.06 -22.57 -22.38 -22.33  1656    1

Samples were drawn using NUTS(diag_e) at Tue Apr 23 00:32:59 2024.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<p>This summary can also be accessed as a matrix with
<code>summary(binom_samples)$summary</code>.</p>
<p>Often, however, it is necessary to work with the individual samples.
These can be extracted as follows:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta_samples</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">binom_samples</span>, <span class="st">"theta"</span><span class="op">)</span><span class="op">[[</span><span class="st">"theta"</span><span class="op">]</span><span class="op">]</span></span></code></pre>
</div>
<p>Now we can use the methods presented in the previous Episode to
compute posterior summaries, credible intervals and to generate
figures.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Compute the 95% credible intervals for the samples drawn with Stan.
What is the probability that <span class="math inline">\(\theta \in
(0.05, 0.15)\)</span>? Plot a histogram of the posterior samples.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">CI95</span> <span class="op">&lt;-</span> <span class="fu">quantile</span><span class="op">(</span><span class="va">theta_samples</span>, probs <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0.025</span>, <span class="fl">0.975</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">theta_between_0.05_0.15</span> <span class="op">&lt;-</span> <span class="fu">mean</span><span class="op">(</span><span class="va">theta_samples</span><span class="op">&gt;</span><span class="fl">0.05</span> <span class="op">&amp;</span> <span class="va">theta_samples</span><span class="op">&lt;</span><span class="fl">0.15</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_samples</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">30</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/stan-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Try modifying the Stan program so that you add a <span class="math inline">\(Beta(\alpha, \beta)\)</span> prior for <span class="math inline">\(\theta\)</span>.</p>
<p>Can you modify the Stan program further so that you can set the
hyperparameters <span class="math inline">\(\alpha, \beta\)</span> as
part of the data? What is the benefit of using this approach?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>If the data block is modified so that it declares the hyperparameters
as data (e.g. <code>real&lt;lower=0&gt; alpha;</code>), it enables
setting the hyperparameter values as part of data. This makes it
possible to change the hyperparameters without modifying the Stan
file.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="additional-stan-blocks"><h2 class="section-heading">Additional Stan blocks<a class="anchor" aria-label="anchor" href="#additional-stan-blocks"></a>
</h2>
<hr class="half-width">
<p>In addition the data, parameters, and model blocks there are
additional blocks that can be included in the program.</p>
<ol style="list-style-type: decimal">
<li><p>Functions: For user-defined functions. This block must be the
first in the Stan program. It allows users to define custom
functions.</p></li>
<li><p>Transformed data: This block is used for transformations of the
data variables. It is often employed to preprocess or modify the input
data before it is used in the main model. Common tasks include
standardization, scaling, or other data adjustments.</p></li>
<li><p>Transformed parameters: In this block, transformations of the
parameters are defined. If transformed parameters are used on the
left-hand side of sampling statements in the model block, the Jacobian
adjustment for the posterior density needs to be included in the model
block as well.</p></li>
<li><p>Generated quantities: This block is used to define quantities
based on both data and model parameters. These quantities are not part
of the model but are useful for post-processing.</p></li>
</ol>
<p>Examples of usage will be included in subsequent illustrations.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>There are tools like Bayesplot, rstanarm, and brms built on top of
Stan that make it easier to use many common statistical models and tools
for analyzing and visualizing results. However, we won’t use these much
in this course. The idea is that learning Stan from the basics helps you
understand Bayesian modeling better. It lets you have more control over
your models and helps you learn how to build, fix, and improve them. So,
by working directly with Stan and its details, you’ll get better at
making your own models and doing more customized Bayesian analyses later
on. Furthermore, top-down tools can later be adopted with more
confidence as you will understand what is happening under the proverbial
hood.</p>
</div>
</div>
</div>
</section><section id="example-2-normal-model"><h2 class="section-heading">Example 2: normal model<a class="anchor" aria-label="anchor" href="#example-2-normal-model"></a>
</h2>
<hr class="half-width">
<p>Next, let’s implement the normal model in Stan. First generate some
data <span class="math inline">\(X\)</span> with unknown mean and
standard deviation parameters <span class="math inline">\(\mu\)</span>
and <span class="math inline">\(\sigma\)</span></p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">99</span></span>
<span></span>
<span><span class="co"># Generate data with unknown parameters</span></span>
<span><span class="va">unknown_sigma</span> <span class="op">&lt;-</span> <span class="fu">runif</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">unknown_mu</span> <span class="op">&lt;-</span> <span class="fu">runif</span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span>n <span class="op">=</span> <span class="va">N</span>,</span>
<span>           mean <span class="op">=</span> <span class="va">unknown_mu</span>,</span>
<span>           sd <span class="op">=</span> <span class="va">unknown_sigma</span><span class="op">)</span> </span></code></pre>
</div>
<p>The Stan program for the normal model is specified in the next code
chunk. It introduces a new data type (vector) and leverages
vectorization in the likelihood statement. Toward the end of the
program, a generated quantities block is included, generating new data
(X_tilde) to estimate what unseen data points might look like. This
resulting distribution is referred to as the posterior predictive
distribution. The way this works is by generating a random realization
from the normal distribution for each posterior sample of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>}</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>}</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>  <span class="co">// Likelihood is vectorized!</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>  X ~ normal(mu, sigma);</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>  </span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>}</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>  <span class="dt">real</span> X_tilde;</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>  X_tilde = normal_rng(mu, sigma);</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Instead of vectorizing the likelihood, one could write a for loop
with sampling statements for each individual data point, such as
<code>X[i] ~ normal(\mu, \sigma)</code>. However, this approach would
result in an inefficient implementation, and vectorization is generally
recommended for improved performance. Nevertheless, when dealing with
complex models, it may be useful to initially write the model in an
unvectorized format to facilitate easier debugging.</p>
<p>Let’s again fit the model to the data</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">normal_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>, </span>
<span>                                  <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 5e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 1:                0.007 seconds (Sampling)
Chain 1:                0.015 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 2e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 2:                0.007 seconds (Sampling)
Chain 2:                0.015 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 2e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 3:                0.007 seconds (Sampling)
Chain 3:                0.015 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 2e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 4:                0.006 seconds (Sampling)
Chain 4:                0.014 seconds (Total)
Chain 4: </code></pre>
</div>
<p>Next, we’ll extract posterior samples and generate a plot for the
joint, and marginal posteriors. The true unknown parameter values are
included in the plots in red.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract parameter samples</span></span>
<span><span class="va">par_samples</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">data.frame</span></span>
<span></span>
<span></span>
<span><span class="co"># Full posterior</span></span>
<span><span class="va">p_posterior</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_samples</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mu</span>, y <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">unknown_mu</span>, y <span class="op">=</span> <span class="va">unknown_sigma</span><span class="op">)</span>,</span>
<span>             color <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Marginal posteriors</span></span>
<span><span class="va">p_marginals</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_samples</span> <span class="op">%&gt;%</span> <span class="va">gather</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">40</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span>key <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span>, </span>
<span>                               value <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">unknown_mu</span>, <span class="va">unknown_sigma</span><span class="op">)</span><span class="op">)</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">value</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">key</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu">cowplot</span><span class="fu">::</span><span class="fu">plot_grid</span><span class="op">(</span><span class="va">p_posterior</span>, <span class="va">p_marginals</span>,</span>
<span>                  ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/stan-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s also plot the posterior predictive distribution:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PPD</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilde"</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>X_tilde <span class="op">=</span> <span class="va">.</span> <span class="op">)</span></span>
<span></span>
<span><span class="va">p_PPD</span> <span class="op">&lt;-</span> <span class="va">PPD</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_tilde</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">40</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_PPD</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/stan-rendered-unnamed-chunk-12-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="example-3-linear-regression"><h2 class="section-heading">Example 3: Linear regression<a class="anchor" aria-label="anchor" href="#example-3-linear-regression"></a>
</h2>
<hr class="half-width">
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge3"></a>
</h3>
<div class="callout-content">
<p>Write a Stan program for linear regression with one dependent
variable.</p>
<p>Generate data from the linear model and use the Stan program to
estimate the intercept <span class="math inline">\(\alpha\)</span>,
slope <span class="math inline">\(\beta\)</span> and noise term <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N; <span class="co">// Sample size</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] x; <span class="co">// x-values</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>  <span class="dt">vector</span>[N] y; <span class="co">// y-values</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>}</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>  <span class="dt">real</span> alpha; <span class="co">// intercept</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>  <span class="dt">real</span> beta;  <span class="co">// slope</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma; <span class="co">// noise</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>}</span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a></span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a>  </span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a>  y ~ normal(alpha + beta * x, sigma);</span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a>  </span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb15-18"><a href="#cb15-18" tabindex="-1"></a>  alpha ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb15-19"><a href="#cb15-19" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb15-20"><a href="#cb15-20" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb15-21"><a href="#cb15-21" tabindex="-1"></a>}</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge4"></a>
</h3>
<div class="callout-content">
<p>Modify the program for linear regression so it facilitates <span class="math inline">\(M\)</span> dependent variables.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N; <span class="co">// Sample size</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; M; <span class="co">// Number of features</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>  <span class="dt">matrix</span>[N, M] x; <span class="co">// x-values</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>  <span class="dt">vector</span>[N] y; <span class="co">// y-values</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>}</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>  <span class="dt">real</span> alpha; <span class="co">// intercept</span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>  <span class="dt">vector</span>[M] beta;  <span class="co">// slopes</span></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma; <span class="co">// noise</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>}</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>  </span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a>  y ~ normal(alpha + x * beta, sigma);</span>
<span id="cb16-18"><a href="#cb16-18" tabindex="-1"></a>  </span>
<span id="cb16-19"><a href="#cb16-19" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb16-20"><a href="#cb16-20" tabindex="-1"></a>  alpha ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb16-21"><a href="#cb16-21" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb16-22"><a href="#cb16-22" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb16-23"><a href="#cb16-23" tabindex="-1"></a>}</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Stan is a powerful tool for generating posterior distribution
samples.</li>
<li>A Stan program is specified in a separate text file that consists of
code blocks, with the data, parameters, and model blocks being the most
crucial ones.</li>
</ul>
</div>
</div>
</div>
</section><section id="resources"><h2 class="section-heading">Resources<a class="anchor" aria-label="anchor" href="#resources"></a>
</h2>
<hr class="half-width">
<ul>
<li>Official release paper <a href="https://www.jstatsoft.org/article/view/v076i01" class="external-link uri">https://www.jstatsoft.org/article/view/v076i01</a>
</li>
<li>User’s guide <a href="https://mc-stan.org/docs/2_18/stan-users-guide/" class="external-link uri">https://mc-stan.org/docs/2_18/stan-users-guide/</a>
</li>
<li>Function’s reference <a href="https://mc-stan.org/docs/functions-reference/" class="external-link uri">https://mc-stan.org/docs/functions-reference/</a>
</li>
<li>Reference manual <a href="https://mc-stan.org/docs/reference-manual/" class="external-link uri">https://mc-stan.org/docs/reference-manual/</a>
</li>
<li>Stan forum <a href="https://discourse.mc-stan.org" class="external-link uri">https://discourse.mc-stan.org</a>
</li>
<li>Case studies <a href="https://mc-stan.org/users/documentation/case-studies" class="external-link uri">https://mc-stan.org/users/documentation/case-studies</a>
</li>
</ul></section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>BDA3: Ch. 12.6, Appendix C</li>
<li>Bayes Rules!: Ch. 6.2</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-mcmc"><p>Content from <a href="mcmc.html">MCMC</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/mcmc.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can the posterior distribution be sampled?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn the basic idea of the Metropolis-Hasting algorithm</li>
<li>Know how to assess MCMC convergence</li>
<li>Be able to implement a random walk Metropolis-Hasting algorithm</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the general case, computing the posterior distribution
analytically for a probabilistic model poses an insurmountable
challenge. Moreover, even if the analytical form were available,
marginalizing it could still be difficult. For these reasons,
approximation methods need to be relied upon. In Episode 2, we saw that
drawing conclusions about the inference could be achieved effortlessly
by working with samples drawn from the posterior distribution. The topic
of this episode, Markov chain Monte Carlo (MCMC) methods, is a means for
generating such samples and the most extensively employed solution for
fitting probabilistic models.</p>
<section id="metropolis-hastings-algorithm"><h2 class="section-heading">Metropolis-Hastings algorithm<a class="anchor" aria-label="anchor" href="#metropolis-hastings-algorithm"></a>
</h2>
<hr class="half-width">
<p>MCMC methods draw samples from the posterior distribution by
constructing sequences (chains) of values in the parameter space that
ultimately converge to the posterior. While there are other variants of
MCMC, on this course we will mainly focus on the Metropolis-Hasting (MH)
algorithm outlined below. As this algorithm is ran long enough,
convergence to posterior (or to other specified target density) is
guaranteed and eventually the samples will start approximating the
posterior distribution.</p>
<p>A chain starts at some initial value <span class="math inline">\(\theta^{0}\)</span>, which can be random or based
on some more informed criterion. The only precondition is that <span class="math inline">\(p(\theta^{0} | X) &gt; 0\)</span>. A transition
distribution <span class="math inline">\(T_i\)</span> is used to
generate a proposal for the subsequent value. An often-used solution is
the normal distribution centered at the current value, <span class="math inline">\(\theta^* \sim N(\theta^{i}, \sigma^2)\)</span>.
This is where the term “Markov chain” comes from, each element is
generated based on only the previous one.</p>
<p>Next, the generated proposal <span class="math inline">\(\theta^*\)</span> is either accepted or rejected.
If each proposal was accepted, the sequence would simply be a random
walk in the parameter space and would not approximate the posterior to
any degree. The rule that determines the acceptance should reflect this;
proposals towards higher posterior densities should be favored over
proposals toward low density areas. The solution is to compute the
ratio</p>
<p><span class="math display">\[r = \frac{p(\theta^* | X) / T_i(\theta^*
| \theta^{i})}{p(\theta^i | X) / T_i(\theta^{i} | \theta^{*})},\]</span>
and use is as the probability to move to the proposed value. In other
words, the next element in the chain is <span class="math inline">\(\theta^{i+1} = \theta^*\)</span> with probability
<span class="math inline">\(\max(r, 1)\)</span>, and with probability
<span class="math inline">\(1-r\)</span>, the proposal is rejected and
the chain stays at the current value, <span class="math inline">\(\theta^{i+1} = \theta^{i}.\)</span> This approach
induces directional randomness in the chain; proposals towards higher
density areas are generally accepted but transitions away from it are
also possible.</p>
<p>In situations where the transition density is symmetric, such as with
the normal distribution, <span class="math inline">\(r\)</span> reduces
simply to the ratio of the posterior values, and all proposals toward
higher posterior density areas are accepted.</p>
<div class="section level3">
<h3 id="example-banana-distribution">Example: Banana distribution<a class="anchor" aria-label="anchor" href="#example-banana-distribution"></a>
</h3>
<p>Let’s implement the MH algorithm and use it to generate posterior
samples of the following statistical model:</p>
<p><span class="math display">\[X \sim N(\theta_1 + \theta_2^2, 1) \\
\theta_1, \theta_2 \sim N(0, 1),\]</span></p>
<div class="section level4">
<h4 id="helper-functions">Helper functions<a class="anchor" aria-label="anchor" href="#helper-functions"></a>
</h4>
<p>Let’s begin by writing some helper functions that carry out the
incremental steps of the MH algorithm.</p>
<p>First, we need to be able to generate the proposals. Let’s use the
multivariate (2D) normal with diagonal covariance scaled by a scalar
<code>jump_scale</code>.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">generate_proposal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">pars_now</span>, <span class="va">jump_scale</span> <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Number of parameters</span></span>
<span>  <span class="va">my_n</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">pars_now</span><span class="op">)</span></span>
<span>  <span class="co"># Random draw from multivariate normal</span></span>
<span>  <span class="va">theta_star</span> <span class="op">&lt;-</span> <span class="fu">mvtnorm</span><span class="fu">::</span><span class="fu">rmvnorm</span><span class="op">(</span><span class="fl">1</span>,</span>
<span>                                 mean <span class="op">=</span> <span class="va">pars_now</span>,</span>
<span>                                 sigma <span class="op">=</span> <span class="va">jump_scale</span><span class="op">*</span><span class="fu">diag</span><span class="op">(</span><span class="va">my_n</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">theta_star</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Running MH also requires computing the (unnormalized) posterior
density at the proposed parameter values. This functions returns the log
posterior value at point <code>pars</code>. The density is computed on
log scale to avoid issues with numerical precision.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">get_log_target_value</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># log(likelihood)</span></span>
<span>  <span class="fu">sum</span><span class="op">(</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">X</span>,</span>
<span>          mean <span class="op">=</span> <span class="va">pars</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">pars</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span>, </span>
<span>          sd <span class="op">=</span> <span class="fl">1</span>,</span>
<span>          log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>      <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># log(prior)</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">pars</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fl">0</span>, <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">pars</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">0</span>, <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Then, we’ll write a function that computes the acceptance ratio <span class="math inline">\(r\)</span>. Since the proposal is symmetric, the
expression reduces to the ratio of the posterior densities of the
proposed and current parameter values. Notice that a ratio on a log
scale is equal to the difference of logarithms.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute ratio</span></span>
<span><span class="va">get_ratio</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span>, <span class="va">pars_proposal</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">exp</span><span class="op">(</span></span>
<span>    <span class="fu">get_log_target_value</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_proposal</span><span class="op">)</span> <span class="op">-</span> </span>
<span>      <span class="fu">get_log_target_value</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Finally, we can wrap the helpers in a function that loops over the
algorithm steps.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sampler</span></span>
<span><span class="va">MH_sampler</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="co"># Data</span></span>
<span>                       <span class="va">inits</span>, <span class="co"># Initial values</span></span>
<span>                       <span class="va">n_samples</span> <span class="op">=</span> <span class="fl">1000</span>, <span class="co"># Number of iterations</span></span>
<span>                       <span class="va">jump_scale</span> <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># Proposal jump variance</span></span>
<span>                       <span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  </span>
<span>  <span class="co"># Matrix for samples</span></span>
<span>  <span class="va">pars</span> <span class="op">&lt;-</span> <span class="fu">matrix</span><span class="op">(</span>nrow <span class="op">=</span> <span class="va">n_samples</span>, ncol <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">inits</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Set initial values</span></span>
<span>  <span class="va">pars</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">inits</span></span>
<span></span>
<span>  <span class="co"># Generate samples </span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">n_samples</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="co"># Current parameters</span></span>
<span>    <span class="va">pars_now</span> <span class="op">&lt;-</span> <span class="va">pars</span><span class="op">[</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span>, <span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Proposal</span></span>
<span>    <span class="va">pars_proposal</span> <span class="op">&lt;-</span> <span class="fu">generate_proposal</span><span class="op">(</span><span class="va">pars_now</span>, <span class="va">jump_scale</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Ratio</span></span>
<span>    <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">get_ratio</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span>, <span class="va">pars_proposal</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">min</span><span class="op">(</span><span class="fl">1</span>, <span class="va">r</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Does the sampler move?</span></span>
<span>    <span class="va">move</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>                   size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                   prob <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">r</span>, <span class="fl">1</span><span class="op">-</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="co"># OR: </span></span>
<span>    <span class="co"># move &lt;- runif(n = 1, min = 0, max = 1) &lt;= r</span></span>
<span>    </span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">move</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">pars</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pars_proposal</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="va">pars</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pars_now</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># Into data frame</span></span>
<span>  <span class="va">pars</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span><span class="va">pars</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">pars</span><span class="op">)</span> </span>
<span>  </span>
<span><span class="op">}</span></span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="run-mh">Run MH<a class="anchor" aria-label="anchor" href="#run-mh"></a>
</h4>
<p>Now we can try out our MH implementation. Let’s use the simulated
data points stored in vector <code>X</code>:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="fl">3.78</span>, <span class="fl">2.76</span>, <span class="fl">2.84</span>, <span class="fl">2.92</span>, <span class="fl">1.3</span>, <span class="fl">3.93</span>, <span class="fl">3.69</span>, <span class="fl">2.28</span>, <span class="fl">2.81</span>, <span class="fl">0.71</span><span class="op">)</span></span></code></pre>
</div>
<p>We’ll generate 1000 samples with initial value (0, 5) and jump scale
0.01. The trajectory of samples is plotted over the posterior density
computed with the grid approximation.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">12</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Draw samples</span></span>
<span><span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu">MH_sampler</span><span class="op">(</span><span class="va">X</span>,</span>
<span>                      inits <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span>,</span>
<span>                      n_samples <span class="op">=</span> <span class="fl">1000</span>, </span>
<span>                      jump_scale <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">colnames</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"theta1"</span>, <span class="st">"theta2"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add column for sample index</span></span>
<span><span class="va">samples</span><span class="op">$</span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot joint posterior samples</span></span>
<span><span class="va">p_MH1</span> <span class="op">&lt;-</span> <span class="va">p_grid</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta1</span>, y <span class="op">=</span> <span class="va">theta2</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_MH1</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/mcmc-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Looking at the figure, a few observations become evident. Firstly,
despite the chosen initial value being moderately distant from the
high-density areas of the posterior, the algorithm quickly converges to
the target region. This rapid convergence is due to the fact that
proposals toward higher density areas are favored, in fact they are
always accepted when using normal density proposals. However, it’s
important to note that such swift convergence is not guaranteed in all
scenarios. In cases with a high number of model parameters, there’s an
increased likelihood of the sampler taking ‘wrong’ directions, a
phenomenon known as the curse of dimensionality. The sampler, before
convergence, may introduce bias to the posterior approximation.</p>
<p>Secondly, the posterior is not fully explored; no samples are
generated from the lower mode in the figure. This highlights a crucial
point: even if the sampler has converged, it doesn’t necessarily imply
that the drawn samples provide a representative picture of the
target.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Consider how you could address the two issues raised above:</p>
<ol style="list-style-type: decimal">
<li>Initial unconverged samples introduce a bias.</li>
<li>The sampler may not have explored the target distribution
properly.</li>
</ol>
<p>Try different proposal distributions variances in the MCMC example
above by changing <code>jump_scale</code>. How does this affect the
inference and convergence? Why?</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="assessing-convergence"><h2 class="section-heading">Assessing convergence<a class="anchor" aria-label="anchor" href="#assessing-convergence"></a>
</h2>
<hr class="half-width">
<p>Although convergence of MCMC is theoretically guaranteed, in
practice, this is not always the case. Monitoring convergence is crucial
whenever MCMC is utilized to ensure the reliability of recovered
results.</p>
<p>Depending on the model used, initial values, amount of data, among
other factors, can cause convergence issues. Earlier, we mentioned two
common complications, and here we will list a few more, along with
actions that can alleviate the issues.</p>
<ol style="list-style-type: decimal">
<li><p>Slow convergence can occur when initial values of the chain are
far from most of the target mass, resulting in early iterations biasing
the approximation. Another cause for slow convergence is that the
proposals are not far enough from the current value, and the sampler
moves too slowly.</p></li>
<li><p>Incomplete exploration: This means that the sampler doesn’t spend
enough time in all significant posterior areas.</p></li>
<li><p>A large proportion of the proposals is rejected. When the
proposal distribution generates proposals too far from the current
value, the proposals are rejected and the sampler stands still for many
iterations. This leads to inefficiency.</p></li>
<li><p>Sample autocorrelation: Consecutive samples are close to each
other. Ideally, we’d like to generate independent samples from the
target. High sample autocorrelation can be caused by several factors,
including the ones mentioned in the previous points</p></li>
</ol>
<p>These issues can be remedied with:</p>
<ol style="list-style-type: decimal">
<li><p>Running multiple long chains with distinct or random initial
values.</p></li>
<li><p>Discarding the early proportion of the chain as warm-up.</p></li>
<li><p>Setting an appropriate proposal distribution. This is easier said
than done and not trivial in practice.</p></li>
</ol>
<p>It also important to somehow be able to monitor whether or not the
sampler has converged. This can be done with statistics, such as
<em>effective sample size</em> and <span class="math inline">\(\hat{R}\)</span>. Effective sample size estimates
how many independent samples have been generated. Ideally, this number
should be close to the total number of iterations the sampler has been
ran. <span class="math inline">\(\hat{R}\)</span> on the other hand
measures chain mixing, that is, how well the chains agree with each
other. It is computed by comparing the variance within each chain to the
total variance of all samples. Usually, values of <span class="math inline">\(\hat{R} &gt; 1.1\)</span> are considered as
signaling convergence issues.</p>
<p>Besides statistics, visually evaluation the samples can be useful.
<em>Trace plots</em> refer to graphs where the marginal posterior
samples are plotted against sample index. Trace plots can be used to
investigate convergence and mixing properties, and can reveal, for
example, multimodality.</p>
<p>In Stan, many of the above-mentioned points have been automatized. By
default, Stan runs 4 chains with 2000 iterations each, and discards the
initial 50% as warm-up. Moreover, it computes <span class="math inline">\(\hat{R}\)</span>, effective sample size, and other
statistics and throws warnings in case or issues.</p>
<div class="section level3">
<h3 id="example-continued">Example continued<a class="anchor" aria-label="anchor" href="#example-continued"></a>
</h3>
<p>In light of the above information, let’s re-fit the model of the
previous example. Now, we’ll run 4 chains with random initial values,
10000 samples each, and discard the first 50% of each chain as warm-up.
We’ll use 0.1 as the proposal variance.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Number of chains</span></span>
<span><span class="va">n_chains</span> <span class="op">&lt;-</span> <span class="fl">4</span></span>
<span></span>
<span><span class="co"># Number of samples</span></span>
<span><span class="va">n_samples</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span></span>
<span><span class="co"># Consider first p% samples as warmup</span></span>
<span><span class="va">warmup</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span></span>
<span><span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_chains</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Use random initial values</span></span>
<span>  <span class="va">inits</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">chain</span> <span class="op">&lt;-</span> <span class="fu">MH_sampler</span><span class="op">(</span><span class="va">X</span>, inits <span class="op">=</span> <span class="va">inits</span>,</span>
<span>                        n_samples <span class="op">=</span> <span class="va">n_samples</span>, </span>
<span>                        jump_scale <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Wrangle</span></span>
<span>  <span class="fu">colnames</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"theta1"</span>, <span class="st">"theta2"</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">$</span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">$</span><span class="va">chain</span> <span class="op">&lt;-</span> <span class="fu">as.factor</span><span class="op">(</span><span class="va">i</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fu">round</span><span class="op">(</span><span class="va">warmup</span><span class="op">*</span><span class="va">n_samples</span><span class="op">)</span>, <span class="st">"warmup"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span>  <span class="va">chain</span><span class="op">[</span><span class="op">(</span><span class="fu">round</span><span class="op">(</span><span class="va">warmup</span><span class="op">*</span><span class="va">n_samples</span><span class="op">)</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="va">n_samples</span>, <span class="st">"warmup"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<p>Now it’s evident that the sample trajectories explore the entire
posterior distribution:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot</span></span>
<span><span class="va">p_joint_2</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># warmup samples</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span></span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span><span class="va">theta1</span>, <span class="va">theta2</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span>,</span>
<span>            alpha <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># post-warmup samples</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span></span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span><span class="va">theta1</span>, <span class="va">theta2</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_joint_2</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/mcmc-rendered-unnamed-chunk-9-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s see what the trace plots look like.</p>
<p><em>Trace plot conclusions: —-&gt; </em></p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Trace plots</span></span>
<span><span class="va">p_trace_2</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"parameter"</span>,</span>
<span>                     value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                     <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="st">"sample"</span>, <span class="st">"chain"</span>, <span class="st">"warmup"</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sample</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span>, </span>
<span>            alpha <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"parameter"</span>,</span>
<span>                     value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                     <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="st">"sample"</span>, <span class="st">"chain"</span>, <span class="st">"warmup"</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sample</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">parameter</span>,</span>
<span>             ncol <span class="op">=</span> <span class="fl">1</span>,</span>
<span>             scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_trace_2</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/mcmc-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</section><section id="hamiltonian-monte-carlo"><h2 class="section-heading">Hamiltonian Monte Carlo<a class="anchor" aria-label="anchor" href="#hamiltonian-monte-carlo"></a>
</h2>
<hr class="half-width">
<p>Hamiltonian Monte Carlo (HMC) is a variant of the Metropolis-Hastings
algorithm implemented in Stan. The defining feature is the elaborate
scheme it uses to generate proposals. Briefly, the idea is to simulate
the dynamics of a particle moving in a potential landscape defined by
the posterior. At each iteration, the particle is given a random
momentum vector and then its dynamics are simulated forward for some
time. The end of the trajectory is then taken as the proposal value.</p>
<p>Compared to the random walk Metropolis-Hastings we implemented in
this episode, HMC is very efficient. The main advantages of HMC is its
ability to explore high-dimensional spaces more effectively, making it
especially useful in complex models with many parameters</p>
<p>A type of convergence criterion exclusive to HMC are divergent
transitions. In region of the parameter space where the posterior has
high curvature, the simulated particle dynamics can produce spurious
transitions which do not represent the posterior accurately. Such
transitions are called divergent and signal that the particular area of
parameter space is not explored accurately. Stan provides information
about divergent transitions automatically.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Markov chain Monte Carlo methods can be used to generate samples
from a posterior distribution.</li>
<li>MCMC convergence should always be monitored.</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li><p>See interactive visualization of different MCMC algorithms: <a href="https://chi-feng.github.io/mcmc-demo/app.html" class="external-link uri">https://chi-feng.github.io/mcmc-demo/app.html</a></p></li>
<li><p>Bayesian Data Analysis (3rd ed.): Ch. 11-12</p></li>
<li><p>Statistical Rethinking (2nd ed.): Ch. 9</p></li>
<li><p>Bayes Rules!: Ch. 6-7</p></li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-hierarchical-models"><p>Content from <a href="hierarchical-models.html">Hierarchical Models</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/hierarchical-models.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does Bayesian modeling accommodate group structure?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn to construct and fit hierarchical models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p><strong>Hierarchical</strong> (or multi-level) models are a class of
models well-suited for situations where the study population comprises
distinct but interconnected groups. For example, analyzing student
performance across different schools, income disparities among various
regions, or studying animal behavior within different populations are
scenarios where such models can offer valuable insights.</p>
<p>Incorporating group-wise parameters allows us to model each group
separately, and a model becomes hierarchical when we treat the
parameters of the prior distribution as unknown. These parameters, known
as hyperparameters, are assigned their own prior distribution, referred
to as a hyperprior, and are learned during the model fitting process.
Conceptually, these hyperparameters and hyperpriors operate at a higher
level of hierarchy, hence the name.</p>
<p>For instance, let’s consider the beta-binomial model discussed in
Episode 1. It was employed to estimate the prevalence of left-handedness
based on a sample of 50 students. If we were to include additional
information, such as students’ majors, we could extend the model as
follows:</p>
<p><span class="math display">\[X_g \sim \text{Bin}(N_g, \theta_g) \\
\theta_g \sim \text{Beta}(\alpha, \beta) \\
\alpha, \beta \sim \Gamma(2, 0.1).\]</span></p>
<p>Here, the subscript <span class="math inline">\(g\)</span> indexes
the groups based on majors. The group-specific prevalences for
left-handedness $ _g $ are assigned a beta prior with hyperparameters $
$ and $ $ treated as random variables. The final line indicates the
hyperprior $ (2, 0.1) $ governing the prior beliefs about the
hyperparameters.</p>
<p>In this hierarchical beta-binomial model, students are considered
exchangeable within their majors but no longer across the entire
population. However, an underlying assumption of similarity exists
between the groups since they share a common prior. This is often
referred to as partial pooling, where groups are not entirely
independent but are not treated as equal either.</p>
<p>One of the key advantages of Bayesian hierarchical models is their
capacity to leverage information across groups. By pooling information
from various groups, these models can yield more robust estimates,
particularly when data availability is limited.</p>
<p>Another distinction from non-hierarchical models is that the prior,
or the <strong>population distribution</strong>, of the parameters is
learned in the process. This population distribution can offer insights
into parameter variability on a broader scale, even for groups where
data is scarce or completely missing. For instance, if we had data on
the handedness of students majoring in natural sciences, the population
distribution could provide insights into students in humanities and
social sciences as well.</p>
<p>In the following example, we will perform a hierarchical analysis of
human heights across different countries.</p>
<section id="example-human-height"><h2 class="section-heading">Example: human height<a class="anchor" aria-label="anchor" href="#example-human-height"></a>
</h2>
<hr class="half-width">
<p>Let’s examine the heights of adults in various countries. In [1],
averages and standard errors of adult heights in centimeters across
different countries and age groups were provided. We’ll utilize this
dataset to generate a sample of hypothetical individual heights and then
assess our ability to accurately reproduce these measured height
statistics.</p>
<p>This approach is commonly employed in model testing: we generate
simulated data with known parameters and subsequently compare the
inferred results to these known parameters. In our case, the true
parameters are derived from real-world data.</p>
<p>We’ll employ a normal model with unknown mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> as our generative model, and treat
these parameters hierarchically.</p>
<p>First, let’s load the data and examine its structure.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">height</span> <span class="op">&lt;-</span> <span class="fu">read.csv</span><span class="op">(</span><span class="st">"data/height_data.csv"</span><span class="op">)</span></span>
<span><span class="fu">str</span><span class="op">(</span><span class="va">height</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>'data.frame':	210000 obs. of  8 variables:
 $ Country                                   : chr  "Afghanistan" "Afghanistan" "Afghanistan" "Afghanistan" ...
 $ Sex                                       : chr  "Boys" "Boys" "Boys" "Boys" ...
 $ Year                                      : int  1985 1985 1985 1985 1985 1985 1985 1985 1985 1985 ...
 $ Age.group                                 : int  5 6 7 8 9 10 11 12 13 14 ...
 $ Mean.height                               : num  103 109 115 120 125 ...
 $ Mean.height.lower.95..uncertainty.interval: num  92.9 99.9 106.3 112.2 117.9 ...
 $ Mean.height.upper.95..uncertainty.interval: num  114 118 123 128 132 ...
 $ Mean.height.standard.error                : num  5.3 4.72 4.27 3.92 3.66 ...</code></pre>
</div>
<p>Let’s subset this data to simplify the analysis and focus on the
height of adult women measured in 2019.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">height_women</span> <span class="op">&lt;-</span> <span class="va">height</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">filter</span><span class="op">(</span></span>
<span>    <span class="va">Age.group</span> <span class="op">==</span> <span class="fl">19</span>, </span>
<span>    <span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span>,</span>
<span>    <span class="va">Year</span> <span class="op">==</span> <span class="fl">2019</span></span>
<span>    <span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Select variables of interest</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span>, <span class="va">Mean.height</span>, <span class="va">Mean.height.standard.error</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s select 10 countries randomly</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Select countries</span></span>
<span><span class="va">N_countries</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">Countries</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span><span class="fu">unique</span><span class="op">(</span><span class="va">height_women</span><span class="op">$</span><span class="va">Country</span><span class="op">)</span>,</span>
<span>                    size <span class="op">=</span> <span class="va">N_countries</span>,</span>
<span>                    replace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">sort</span></span>
<span></span>
<span><span class="va">height_women10</span> <span class="op">&lt;-</span> <span class="va">height_women</span> <span class="op">%&gt;%</span> <span class="fu">filter</span><span class="op">(</span><span class="va">Country</span> <span class="op">%in%</span> <span class="va">Countries</span><span class="op">)</span></span>
<span></span>
<span><span class="va">height_women10</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>              Country   Sex Mean.height Mean.height.standard.error
1       Guinea Bissau Girls    158.7194                  1.1152409
2            Honduras Girls    155.1831                  0.8690683
3           Indonesia Girls    154.3552                  0.5208758
4          Kyrgyzstan Girls    160.2019                  0.8355103
5          Mozambique Girls    155.4210                  0.7713099
6         Netherlands Girls    170.3612                  1.0355356
7              Panama Girls    158.1865                  0.9293961
8  Russian Federation Girls    164.5174                  0.7749542
9              Serbia Girls    168.2851                  0.7831222
10           Viet Nam Girls    158.4257                  0.5720185</code></pre>
</div>
<div class="section level3">
<h3 id="simulate-data">Simulate data<a class="anchor" aria-label="anchor" href="#simulate-data"></a>
</h3>
<p>Now, we can treat the values in the table above as ground truth and
simulate some data based on them. Let’s generate <span class="math inline">\(N=15\)</span> samples for each country from the
normal model with <span class="math inline">\(\mu =
\text{Mean.height}\)</span> and <span class="math inline">\(\sigma =
\text{Mean.height.standard.error}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size per group </span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">15</span></span>
<span></span>
<span><span class="co"># For each country, generate heights</span></span>
<span><span class="va">height_sim</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N_countries</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">my_df</span> <span class="op">&lt;-</span> <span class="va">height_women10</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>Country <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Country</span>, </span>
<span>             <span class="co"># Random values from normal</span></span>
<span>             Height <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="va">N</span>,</span>
<span>                            mean <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Mean.height</span>,</span>
<span>                            sd <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Mean.height.standard.error</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s plot the data</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot</span></span>
<span><span class="va">height_sim</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Height</span>, y <span class="op">=</span> <span class="va">Country</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Simulated data"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/hierarchical-models-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="modeling">Modeling<a class="anchor" aria-label="anchor" href="#modeling"></a>
</h3>
<p>Let’s build a normal model that uses partial pooling for the country
means and standard deviations. The model can be stated as follows:</p>
<p><span class="math display">\[\begin{align}
X_{gi} &amp;\sim \text{N}(\mu_g, \sigma_g) \\
\mu_g &amp;\sim \text{N}(\mu_\mu, \sigma_\mu) \\
\sigma_g &amp;\sim \Gamma(\alpha_\sigma, \beta_\sigma) \\
\mu_\mu &amp;\sim \text{N}(0, 100)\\
\sigma_\mu &amp;\sim \Gamma(2, 0.1) \\
\alpha_\sigma, \beta_\sigma  &amp;\sim \Gamma(2, 0.01).
\end{align}\]</span></p>
<p>Above, <span class="math inline">\(X_{gi}\)</span> denotes the height
for individual <span class="math inline">\(i\)</span> in country <span class="math inline">\(g\)</span>. The country specific parameters <span class="math inline">\(\mu_g\)</span> and <span class="math inline">\(\sigma_g\)</span> are given normal and gamma
priors, respectively, with unknown hyperparameters that, in turn, are
given hyperpriors on the last two lines.</p>
<p>Below is the Stan program for this model. The data points are input
as a concatenated vector <code>X</code>. The country-specific start and
end indices are computed in the transformed data block. This approach
accommodates uneven sample sizes between groups, although in our data
these are equal.</p>
<p>The parameters block contains the declarations of mean and standard
deviation vectors, along with the hyperparameters. The hyperparameter
subscripts denote the parameter they are assigned to so, for instance,
<span class="math inline">\(\sigma_{\mu}\)</span> is the standard
deviation of the mean parameter <span class="math inline">\(\mu\)</span>. The generated quantities block
generates samples from the population distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and a posterior predictive
distribution <span class="math inline">\(\tilde{X}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; G; <span class="co">// number of groups</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N[G]; <span class="co">// sample size within each group</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>  <span class="dt">vector</span>[sum(N)] X; <span class="co">// concatenated observations</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>}</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  <span class="co">// get first and last index for each group in X</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  <span class="dt">int</span> start_i[G];</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  <span class="dt">int</span> end_i[G];</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  </span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>  <span class="cf">for</span>(g <span class="cf">in</span> <span class="dv">1</span>:G) {</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    </span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    <span class="cf">if</span>(g == <span class="dv">1</span>) {</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>      start_i[<span class="dv">1</span>] = <span class="dv">1</span>;</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>      start_i[g] = start_i[g<span class="dv">-1</span>] + N[g<span class="dv">-1</span>];</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    }</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    </span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    end_i[g] = start_i[g] + N[g]-<span class="dv">1</span>;</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>  }</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>}</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>  </span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>  <span class="co">// parameters</span></span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>  <span class="dt">vector</span>[G] mu;</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[G] sigma;</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>  </span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>  <span class="co">// hyperparameters</span></span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>  <span class="dt">real</span> mu_mu;</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_mu;</span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha_sigma;</span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; beta_sigma;</span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>}</span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>  </span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>  <span class="co">// Likelihood for each group</span></span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:G) {</span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a>    X[start_i[i]:end_i[i]] ~ normal(mu[i], sigma[i]);</span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a>  }</span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a>  </span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>  mu ~ normal(mu_mu, sigma_mu);</span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>  sigma ~ gamma(alpha_sigma, beta_sigma);</span>
<span id="cb8-47"><a href="#cb8-47" tabindex="-1"></a>  </span>
<span id="cb8-48"><a href="#cb8-48" tabindex="-1"></a>  <span class="co">// Hyperpriors</span></span>
<span id="cb8-49"><a href="#cb8-49" tabindex="-1"></a>  mu_mu ~ normal(<span class="dv">0</span>, <span class="dv">100</span>);</span>
<span id="cb8-50"><a href="#cb8-50" tabindex="-1"></a>  sigma_mu ~ inv_gamma(<span class="dv">2</span>, <span class="fl">0.1</span>);</span>
<span id="cb8-51"><a href="#cb8-51" tabindex="-1"></a>  alpha_sigma ~ gamma(<span class="dv">2</span>, <span class="fl">0.01</span>);</span>
<span id="cb8-52"><a href="#cb8-52" tabindex="-1"></a>  beta_sigma ~ gamma(<span class="dv">2</span>, <span class="fl">0.01</span>);</span>
<span id="cb8-53"><a href="#cb8-53" tabindex="-1"></a>}</span>
<span id="cb8-54"><a href="#cb8-54" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb8-56"><a href="#cb8-56" tabindex="-1"></a>  </span>
<span id="cb8-57"><a href="#cb8-57" tabindex="-1"></a>  <span class="dt">real</span> mu_tilda;</span>
<span id="cb8-58"><a href="#cb8-58" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_tilda;</span>
<span id="cb8-59"><a href="#cb8-59" tabindex="-1"></a>  <span class="dt">real</span> X_tilda; </span>
<span id="cb8-60"><a href="#cb8-60" tabindex="-1"></a>  </span>
<span id="cb8-61"><a href="#cb8-61" tabindex="-1"></a>  <span class="co">// Population distributions</span></span>
<span id="cb8-62"><a href="#cb8-62" tabindex="-1"></a>  mu_tilda = normal_rng(mu_mu, sigma_mu);</span>
<span id="cb8-63"><a href="#cb8-63" tabindex="-1"></a>  sigma_tilda = gamma_rng(alpha_sigma, beta_sigma);</span>
<span id="cb8-64"><a href="#cb8-64" tabindex="-1"></a>  </span>
<span id="cb8-65"><a href="#cb8-65" tabindex="-1"></a>  <span class="co">// Posterior predictive distribution</span></span>
<span id="cb8-66"><a href="#cb8-66" tabindex="-1"></a>  X_tilda = normal_rng(mu_tilda, sigma_tilda);</span>
<span id="cb8-67"><a href="#cb8-67" tabindex="-1"></a>  </span>
<span id="cb8-68"><a href="#cb8-68" tabindex="-1"></a>} </span></code></pre>
</div>
<p>Now we can call Stan and fit the model. Hierarchical models can
encounter convergence issues and for this reason, we’ll use 10000
iterations and set <code>adapt_delta = 0.99</code>. Moreover, we’ll
speed up the inference by running 2 chains in parallel by setting
<code>cores = 2</code>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">stan_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>G <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="fu">unique</span><span class="op">(</span><span class="va">height_sim</span><span class="op">$</span><span class="va">Country</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                  N <span class="op">=</span> <span class="fu">rep</span><span class="op">(</span><span class="va">N</span>, <span class="fu">length</span><span class="op">(</span><span class="va">Countries</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                  X <span class="op">=</span> <span class="va">height_sim</span><span class="op">$</span><span class="va">Height</span><span class="op">)</span></span>
<span></span>
<span><span class="va">normal_hier_fit</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">normal_hier_model</span>,</span>
<span>                                   <span class="va">stan_data</span>, </span>
<span>                                   iter <span class="op">=</span> <span class="fl">10000</span>,</span>
<span>                                   chains <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                                   <span class="co"># Use to avoid divergent transitions:</span></span>
<span>                                   control <span class="op">=</span> <span class="fu">list</span><span class="op">(</span>adapt_delta <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span>, </span>
<span>                                   cores <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                                   <span class="co"># Track progress every 5000 iterations</span></span>
<span>                                   refresh <span class="op">=</span> <span class="fl">5000</span></span>
<span>                                   <span class="op">)</span></span></code></pre>
</div>
<!-- Non-pooled analysis -->
<!-- Fit -->
</div>
</section><section id="results"><h2 class="section-heading">Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="country-specific-estimates">Country-specific estimates<a class="anchor" aria-label="anchor" href="#country-specific-estimates"></a>
</h3>
<p>Let’s first compare the marginal posteriors for the country-specific
estimates:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par_summary</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">summary</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">summary</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rownames_to_column</span><span class="op">(</span>var <span class="op">=</span> <span class="st">"par"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">separate</span><span class="op">(</span><span class="va">par</span>, into <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"par"</span>, <span class="st">"country"</span><span class="op">)</span>, sep <span class="op">=</span> <span class="st">"\\["</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>country <span class="op">=</span> <span class="fu">gsub</span><span class="op">(</span><span class="st">"\\]"</span>, <span class="st">""</span>, <span class="va">country</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>country <span class="op">=</span> <span class="va">Countries</span><span class="op">[</span><span class="fu">as.integer</span><span class="op">(</span><span class="va">country</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot</span></span>
<span></span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="co"># geom_point(data = par_summary, aes(x = country, y = mean),</span></span>
<span>  <span class="co">#            color = posterior_color) +</span></span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_summary</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">country</span>, ymin <span class="op">=</span> <span class="va">X2.5.</span>, ymax <span class="op">=</span> <span class="va">X97.5.</span><span class="op">)</span>,</span>
<span>                color <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women10</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>,</span>
<span>                      value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                      <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span><span class="op">)</span><span class="op">)</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Country</span>, y <span class="op">=</span> <span class="va">value</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_errorbar</span><span class="op">(</span>data <span class="op">=</span> <span class="va">unpooled_summaries</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">country</span>, ymin <span class="op">=</span> <span class="va">X2.5.</span>, ymax <span class="op">=</span> <span class="va">X97.5.</span><span class="op">)</span>,</span>
<span>                color <span class="op">=</span> <span class="st">"brown"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span> <span class="va">par</span>, scales <span class="op">=</span> <span class="st">"free"</span>, ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_flip</span><span class="op">(</span><span class="op">)</span> </span></code></pre>
</div>
<figure><img src="fig/hierarchical-models-rendered-unnamed-chunk-12-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Above, the black points represent the true values, and the intervals
are the 95% CIs for a hierarchical and non-hierarchical models,
respectively.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Experiment with the data and fit. Explore the effect of sample size,
unequal sample sizes between countries, and the amount of countries, for
example.</p>
</div>
</div>
</div>
</div>
</section><section id="hyperparameters"><h2 class="section-heading">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h2>
<hr class="half-width">
<p>Let’s then plot the population distribution’s parameters, that is,
the hyperparameters. The sample-based values are included in the plots
of <span class="math inline">\(\mu_\mu\)</span> and <span class="math inline">\(\sigma_\mu\)</span> (why not for the other two
hyperparameters?).</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Population distributions:</span></span>
<span><span class="va">population_samples_l</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>,</span>
<span>                                       <span class="fu">c</span><span class="op">(</span><span class="st">"mu_mu"</span>, <span class="st">"sigma_mu"</span>, <span class="st">"alpha_sigma"</span>, <span class="st">"beta_sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu_mu"</span>, <span class="st">"sigma_mu"</span>, <span class="st">"alpha_sigma"</span>, <span class="st">"beta_sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"hyperpar"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">sample</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">population_samples_l</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">posterior_color</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>               <span class="fu">filter</span><span class="op">(</span><span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">summarise</span><span class="op">(</span>mu_mu <span class="op">=</span> <span class="fu">mean</span><span class="op">(</span><span class="va">mu</span><span class="op">)</span>, sigma_mu <span class="op">=</span> <span class="fu">sd</span><span class="op">(</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"hyperpar"</span>, value <span class="op">=</span> <span class="st">"value"</span><span class="op">)</span>,</span>
<span>             <span class="fu">aes</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">value</span><span class="op">)</span></span>
<span>             <span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">hyperpar</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/hierarchical-models-rendered-unnamed-chunk-13-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="population-distributions"><h2 class="section-heading">Population distributions<a class="anchor" aria-label="anchor" href="#population-distributions"></a>
</h2>
<hr class="half-width">
<p>Let’s then plot the population distributions and compare to the
sample <span class="math inline">\(\mu\)</span>’s and <span class="math inline">\(\sigma\)</span>’s</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">population_l</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu_tilda"</span>, <span class="st">"sigma_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span> <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">sample</span><span class="op">)</span></span>
<span></span>
<span></span>
<span> </span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">population_l</span>,</span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women</span> <span class="op">%&gt;%</span></span>
<span>                     <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>                     <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>                     <span class="fu">filter</span><span class="op">(</span><span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span><span class="op">)</span>, </span>
<span>                   <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                   alpha <span class="op">=</span> <span class="fl">0.75</span>, bins <span class="op">=</span> <span class="fl">30</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">par</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Blue = posterior; black = sample"</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="fig/hierarchical-models-rendered-unnamed-chunk-14-1.png" style="display: block; margin: auto;" class="figure">
We can see that the population distribution is able to capture the
measured average heights and standard deviations relatively well.
Remember that these estimates are based on a limited sample: 10 out of
200 countries with 15 individuals in each group.</p>
</section><section id="posterior-predictive-distribution"><h2 class="section-heading">Posterior predictive distribution<a class="anchor" aria-label="anchor" href="#posterior-predictive-distribution"></a>
</h2>
<hr class="half-width">
<p>Finally, let’s plot the posterior predictive distribution. Let’s
overlay it with the simulated data based on all countries.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># For each country, generate some random girl's heights</span></span>
<span><span class="va">Height_all</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">height_women</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">my_df</span> <span class="op">&lt;-</span> <span class="va">height_women</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>Country <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Country</span>, </span>
<span>             Sex <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Sex</span>, </span>
<span>             <span class="co"># Random normal values based on sample mu and sd</span></span>
<span>             Height <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="va">N</span>, <span class="va">my_df</span><span class="op">$</span><span class="va">mu</span>, <span class="va">my_df</span><span class="op">$</span><span class="va">sigma</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract the posterior predictive distribution</span></span>
<span><span class="va">PPD</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span> <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">PPD</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_tilda</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">Height_all</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Height</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                 alpha <span class="op">=</span> <span class="fl">0.75</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/hierarchical-models-rendered-unnamed-chunk-16-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="extensions"><h2 class="section-heading">Extensions<a class="anchor" aria-label="anchor" href="#extensions"></a>
</h2>
<hr class="half-width">
<p>We analyzed women’s heights in a few countries and modeled them
hierarchically. You could make the structure richer in many ways, for
instance by adding hierarchy between sexes, continents,
developed/developing countries etc.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Hierarchical models are appropriate for scenarios where the study
population naturally divides into subgroups.</li>
<li>Hierarchical model borrow statistical strength across the
groups.</li>
<li>Population distributions hold information about the variation of the
model parameters in the whole population.</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Bayesian Data Analysis (3rd ed.): Ch. 5</li>
<li>Statistical Rethinking (2nd ed.): Ch. 13</li>
<li>Bayes Rules!: Ch. 15-19</li>
</ul></section><section id="references"><h2 class="section-heading">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<hr class="half-width">
<p>[1] Height: Height and body-mass index trajectories of school-aged
children and adolescents from 1985 to 2019 in 200 countries and
territories: a pooled analysis of 2181 population-based studies with 65
million participants. Lancet 2020, 396:1511-1524</p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-model-critisism"><p>Content from <a href="model-critisism.html">Model checking</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/model-critisism.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is model checking?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Prior/Posterior predictive check</p></li>
<li>
<p>Model comparison with</p>
<ul>
<li>AIC, BIC, WAIC</li>
</ul>
</li>
<li><p>Bayesian cross-validation</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>This episode focuses on model checking, a crucial step in Bayesian
data analysis when dealing with competing models that require systematic
comparison. We’ll explore three different approaches for this
purpose.</p>
<p>Firstly, we’ll delve into posterior predictive checks, a method that
involves comparing a fitted model’s predictions with observed data.</p>
<p>Next, we’ll examine information criteria, a tool that measures the
balance between model complexity and goodness-of-fit.</p>
<p>We’ll end the episode with an exploration of Bayesian
cross-validation.</p>
<p>Throughout the episode, we’ll use the same simulated dataset for
examples.</p>
<section id="data"><h2 class="section-heading">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h2>
<hr class="half-width">
<p>For data, we’re using <span class="math inline">\(N=88\)</span>
univariate numerical data. Looking at a histogram, it’s evident that the
data is approximately symmetrically distributed around 0. However, there
is some dispersion in the values, suggesting that the tails might be
longer than those of the normal distribution. Next, we’ll compare the
suitability of the normal and Cauchy distributions on this data.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1]  -2.270   1.941   0.502  -0.378  -0.226  -0.786  -0.209  -0.637   0.814
[10]   0.566  -1.901  -2.047  -0.689  -3.509   0.133  -4.353   1.067   0.722
[19]   0.861   0.523   0.681   2.982   0.429  -0.539  -0.512  -1.090  -8.044
[28]  -0.387  -0.007 -11.126   1.036   1.734  -0.203   1.036   0.582  -2.922
[37]  -0.543  -6.120  -0.649   4.547  -0.867   1.942   7.148  -0.044  -0.681
[46]  -3.461  -0.142   0.678   0.644  -0.039   0.354   1.783   0.369   0.175
[55]   0.980  -0.097  -4.408   0.442   0.158   0.255   0.084   0.775   2.786
[64]   0.008  -0.664  43.481   1.943   0.334  -0.118   3.901   1.736  -0.665
[73]   2.695   0.002  -1.904  -2.194  -4.015   0.329   1.140  -3.816 -14.788
[82]   0.047   6.205   1.119  -0.003   3.618   1.666 -10.845</code></pre>
</div>
<figure><img src="fig/model-critisism-rendered-unnamed-chunk-1-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="posterior-predictive-check"><h2 class="section-heading">Posterior predictive check<a class="anchor" aria-label="anchor" href="#posterior-predictive-check"></a>
</h2>
<hr class="half-width">
<p>The idea of posterior predictive checking is to use the posterior
predictive distribution to simulate a replicate data set and compare it
to the observed data. The reasoning behind this approach is, as
formulated in BDA3 p.143, “If the model fits, then replicated data
generated under the model should look similar to observed data.”</p>
<p>Any qualitative discrepancies between the simulated and observed data
can imply shortcomings in the model that do not match the properties of
the data or the domain. Comparison between simulated and actual data can
be done in different ways. Visual check is one option but a more
rigorous approach is to compute the posterior predictive p-value (ppp),
which measures how well the the model can reproduce the observed
data.</p>
<p>The posterior predictive check, utilizing ppp, can be formulated as
follows:</p>
<ol style="list-style-type: decimal">
<li>Generate replicate data: Use the posterior predictive distribution
to simulate new datasets <span class="math inline">\(X^{rep}\)</span>
with characteristics matching the observed data. In our example, this
amounts to generating a large number of replications with sample size
<span class="math inline">\(N=88\)</span>.</li>
<li>Choose test quantity <span class="math inline">\(T(X)\)</span>:
Choose an aspect of the data that you wish to check. We’ll use the
maximum value of the data as the test quantity and compute it for the
observed data and for each replication: <span class="math inline">\(T(X^{rep})\)</span>.</li>
<li>Compute ppp: The posterior predictive p-value defined as the
probability <span class="math inline">\(Pr(T(X^{rep}) \geq T(X) |
X)\)</span>, that is the probability that the predictions produce test
quantities at least as extreme as those found in the data. Using
samples, it is computed as the proportion of replicate data sets with
<span class="math inline">\(T\)</span> not smaller than that of <span class="math inline">\(T(X)\)</span>.</li>
</ol>
<p>A small ppp-value would indicate that the model doesn’t capture the
properties of the data.</p>
<p>Next, we’ll perform a posterior predictive check on the example data
and compare the results for the normal and Cauchy models.</p>
<div class="section level3">
<h3 id="normal-model">Normal model<a class="anchor" aria-label="anchor" href="#normal-model"></a>
</h3>
<p>We’ll use a basic Stan program for the normal model and produce the
replicate data in the generated quantities block. Notice that
<code>X_rep</code> is a vector with length equal to the sample size
<span class="math inline">\(N\)</span>. The values of <code>X_rep</code>
are generated in a loop. Notice that a single posterior value of <span class="math inline">\((\mu, \sigma)\)</span> is used for each evaluation
of the generated quantities block; the values do not change in the
iterations of the for loop.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>}</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>}</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>  X ~ normal(mu, sigma);</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>  </span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>  sigma ~ gamma(<span class="dv">2</span>, <span class="dv">1</span>);</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>}</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a>  <span class="dt">vector</span>[N] X_rep;</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>    X_rep[i] = normal_rng(mu, sigma);</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>  }</span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Fit model and extract replicates.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit</span></span>
<span><span class="va">normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract </span></span>
<span><span class="va">X_rep</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_fit</span>, <span class="st">"X_rep"</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<p>Below is a comparison of 12 samples of <span class="math inline">\(X^{rep}\)</span> against the data (the panel
titles correspond to MCMC sample numbers). The discrepancy between the
data and replicates indicates an issue with the model choice. It seems
like the normal model underestimates the data tails.</p>
<figure><img src="fig/model-critisism-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s quantify this discrepancy by computing the ppp using the
maximum of the data as a test statistic. The maximum of the original
data is max(<span class="math inline">\(X\)</span>) = 43.481. The
following histogram shows this value (vertical line) against the maximum
computed for each replicate data set <span class="math inline">\(X^{rep}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute X_rep max</span></span>
<span><span class="va">rep_maxs</span> <span class="op">&lt;-</span> <span class="va">X_rep</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">sample</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">apply</span><span class="op">(</span>MARGIN <span class="op">=</span> <span class="fl">1</span>, FUN <span class="op">=</span> <span class="va">max</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>max <span class="op">=</span> <span class="va">.</span>, sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">rep_maxs</span>,</span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">max</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">50</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fu">max</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Max value of the replicate data sets"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/model-critisism-rendered-unnamed-chunk-5-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The proportion of replications <span class="math inline">\(X_{rep}\)</span> that produce at least as extreme
values as the data is called the posterior predictive p-value (<span class="math inline">\(ppp\)</span>). The <span class="math inline">\(ppp\)</span> quantifies the evidence for the
suitability of the model for the data with higher <span class="math inline">\(ppp\)</span> implying a lesser conflict. In this
case, the value is <span class="math inline">\(ppp =\)</span> 1 which
means that the maximum was as at least as large as in the data in 0%
replications. This indicates strong evidence that the normal model is a
poor choice for the data.</p>
</div>
<div class="section level3">
<h3 id="cauchy-model">Cauchy model<a class="anchor" aria-label="anchor" href="#cauchy-model"></a>
</h3>
<p>Let’s do a similar analysis utilizing the Cauchy model, and compute
the <span class="math inline">\(ppp\)</span> for this model.</p>
<p>The code used is essentially copy-pasted from above, with the
distinction of the Stan program.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>}</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>  <span class="co">// Scale</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>  <span class="co">// location</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>}</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>  <span class="co">// location = mu and scale = sigma</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>  X ~ cauchy(mu, sigma);</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>  </span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>  sigma ~ gamma(<span class="dv">2</span>, <span class="dv">1</span>);</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>}</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>  <span class="dt">vector</span>[N] X_rep;</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    X_rep[i] = cauchy_rng(mu, sigma);</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>  }</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>}</span></code></pre>
</div>
<p>With the cauchy model there is little discrepancy between the data
and replicate data:</p>
<figure><img src="fig/model-critisism-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The maximum observed value is close to the average of the
distribution of maximum value for replicate sets. Moreoever, the <span class="math inline">\(ppp\)</span> is large, indicating no issues with
the suitability of the model on the data.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Compute ppp</span></span>
<span><span class="va">rep_maxs</span> <span class="op">&lt;-</span> <span class="va">X_rep</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="op">-</span><span class="va">sample</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">apply</span><span class="op">(</span>MARGIN <span class="op">=</span> <span class="fl">1</span>, FUN <span class="op">=</span> <span class="va">max</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>max <span class="op">=</span> <span class="va">.</span>, sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">rep_maxs</span>,</span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">max</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">10000</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="fu">max</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="fu">paste0</span><span class="op">(</span><span class="st">"ppp = "</span>, <span class="fu">mean</span><span class="op">(</span><span class="va">rep_maxs</span><span class="op">$</span><span class="va">max</span> <span class="op">&gt;</span> <span class="fu">max</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># set plot limits to aid with visualizations</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1000</span><span class="op">)</span><span class="op">)</span> </span></code></pre>
</div>
<figure><img src="fig/model-critisism-rendered-unnamed-chunk-8-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</section><section id="information-criteria"><h2 class="section-heading">Information criteria<a class="anchor" aria-label="anchor" href="#information-criteria"></a>
</h2>
<hr class="half-width">
<p>Information criteria are statistics used in model selection and
comparison within both framework of Bayesian and classical frequentist
statistics. The aim of these criteria is to estimate out-of-sample
predictive accuracy, and to provide a principled approach to assess the
relative performance of competing models.</p>
<p>The Widely Applicable Information Criterion (WAIC) is an of
information criteria that was developed within the Bayesian paradigm.
WAIC is computed using the log pointwise predictive density, lppd, and a
penalization term, <span class="math inline">\(p_{WAIC}\)</span>:</p>
<p><span class="math display">\[WAIC = -2(\text{lppd} -
p_{WAIC}).\]</span></p>
<p>The log pointwise predictive density is computed as $<em>{i=1}^N (
</em>{s=1}^S p(X_i | ^s), $, where <span class="math inline">\(X_i,
\,i=1,\ldots,N\)</span> are data points and <span class="math inline">\(S\)</span> the number of posterior samples. Since
the predictive density is computed on the data used to fit the model,
the estimate may be over-confident. The penalization term <span class="math inline">\(p_{WAIC} = \sum_{i=1}^N \text{Var}(\log p(y_i |
\theta^s))\)</span> correct for this bias</p>
<p>Lower WAIC values imply better fit.</p>
<p>Let’s then compare the normal and Cauchy models with the WAIC. First
we’ll need to fit both models on the data.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">stan_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit</span></span>
<span><span class="va">normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>, <span class="va">stan_data</span>,</span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">cauchy_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>, <span class="va">stan_data</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Extract samples</span></span>
<span><span class="va">normal_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">data.frame</span></span>
<span><span class="va">cauchy_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">cauchy_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">data.frame</span></span></code></pre>
</div>
<p>Then we can write a function that compute the WAIC.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">WAIC</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">samples</span>, <span class="va">data</span>, <span class="va">model</span><span class="op">)</span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Loop over data points</span></span>
<span>  <span class="va">pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Loop over posterior samples  </span></span>
<span>    <span class="va">point_pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op">{</span></span>
<span>      </span>
<span>      <span class="va">my_mu</span> <span class="op">&lt;-</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span></span>
<span>      <span class="va">my_sigma</span> <span class="op">&lt;-</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span></span>
<span>      </span>
<span>      <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="co"># Model: y ~ normal(mu, sigma)</span></span>
<span>        <span class="fu">dnorm</span><span class="op">(</span>x <span class="op">=</span> <span class="va">my_x</span>,</span>
<span>              mean <span class="op">=</span> <span class="va">my_mu</span>,</span>
<span>              sd <span class="op">=</span> <span class="va">my_sigma</span><span class="op">)</span></span>
<span>      <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="co"># Model: y ~ cauchy(mu, sigma)</span></span>
<span>        <span class="fu">dcauchy</span><span class="op">(</span>x <span class="op">=</span> <span class="va">my_x</span>,</span>
<span>                location <span class="op">=</span> <span class="va">my_mu</span>,</span>
<span>                scale <span class="op">=</span> <span class="va">my_sigma</span><span class="op">)</span></span>
<span>      <span class="op">}</span></span>
<span>      </span>
<span>    <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>      <span class="fu">unlist</span><span class="op">(</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="kw">return</span><span class="op">(</span><span class="va">point_pp_dens</span><span class="op">)</span></span>
<span>    </span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  </span>
<span>  <span class="co"># See BDA3 p.169</span></span>
<span>  <span class="va">lppd</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span>X <span class="op">=</span> <span class="va">pp_dens</span>,</span>
<span>                MARGIN <span class="op">=</span> <span class="fl">1</span>, </span>
<span>                FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">log</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="va">sum</span></span>
<span>  </span>
<span>  <span class="co"># See BDA3 p.173</span></span>
<span>  <span class="va">bias</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span>X <span class="op">=</span> <span class="va">pp_dens</span>,</span>
<span>                MARGIN <span class="op">=</span> <span class="fl">1</span>, </span>
<span>                FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">var</span><span class="op">(</span><span class="fu">log</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="va">sum</span></span>
<span>  </span>
<span>  <span class="co"># WAIC</span></span>
<span>  <span class="va">waic</span> <span class="op">=</span> <span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">lppd</span> <span class="op">-</span> <span class="va">bias</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">waic</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Applying this function to the posterior samples, we’ll recover a
lower value for the Cauchy model, implying a better fit on the data.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">WAIC</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="va">df</span><span class="op">$</span><span class="va">X</span>, model <span class="op">=</span> <span class="st">"normal"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 582.2829</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">WAIC</span><span class="op">(</span><span class="va">cauchy_samples</span>, <span class="va">df</span><span class="op">$</span><span class="va">X</span>, model <span class="op">=</span> <span class="st">"cauchy"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 413.9462</code></pre>
</div>
</section><section id="bayesian-cross-validation"><h2 class="section-heading">Bayesian cross-validation<a class="anchor" aria-label="anchor" href="#bayesian-cross-validation"></a>
</h2>
<hr class="half-width">
<p>Idea in Bayesian cross-validation is….</p>
<p>Helper function that computes..</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get log predictive density for a point x,</span></span>
<span><span class="co"># given data X and posterior samples</span></span>
<span><span class="co"># See BDA3 p.175</span></span>
<span><span class="va">get_lpd</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">X</span>, <span class="va">samples</span>, <span class="va">model</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Loop over posterior samples  </span></span>
<span>  <span class="va">pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># Normal(x | mu, sigma^2)</span></span>
<span>      <span class="fu">dnorm</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>,</span>
<span>            mean <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span>,</span>
<span>            sd <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># Cauchy(x | location = mu, scale = sigma^2)</span></span>
<span>      <span class="fu">dcauchy</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>,</span>
<span>              location <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span>,</span>
<span>              scale <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    </span>
<span>    </span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">unlist</span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">lpd</span> <span class="op">&lt;-</span> <span class="fu">log</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">pp_dens</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpd</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Now we can perform CV</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Loop over data partitions</span></span>
<span><span class="va">normal_loo_lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Training set</span></span>
<span>  <span class="va">my_X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Test set</span></span>
<span>  <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">my_X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">my_X</span><span class="op">)</span>,</span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span> <span class="co"># omits output</span></span>
<span>                            <span class="op">)</span> </span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get lpd</span></span>
<span>  <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">my_x</span>, <span class="va">my_X</span>, <span class="va">my_samples</span>, <span class="st">"normal"</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"normal_loo"</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictive density for data points using full data in training</span></span>
<span><span class="va">normal_full_lpd</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">dummy</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">X</span><span class="op">)</span>, </span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Compute lpds</span></span>
<span>  <span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">X</span>, <span class="va">my_samples</span>, <span class="st">"normal"</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"normal"</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpds</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Same for Cauchy:</span></span>
<span><span class="va">cauchy_loo_lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Subset data</span></span>
<span>  <span class="va">my_X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">my_X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">my_X</span><span class="op">)</span>, </span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get lpd</span></span>
<span>  <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">my_x</span>, <span class="va">my_X</span>, <span class="va">my_samples</span>, <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"cauchy_loo"</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cauchy_full_lpd</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">dummy</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_cachy_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>,</span>
<span>                           <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">X</span><span class="op">)</span>,</span>
<span>                                X <span class="op">=</span> <span class="va">X</span><span class="op">)</span>, </span>
<span>                           refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_cachy_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Compute lpds</span></span>
<span>  <span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">X</span>, <span class="va">my_samples</span>, <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpds</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Combine</span></span>
<span><span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">rbind</span><span class="op">(</span><span class="va">normal_loo_lpds</span>, </span>
<span>              <span class="va">normal_full_lpd</span>, </span>
<span>              <span class="va">cauchy_loo_lpds</span>,</span>
<span>              <span class="va">cauchy_full_lpd</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lpd_summary</span> <span class="op">&lt;-</span> <span class="va">lpds</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>lppd <span class="op">=</span> <span class="fu">sum</span><span class="op">(</span><span class="va">lpd</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Effective number of parameters</span></span>
<span><span class="va">p_loo_cv_normal</span> <span class="op">&lt;-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span>, <span class="st">"lppd"</span><span class="op">]</span> <span class="op">-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal_loo"</span>, <span class="st">"lppd"</span><span class="op">]</span></span>
<span><span class="va">p_loo_cv_cauchy</span> <span class="op">&lt;-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span>, <span class="st">"lppd"</span><span class="op">]</span> <span class="op">-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy_loo"</span>, <span class="st">"lppd"</span><span class="op">]</span></span>
<span></span>
<span></span>
<span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Effective number of parameters, normal = "</span>, <span class="va">p_loo_cv_normal</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Effective number of parameters, normal = 33.7046896782354"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Effective number of parameters, cauchy = "</span>, <span class="va">p_loo_cv_cauchy</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Effective number of parameters, cauchy = 1.903135942788"</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Statistical Rethinking: Ch. 7</li>
<li>BDA3: p.143: 6.3 Posterior predictive checking</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-gaussian-processes"><p>Content from <a href="gaussian-processes.html">Gaussian processes</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/gaussian-processes.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How to do probabilistic non-parameteric regression?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn to perform Gaussian process regression with Stan</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Gaussian processes (GPs) represent a class of stochastic (random)
processes widely employed for non-parametric regression.</p>
<p>Formally, a Gaussian process <span class="math inline">\(GP(\mu,
\Sigma)\)</span> is characterized as a collection of random variables
<span class="math inline">\(X\)</span> such that any finite subset <span class="math inline">\(X_I \subset X\)</span> follows a multivariate
normal distribution with mean <span class="math inline">\(\mu\)</span>
and covariance <span class="math inline">\(\Sigma\)</span>. Generating a
sample from a GP yields a random function <span class="math inline">\(X_I \to \mathbb{R}\)</span>, and this feature
enables using GPs as prior distributions for functions.</p>
<p>As an example, consider the modeling of crop yields as a function of
fertilizer use. Presumably, there exists a non-linear trend between
these variables, as insufficient or excessive fertilizer may lead to
suboptimal yields. In the absence of a parametric mechanistic model, GPs
can function as a prior for the relationship <span class="math inline">\(f\)</span> between fertilizer and yield. In its
simplest form, measured yields could be modeled as noisy observations
from <span class="math inline">\(f\)</span>: <span class="math display">\[ f(x) \sim GP(\mu, \Sigma),\]</span></p>
<p>where <span class="math inline">\(x\)</span> represents the amount of
fertilizer used.</p>
<p>As with all priors, the chosen hyperparameters (here <span class="math inline">\(\mu, \, \Sigma\)</span>) influence the inference.
The mean parameter <span class="math inline">\(\mu\)</span> defines the
average level of the process, while the covariance function <span class="math inline">\(\Sigma\)</span> exerts a more defining effect on
the process characteristics.</p>
<p>Perhaps the most frequently used covariance function is the squared
exponential kernel <span class="math inline">\(K_{SE}(x, x’) = \alpha^2
\exp^{ \frac{(x - x’)^2}{2 \lambda} }\)</span>. The parameter <span class="math inline">\(\alpha\)</span> sets the variance of the process,
and <span class="math inline">\(\lambda\)</span> determines the scale of
the correlation; increasing <span class="math inline">\(\lambda\)</span>
increases the correlation between <span class="math inline">\(x\)</span>
and <span class="math inline">\(x’\)</span>. In the figure below, we’ve
plotted some realizations from a GP with <span class="math inline">\(\mu
= (0, 0, \ldots, 0)\)</span> and squared exponential covariance function
with <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\lambda = 25\)</span>. The input space <span class="math inline">\(X_I\)</span> is the integers between 0 and
100.</p>
<figure><img src="fig/gaussian-processes-rendered-unnamed-chunk-2-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Generate samples from the GP above with different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> to get intuition about the role
of these hyperparameters.</p>
</div>
</div>
</div>
<p>Next, we’ll explore some simple examples that leverage Gaussian
processes.</p>
<section id="gaussian-process-regression"><h2 class="section-heading">Gaussian process regression<a class="anchor" aria-label="anchor" href="#gaussian-process-regression"></a>
</h2>
<hr class="half-width">
<p>Assume we have 5 data available and wed like to estimate a trend in
the data.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">2.76</span>, <span class="fl">2.46</span>, <span class="op">-</span><span class="fl">1.52</span>, <span class="op">-</span><span class="fl">4.34</span>, <span class="fl">4.54</span>,  <span class="fl">1</span><span class="op">)</span>,</span>
<span>                 y <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">0.81</span>, <span class="op">-</span><span class="fl">0.85</span>, <span class="fl">0.76</span>, <span class="op">-</span><span class="fl">0.41</span>, <span class="op">-</span><span class="fl">1.48</span>,  <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">df</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot data</span></span>
<span><span class="va">p_data</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_data</span></span></code></pre>
</div>
<figure><img src="fig/gaussian-processes-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s assume these are noisy observations from some unknown function
<span class="math inline">\(f\)</span> and try to estimate this function
by giving <span class="math inline">\(f\)</span> a Gaussian process
prior with the squared exponential covariance function.</p>
<p>The covariance function should be computed using the data points
<span class="math inline">\(x\)</span> and all those locations where we
want to predict the value of <span class="math inline">\(f\)</span>.
Let’s predict the <span class="math inline">\(f\)</span> on a grid of
points spanning the interval (-5, 5). The grid points are stored in
vector <code>x_pred</code></p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">N_pred</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">x_pred</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span>, length.out <span class="op">=</span> <span class="va">N_pred</span><span class="op">)</span></span></code></pre>
</div>
<p>Next we’ll build the Stan program. The model structure is simple; the
model block defines the likelihood as the normal distribution with an
unknown mean: <code>y ~ normal(f[1:N_data], sigma);</code>. Notice that
this is a vectorized statement so the mean of each <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(f_i\)</span>.</p>
<p>The parameter vector <code>f</code> contains the values of <span class="math inline">\(f\)</span> corresponding to the data points and
then concatenated the values corresponding to the prediction locations.
The covariance function is computed in the transformed data block, where
first a vector of concatenated data and prediction locations is
build.</p>
<p>Take a moment to digest the structure of the Stan program.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>  <span class="co">// Data</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_data;</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>  <span class="dt">real</span> y[N_data];</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>  <span class="dt">real</span> x_data[N_data];</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>  </span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>  <span class="co">// GP hyperparameters</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; lambda;</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>  </span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>  <span class="co">// Observation erro</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>  </span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>  <span class="co">// Prediction points</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_pred;</span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a>  <span class="dt">real</span> x_pred[N_pred];</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>}</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N_data + N_pred;</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>  </span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>  <span class="dt">real</span> x[N];</span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a>  </span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a>  x[<span class="dv">1</span>:N_data] = x_data;</span>
<span id="cb3-26"><a href="#cb3-26" tabindex="-1"></a>  x[(N_data+<span class="dv">1</span>):N] = x_pred;</span>
<span id="cb3-27"><a href="#cb3-27" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" tabindex="-1"></a>  <span class="co">// Covariance function</span></span>
<span id="cb3-29"><a href="#cb3-29" tabindex="-1"></a>  K = gp_exp_quad_cov(x, alpha, lambda);</span>
<span id="cb3-30"><a href="#cb3-30" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" tabindex="-1"></a>  <span class="co">// Add nugget on diagonal for numerical stability</span></span>
<span id="cb3-32"><a href="#cb3-32" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb3-33"><a href="#cb3-33" tabindex="-1"></a>    K[n, n] = K[n, n] + <span class="fl">1e-6</span>;</span>
<span id="cb3-34"><a href="#cb3-34" tabindex="-1"></a>  }</span>
<span id="cb3-35"><a href="#cb3-35" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" tabindex="-1"></a>}</span>
<span id="cb3-37"><a href="#cb3-37" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb3-38"><a href="#cb3-38" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb3-39"><a href="#cb3-39" tabindex="-1"></a>}</span>
<span id="cb3-40"><a href="#cb3-40" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb3-41"><a href="#cb3-41" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb3-42"><a href="#cb3-42" tabindex="-1"></a>  y ~ normal(f[<span class="dv">1</span>:N_data], sigma);</span>
<span id="cb3-43"><a href="#cb3-43" tabindex="-1"></a>  <span class="co">// GP</span></span>
<span id="cb3-44"><a href="#cb3-44" tabindex="-1"></a>  f ~ multi_normal(rep_vector(<span class="dv">0</span>, N), K);</span>
<span id="cb3-45"><a href="#cb3-45" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Let’s fit the model.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit</span></span>
<span><span class="va">gp_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">gp_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N_data <span class="op">=</span> <span class="va">N</span>,</span>
<span>                            x_data <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>,</span>
<span>                            y <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                            lambda <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            alpha <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            sigma <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                            N_pred <span class="op">=</span> <span class="va">N_pred</span>,</span>
<span>                            x_pred <span class="op">=</span> <span class="va">x_pred</span><span class="op">)</span>,</span>
<span>                       chains <span class="op">=</span> <span class="fl">1</span>, iter <span class="op">=</span> <span class="fl">1000</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: There were 322 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: The largest R-hat is 2.1, indicating chains have not mixed.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#r-hat</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<p>The inference takes time (minutes) even though we only use (an
insufficient) single chain and 1000 iterations. There are also some
convergence issues. Let’s ignore these at this point, and look at the
output.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">gp_samples</span>, <span class="st">"f"</span><span class="op">)</span><span class="op">[[</span><span class="st">"f"</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">t</span> <span class="op">%&gt;%</span> <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span>, <span class="va">x_pred</span><span class="op">)</span><span class="op">)</span> <span class="co"># data and prediction locations</span></span>
<span></span>
<span><span class="va">f_samples_l</span> <span class="op">&lt;-</span> <span class="va">f_samples</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"sample"</span>, value <span class="op">=</span> <span class="st">"f"</span>, <span class="op">-</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_f</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">f_samples_l</span>,</span>
<span>    <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">f</span>, group <span class="op">=</span> <span class="va">sample</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">df</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span><span class="st">"red"</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_f</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/gaussian-processes-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The figure contains the data points in red and samples from the
posterior distribution of <span class="math inline">\(f\)</span> in
black. Note that each posterior sample corresponds to a function. This
distribution essentially encapsulates the model’s interpretation of the
underlying trend within the data. The estimate for the trend seems
plausible. The posterior in certain regions, however, seems a bit
strange. For example, the posterior between the farthest two data points
on the right contains most of the mass above the line connecting the
points. This is likely a symptom of the convergence issues.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>In the figure above, where is the posterior uncertainty the highest
and why? What controls the uncertainty at the locations of the data? If
the made the prediction range wider, say, from -10 to 10, what would
look like at the extremes?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Uncertainty grows at locations away from the data points and starts
to resemble the prior. Posterior far from the data would be centered
around 0 and have variance <span class="math inline">\(\alpha^2\)</span>.</p>
</div>
</div>
</div>
</div>
</section><section id="cholesky-parameterization"><h2 class="section-heading">Cholesky parameterization<a class="anchor" aria-label="anchor" href="#cholesky-parameterization"></a>
</h2>
<hr class="half-width">
<p>It is generally recommended, that GPs are used with the Cholesky
parameterization. This considerable improves in inference and enhanced
convergence. In essence, this parameterization suggests that if <span class="math inline">\(\eta\)</span> follows a multivariate normal
distribution with zero mean and identity covariance, then expressing
<span class="math inline">\(f\)</span> as <span class="math inline">\(f
= \mu + L\eta\)</span> defines a GP with mean <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(LL^T = K\)</span>. The left-hand side of the
formula, <span class="math inline">\(LL^T\)</span> is the Cholesky
decomposition of <span class="math inline">\(K\)</span>.</p>
<p>The Stan program below implements this parameterization. Let’s
compile and fit this model on the same data.</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>  <span class="co">// Data</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_data;</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>  <span class="dt">real</span> y[N_data];</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>  <span class="dt">real</span> x_data[N_data];</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>  </span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>  <span class="co">// GP hyperparameters</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; lambda;</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>  </span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>  </span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>  <span class="co">// Prediction points</span></span>
<span id="cb11-14"><a href="#cb11-14" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_pred;</span>
<span id="cb11-15"><a href="#cb11-15" tabindex="-1"></a>  <span class="dt">real</span> x_pred[N_pred];</span>
<span id="cb11-16"><a href="#cb11-16" tabindex="-1"></a>}</span>
<span id="cb11-17"><a href="#cb11-17" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb11-18"><a href="#cb11-18" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N_data + N_pred;</span>
<span id="cb11-19"><a href="#cb11-19" tabindex="-1"></a>  </span>
<span id="cb11-20"><a href="#cb11-20" tabindex="-1"></a>  <span class="dt">real</span> x[N];</span>
<span id="cb11-21"><a href="#cb11-21" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb11-22"><a href="#cb11-22" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] L;</span>
<span id="cb11-23"><a href="#cb11-23" tabindex="-1"></a>  </span>
<span id="cb11-24"><a href="#cb11-24" tabindex="-1"></a>  x[<span class="dv">1</span>:N_data] = x_data;</span>
<span id="cb11-25"><a href="#cb11-25" tabindex="-1"></a>  x[(N_data+<span class="dv">1</span>):N] = x_pred;</span>
<span id="cb11-26"><a href="#cb11-26" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" tabindex="-1"></a>  <span class="co">// Covariance function</span></span>
<span id="cb11-28"><a href="#cb11-28" tabindex="-1"></a>  K = gp_exp_quad_cov(x, alpha, lambda);</span>
<span id="cb11-29"><a href="#cb11-29" tabindex="-1"></a>  </span>
<span id="cb11-30"><a href="#cb11-30" tabindex="-1"></a>  <span class="co">// Add nugget on diagonal for numerical stability</span></span>
<span id="cb11-31"><a href="#cb11-31" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb11-32"><a href="#cb11-32" tabindex="-1"></a>    K[n, n] = K[n, n] + <span class="fl">1e-6</span>;</span>
<span id="cb11-33"><a href="#cb11-33" tabindex="-1"></a>  }</span>
<span id="cb11-34"><a href="#cb11-34" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" tabindex="-1"></a>  L = cholesky_decompose(K);</span>
<span id="cb11-36"><a href="#cb11-36" tabindex="-1"></a>}</span>
<span id="cb11-37"><a href="#cb11-37" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb11-39"><a href="#cb11-39" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb11-40"><a href="#cb11-40" tabindex="-1"></a>}</span>
<span id="cb11-41"><a href="#cb11-41" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb11-43"><a href="#cb11-43" tabindex="-1"></a>  <span class="co">// mu = (0, 0, ..., 0)</span></span>
<span id="cb11-44"><a href="#cb11-44" tabindex="-1"></a>  <span class="dt">vector</span>[N] f = L*eta;</span>
<span id="cb11-45"><a href="#cb11-45" tabindex="-1"></a>}</span>
<span id="cb11-46"><a href="#cb11-46" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb11-47"><a href="#cb11-47" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb11-48"><a href="#cb11-48" tabindex="-1"></a>  y ~ normal(f[<span class="dv">1</span>:N_data], sigma);</span>
<span id="cb11-49"><a href="#cb11-49" tabindex="-1"></a>  <span class="co">// GP</span></span>
<span id="cb11-50"><a href="#cb11-50" tabindex="-1"></a>  eta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb11-51"><a href="#cb11-51" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Fitting is completed in a few seconds with no convergence issues:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gp_cholesky_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">gp_cholesky_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N_data <span class="op">=</span> <span class="va">N</span>,</span>
<span>                            x_data <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>,</span>
<span>                            y <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                            lambda <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            alpha <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            sigma <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                            N_pred <span class="op">=</span> <span class="va">N_pred</span>,</span>
<span>                            x_pred <span class="op">=</span> <span class="va">x_pred</span><span class="op">)</span>,</span>
<span>                       chains <span class="op">=</span> <span class="fl">1</span>, iter <span class="op">=</span> <span class="fl">2000</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s check the results</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_cholesky_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">gp_cholesky_samples</span>, <span class="st">"f"</span><span class="op">)</span><span class="op">[[</span><span class="st">"f"</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">t</span> <span class="op">%&gt;%</span> <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span>, <span class="va">x_pred</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_cholesky_samples_l</span> <span class="op">&lt;-</span> <span class="va">f_cholesky_samples</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"sample"</span>, value <span class="op">=</span> <span class="st">"f"</span>, <span class="op">-</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_cholesky_f</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">f_cholesky_samples_l</span>,</span>
<span>    <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">f</span>, group <span class="op">=</span> <span class="va">sample</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">df</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span><span class="st">"red"</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_cholesky_f</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="fig/gaussian-processes-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="discussion2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion2"></a>
</h3>
<div class="callout-content">
<p>The logistic regression models a binary variable <span class="math inline">\(y \in \{0, 1\}\)</span> as a function of <span class="math inline">\(x \in \mathbb{R}\)</span> as follows:</p>
<p><span class="math display">\[ y \sim \text{Bernoulli}(\theta) \\
\theta = \frac{1}{1 + e^{-(\alpha + \beta x)}},\]</span> where <span class="math inline">\(\alpha, \beta\)</span> are real numbers and <span class="math inline">\(\theta\)</span> is the probability of <span class="math inline">\(y = 1\)</span>.</p>
<p>The model can be used to estimate, for instance, the probability of
passing an exam (<span class="math inline">\(y=1\)</span>) based on the
number of study hours <span class="math inline">\(x\)</span>.
Presumably, low study hours give low probability for passing. On the
other hand, too much study may induce exhaustion and compromise
performance.</p>
<p>Using the following data, estimate the probability of passing as a
function of study hours.</p>
<p>FIXME data</p>
<p>Modify the logistic regression so that the term <span class="math inline">\(\beta x\)</span> is replaced with a non-parametric
function <span class="math inline">\(f\)</span>. Give <span class="math inline">\(f\)</span> a GP prior with squared exponential
covariance function and appropriate hyperparameters. Use a normal prior
<span class="math inline">\(N(0, 1)\)</span> for the baseline parameter
<span class="math inline">\(\alpha\)</span>.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-other-topics"><p>Content from <a href="other-topics.html">Other topics</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/other-topics.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What next?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn more</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Add text</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>You can define a challenge here</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>… and a solution here</p>
</div>
</div>
</div>
</div>
<p>More text</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Discussion poitns</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div></section><section id="aio-exercises"><p>Content from <a href="exercises.html">Exercises</a></p>
<hr>
<p>Last updated on 2024-04-23 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/exercises.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I get routine in probabilistic programming?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>The purpose of this Episodede is to provide material for practicing
probabilistic programming. The exercises are listed approximately based
on the Episodede they refer to.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="basics">1. Basics<a class="anchor" aria-label="anchor" href="#basics"></a>
</h1>
<div class="section level2">
<h2 id="grid-approximation-normal-model-with-unknown-mean">1.1 Grid approximation: normal model with unknown mean<a class="anchor" aria-label="anchor" href="#grid-approximation-normal-model-with-unknown-mean"></a>
</h2>
<p>Generate 1000 data points from the normal model. Use a randomly
generated mean parameter, <span class="math inline">\(\mu \sim
N(0,1)\)</span> and set the standard deviation <span class="math inline">\(\sigma=1\)</span>.</p>
<p>The grid approximation for this model was introduced in Episodede XX
but the implementation doesn’t work for the generated data. Locate the
source of error and make the necessary modifications to get the program
working.</p>
<p>Plot the posterior of <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="section level2">
<h2 id="grid-approximation-gamma-poisson">1.2 Grid approximation: Gamma-Poisson<a class="anchor" aria-label="anchor" href="#grid-approximation-gamma-poisson"></a>
</h2>
<p>The Gamma-Poisson model can be stated as:</p>
<p><span class="math display">\[y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, 1), \]</span></p>
<p>where <span class="math inline">\(y_i\)</span> are non-negative data
points, and Pois is the Poisson distribution with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span>. Implement a grid
approximation for this model.</p>
<p>Apparently (<a href="https://en.wikipedia.org/wiki/Poisson_distribution" class="external-link uri">https://en.wikipedia.org/wiki/Poisson_distribution</a>), the
number of chewing gum on a sidewalk tile is approximately Poisson
distributed.</p>
<p>Estimate the average number of gum on a Reykjavik side walk tile
(lambda), using the data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="less-data-means-bigger-prior-effect">1.3 Less data means bigger prior effect<a class="anchor" aria-label="anchor" href="#less-data-means-bigger-prior-effect"></a>
</h2>
<p>Show that as the amount of available data increases, the effect of
the prior decreases.</p>
<p>Instructions: - Simulate a series of coin tosses: - Generate <span class="math inline">\(p \sim \text{Uniform}(0, 1)\)</span>. - Simulate a
sequence of 50 tosses with Pr(heads) = p.  - Fit the grid approximation
using the first 1, 5, 10, 15,…, 50 tosses. - Use a Beta prior for p. -
Compare the posteriors the prior.</p>
</div>
<div class="section level2">
<h2 id="grid-approximation-for-a-normal-model-with-unknown-mean-and-standard-deviation">1.4 Grid approximation: for a normal model with unknown mean and
standard deviation<a class="anchor" aria-label="anchor" href="#grid-approximation-for-a-normal-model-with-unknown-mean-and-standard-deviation"></a>
</h2>
<p>The following data is a collection of daily milk yield (in liters)
for dairy cows.</p>
<p><span class="math inline">\(X = {30.25, 34.98, 29.66, 20.14, 23.92,
38.61, 36.89, 34.68, 25.83, 29.93}.\)</span></p>
<p>Using the grid approximation for the normal model, estimate the
average daily yield <span class="math inline">\(\mu\)</span>.</p>
<p>Use some non-uniform priors.</p>
<p>Plot the marginal posterior for <span class="math inline">\(\mu\)</span> and compute the 90% credible interval
for the marginal.</p>
<p>What is the probability that the average daily milk yield is more
than 30 liters?</p>
</div>
<div class="section level2">
<h2 id="sampling-the-gamma">1.5 Sampling the Gamma<a class="anchor" aria-label="anchor" href="#sampling-the-gamma"></a>
</h2>
<p>Let’s model the following observations X with the exponential
likelihood, <span class="math inline">\(\text{Exp}(\lambda)\)</span>:</p>
<p><span class="math display">\[X = \{0.166, 1.08, 1.875, 0.413, 1.369,
0.463, 0.735,
       0.24, 0.774, 1.09, 0.463, 0.916, 0.225, \\
        0.889, 0.051, 0.688, 0.119, 0.078, 1.624, 0.553, 0.523,
       0.644, 0.284, 1.744, 1.468\}.\]</span></p>
<p>If we use a <span class="math inline">\(\Gamma(2, 1)\)</span> prior,
the posterior distribution can be shown to be</p>
<p><span class="math inline">\(p(\lambda | X) = \Gamma(2 + n, 1 + X_1 +
X_2 + ... + X_n),\)</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of
observations.</p>
<p>Generate 5000 samples from the posterior and compute 1. the posterior
mean and mode 2. the 50% and 95% credible intervals 3. the probabilities
<span class="math inline">\(Pr(\lambda &gt; 1), Pr( 1 &lt; \lambda &lt;
1.5), Pr(\lambda &lt; 1 \text{  or } \lambda &gt; 1.5)\)</span></p>
</div>
<div class="section level2">
<h2 id="grid-approximation-cauchy-distribution">1.6 Grid approximation: Cauchy distribution<a class="anchor" aria-label="anchor" href="#grid-approximation-cauchy-distribution"></a>
</h2>
<p>(Emulated from BDA3: p59. Ex.11)</p>
<p>Suppose y1,…,y5 are independent samples from a Cauchy distribution
with scale 1 and unknown location <span class="math inline">\(\theta\)</span>. Given the observations <span class="math inline">\((y_1, . . . , y_5) = (43, 44, 45, 46.5,
47.5)\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>Compute the unnormalized posterior density function, <span class="math inline">\(p(\theta)p(y|\theta)\)</span>, on a grid of points
<span class="math inline">\(\theta = 0, 1/m , 2/m ,..., 100\)</span>,
for some large integer <span class="math inline">\(m\)</span>. Using the
grid approximation, compute and plot the normalized posterior density
function as a function of <span class="math inline">\(\theta\)</span>.
Assume a uniform prior for over [0, 100].</li>
<li>Generate 1000 samples of <span class="math inline">\(\theta\)</span>
from the posterior and plot a histogram of the samples.</li>
<li>Use each of the samples generated in b) to generate a new data point
<span class="math inline">\(y_6\)</span> from the likelihood function
and plot a histogram of these draws.</li>
</ol>
</div>
<div class="section level2">
<h2 id="sampling-the-normal">1.7 Sampling the Normal<a class="anchor" aria-label="anchor" href="#sampling-the-normal"></a>
</h2>
<p>The posterior for the normal model with unknown <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> can be sampled as follows
(BDA3:p.65):</p>
<ol style="list-style-type: decimal">
<li>Sample the variance from <span class="math inline">\(\sigma^2 \sim
Inv- \chi^2(n-1, \text{Var}(X))\)</span>
</li>
<li>Sample the mean from <span class="math inline">\(\mu | \sigma^2 ~
N(\bar{X}, \frac{\sigma^2}{n})\)</span>
</li>
</ol>
<p>Here <span class="math inline">\(\bar{X}\)</span> is the mean of the
data points <span class="math inline">\(X = \{X_1, X_2, ...,
X_n\}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Generate 5000 samples from the posterior using the data</li>
</ol>
<p><span class="math inline">\(X = \{21.1, 20.8, 21.9, 20.5, 18.7, 24.1,
18.6, 15.4, 16.9, 20.8\}\)</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compute the posterior for the coefficient of variation <span class="math inline">\(CV = \frac{\sigma}{\mu}\)</span>.</li>
<li>Generate samples for an unseen data point <span class="math inline">\(\tilde{X}\)</span>.</li>
<li>Compare the distribution in c) to one generated using only the MAP
estimate. Is there a discrepancy and why?</li>
</ol>
</div>
<div class="section level2">
<h2 id="hpdi">1.8 HPDI<a class="anchor" aria-label="anchor" href="#hpdi"></a>
</h2>
<p>Another approach to summarizing the posterior is to compute the
<em>shortest</em> interval that contains <span class="math inline">\(p\%\)</span> of the posterior. Such interval is
called highest posterior density interval (HPDI).</p>
<p>Write a function that returns the highest posterior density interval,
given a set of posterior samples.</p>
<p>Assume that the analytical form of a posterior is known to be
Gamma(3, 20). Generate samples from this distribution and compute the
90% HPDI. Compare it to the 90% credible interval.</p>
<p>Hint: If you sort the posterior samples in order, each set of n
consecutive samples contains <span class="math inline">\(100 \cdot
\frac{n}{N} \%\)</span> of the posterior, where <span class="math inline">\(N\)</span> is the total number of samples.</p>
<!-- ************************************************************************ -->
</div>
</div>
<div class="section level1">
<h1 id="stan">2. Stan<a class="anchor" aria-label="anchor" href="#stan"></a>
</h1>
<p>2.1 Gamma-Poisson model</p>
<p>The Gamma-Poisson model can be stated as:</p>
<p><span class="math display">\[y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, 1), \]</span></p>
<p>where <span class="math inline">\(y_i\)</span> are non-negative data
points, and Pois is the Poisson distribution with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<p>Implement the model in Stan. Do it so that the parameter beta is
input as part of data. Heuristically, what effect does altering beta
have on the prior shape?</p>
<p>Fit the model using the data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
<p>Compute the MAP, mean and 90% CIs and include them in a figure with a
histogram of posterior samples.</p>
<p>2.2 Dice</p>
<p>Write a Stan program that implements the following statistical
model:</p>
<p><span class="math display">\[y \sim \text{categorial}(\theta) \\
\theta \sim \text{Dir}(1, 1, ..., 1), \]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is a 6-dimensional
probability vector and Dir is the Dirichlet distribution.</p>
<p>Use the program to assess the fairness of a 6 sided dice. In other
words, estimate the probabilities of each side on a roll using the
following data.</p>
<p>$X = {3, 2, 6, 3, 6, 2, 5, 6, 5, 6, 4, 1, 4, 2, \ 5, 4, 6, 6, 5, 4,
1, 3, 3, 4, 2, 3, 4, 4, 4, 1, 1, \ 3, 4, 4, 1, 6, 4, 6, 5, 5, 2, 6, 1,
1, 4, 4, 1, 6, \ 6, 1, 6, 4, 5, 5, 3, 4, 2, 6, 6, 5, 2, 6, 1, 1, 4, \ 4,
4, 6, 3, 5, 3, 6, 5, 3, 3, 2, 3, 3, 5, 3, 3, 4, \ 6, 4, 3, 6, 6, 4, 4,
6, 5, 1, 3, 5, 1, 2, 4, 4, 1 } $</p>
<ol style="list-style-type: decimal">
<li><p>Plot the marginal posteriors for each <span class="math inline">\(\theta_i\)</span>.</p></li>
<li><p>Is the dice fair? Quantify this somehow</p></li>
</ol>
<p>Hint: You can e.g. compute a posterior probability difference of some
of the dice faces.</p>
<p>2.3 Normal model</p>
<p>Implement the Normal model in Stan.</p>
<p>In the generated quantities block: - Generate a posterior predictive
distribution - Generate posterior for <span class="math inline">\(CV =
\frac{\mu}{\sigma}.\)</span></p>
<p>Fit the model using the data <span class="math inline">\(X = \{3.7,
2.8, 4.03, 2.11, 2.58, 0.96, 1.74, 0.34, 0.75, 2.07\}\)</span>. Plot the
posterior distribution and color the points according to the condition
<span class="math inline">\(CV &lt; 1\)</span>.</p>
<p>2.4. Time series modeling</p>
<p>The AR(1) process is defined by the recursion: <span class="math display">\[x_i \sim N(\phi \cdot x_{i-1},
\sigma^2),\]</span> where <span class="math inline">\(i\)</span> is a
time index. In other words, given some initial value <span class="math inline">\(x_1\)</span>, the next value <span class="math inline">\(x_2\)</span> is generated from a normal
distribution with mean <span class="math inline">\(\phi\cdot
x_1\)</span> and variance <span class="math inline">\(sigma^2\)</span>.
This pattern continues for the successive values.</p>
<p>Write a Stan program that estimates the parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma\)</span> of the AR(1) process.</p>
<p>Using the data in <code>data/time_series.txt</code>, do the
following:</p>
<ol style="list-style-type: decimal">
<li>Estimate the parameters <span class="math inline">\(\phi\)</span>
and <span class="math inline">\(\sigma\)</span>.</li>
<li>Starting from <span class="math inline">\(x_{new} = 5\)</span>
predict the process 50 values into the future and plot the
predictions.</li>
</ol>
</div>
<div class="section level1">
<h1 id="mcmc">3. MCMC<a class="anchor" aria-label="anchor" href="#mcmc"></a>
</h1>
<div class="section level2">
<h2 id="binomial-model">3.1 Binomial model<a class="anchor" aria-label="anchor" href="#binomial-model"></a>
</h2>
<p>Implement the Metropolis-Hastings algorithm for the beta-binomial
model.</p>
<p>Assume there are 50 people of whom 7 are left-handed. Estimate the
probability of being left-handed in the wider population.</p>
<p>Use <span class="math inline">\(p(\theta^* | \theta_{now}) =
\text{Beta}(\theta^* | 2, 2)\)</span> as your proposal distribution,
where <span class="math inline">\(\theta^*\)</span> is the proposal and
<span class="math inline">\(\theta_{now}\)</span> the current
sample.</p>
<p>Compute the proportion of accepted proposals for the sampler.</p>
</div>
<div class="section level2">
<h2 id="gibbs-sampler">3.2. Gibbs sampler<a class="anchor" aria-label="anchor" href="#gibbs-sampler"></a>
</h2>
<p>Consider the distribution <span class="math display">\[ p(x, y) = C
\cdot \exp^{-(x^2y^2 + x^2 + y^2 -8x -8y)/2}\]</span>, where <span class="math inline">\(C\)</span> is a normalizing constant. It is known
that the conditional distribution of x given y is the normal
distribution <span class="math inline">\(p(x|y) = N(\mu,
\sigma^2)\)</span>, where the mean <span class="math inline">\(\mu =
4/(1 + y^2)\)</span> and the standard deviation <span class="math inline">\(\sigma = \sqrt{1/(1 + y^2)}\)</span>. Due to
symmetry, <span class="math inline">\(p(y|x)\)</span> can be recovered
simply by changing <span class="math inline">\(y\)</span>’s to <span class="math inline">\(x\)</span>’s in <span class="math inline">\(p(x|y)\)</span>.</p>
<p>The Gibbs sampler is a special case of the Metropolis-Hastings
algorithm. It can be stated as follows:</p>
<ol style="list-style-type: decimal">
<li>Choose some initial values for the parameters, <span class="math inline">\(x_0\)</span> and <span class="math inline">\(y_0\)</span> in our case.</li>
<li>For <span class="math inline">\(i = 1, ..., N\)</span> do:
<ol style="list-style-type: lower-alpha">
<li>Draw <span class="math inline">\(x_i\)</span> from <span class="math inline">\(p(x_i|y_{i-1})\)</span>
</li>
<li>Draw <span class="math inline">\(y_i\)</span> from <span class="math inline">\(p(y_i|x_i)\)</span>
</li>
</ol>
</li>
</ol>
<p>(In this notation <span class="math inline">\(x_i\)</span> refers to
the <span class="math inline">\(x\)</span> sample at step <span class="math inline">\(i\)</span>, <span class="math inline">\(y_{i-1}\)</span> to the y sample at step <span class="math inline">\(i-1\)</span> and so on)</p>
<p>Build a Gibbs sampler that draws samples from <span class="math inline">\(p(x, y)\)</span>. Visualize the resulting
distribution.</p>
</div>
<div class="section level2">
<h2 id="gamma-with-discrete-rate">3.3. Gamma with discrete rate<a class="anchor" aria-label="anchor" href="#gamma-with-discrete-rate"></a>
</h2>
<p>The data <span class="math inline">\(X = \{10.61, 4.76, 11.25, 5.55,
23.81, 7.45, 17.31, 15.08, \\8.19, 15, 4.29, 10.95, 15.45, 10.09, 7.96,
12.35, \\ 11.43, 7.33, 8.17, 21\}\)</span></p>
<p>is modelled with a <span class="math inline">\(\Gamma(\alpha,
\beta)\)</span> distribution, where the shape <span class="math inline">\(\alpha = 5\)</span>, but the rate <span class="math inline">\(\beta\)</span> is unknown. However, it is known
that beta can only take values <span class="math inline">\(1.1^N\)</span>, where <span class="math inline">\(N\)</span> is an integer.</p>
<p>Implement a Metropolis-Hastings sampler for this model.</p>
<p>Use a proposal distribution that gives equal probability for moving
one step up <span class="math inline">\((N^* = N+1)\)</span> or down
<span class="math inline">\((N^* = N-1)\)</span>, and some positive
probability for staying still <span class="math inline">\((N^*=N)\)</span>.</p>
<p>Generate the initial value randomly: <span class="math inline">\(N_0
\sim \text{Uniform}(-100, 100)\)</span> and compute an estimate for
<span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div class="section level1">
<h1 id="hierarchical-models">4. Hierarchical models<a class="anchor" aria-label="anchor" href="#hierarchical-models"></a>
</h1>
<div class="section level2">
<h2 id="model-analysis">4.1. Model analysis<a class="anchor" aria-label="anchor" href="#model-analysis"></a>
</h2>
<p>Examine the following statistical models. Determine if they exhibit a
hierarchical structure. If not, introduce modifications to make them
hierarchical.</p>
<p>Below, the subscript <span class="math inline">\(i\)</span> denotes a
data point, while <span class="math inline">\(g\)</span> designates a
population subgroup.</p>
<ol style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
y_{g, i} \sim N(a_g + b \cdot x_{g,i}, \sigma^2) \\
a_g \sim N(0, 1) \\
b \sim N(0, 1)  \\
\sigma \sim \text{Exponential}(1) \\
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
y_{g,i} \sim \text{Binom}(N, p_{g,i})  \\
\text{logit}(p_{g,i}) = a_g + b \cdot x_i  \\
a_{g} \sim N(\alpha, 1)   \\
\alpha \sim N(0, 10)   \\
b \sim N(0, 1)
\]</span></p>
<ol start="3" style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
X_i \sim \Gamma(\alpha, \beta)   \\
\alpha \sim \Gamma(1, 1)   \\
\beta \sim \Gamma(1, 1)
\]</span></p>
<p>4.2. Hierarchical Gamma-Poisson</p>
<p>A hierarchical Gamma-Poisson model can be stated as follows:</p>
<p><span class="math display">\[
y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, \beta) \\
\beta \sim \Gamma(2, 1) \\
\]</span></p>
<p>Use data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
<p>Implement the model in Stan.</p>
<p>Identify the data subgroups. Visualize the posterior distribution of
beta. Generate samples from the population distribution of <span class="math inline">\(\lambda\)</span> in the generated quantities block
and plot them in a histogram.</p>
<p>4.3. Hierarchical Poisson regression</p>
<p>A Poisson regression model can be specified as <span class="math inline">\(y_i \sim \text{Pois}(\exp^{\alpha + \beta
x_i})\)</span>, where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are corresponding data points.</p>
<p>Apply Poisson regression to model the annual rental count <span class="math inline">\((y_i)\)</span> for homestay apartments located at
a distance <span class="math inline">\(x_i\)</span> from the city
center.</p>
<p>The data comprises 25 randomly selected apartments in 10 different
cities. The file <code>data/apartment_x.txt</code> contains the
distances of apartments to the center, with rows representing cities and
columns representing apartments. The file
<code>data/apartment_y.txt</code> contains the number of rentals during
the surveyed year for the same apartments.</p>
<p>Build a Stan program for hierarchical Poisson regression, with
hierarchical structure for both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>Generate and visualize posteriors for the population distributions of
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. What is the probability that <span class="math inline">\(\beta &gt; 0\)</span> in the population? What
implications does <span class="math inline">\(\beta &gt; 0\)</span> have
in terms of the application?</p>
</div>
<div class="section level2">
<h2 id="hierarchical-binomial-model">4.4. Hierarchical binomial model<a class="anchor" aria-label="anchor" href="#hierarchical-binomial-model"></a>
</h2>
<p>As commonly acknowledged, multiple humanoid species inhabit various
solar systems within the Milky Way galaxy.</p>
<p>The file, <code>data/handedness.txt</code>, contains data on the
handedness of some of these species, with <span class="math inline">\(N\)</span> representing the sample size and <span class="math inline">\(x\)</span> denoting the count of left-handed
specimens. The objective is to estimate the prevalence of
left-handedness, <span class="math inline">\(\theta\)</span>.</p>
<p>Develop Stan programs for both unpooled and partially pooled binomial
models. Utilize the unpooled model to fit the completely pooled
model.</p>
<p>Incorporate Beta priors for <span class="math inline">\(\theta\)</span>. Compare the estimates, such as
means, derived from the distinct pooling strategies.</p>
</div>
</div>
<div class="section level1">
<h1 id="model-critisism">5. Model critisism<a class="anchor" aria-label="anchor" href="#model-critisism"></a>
</h1>
<div class="section level2">
<h2 id="prior-predictive-check">5.1 Prior predictive check<a class="anchor" aria-label="anchor" href="#prior-predictive-check"></a>
</h2>
<p>In a salmon farm research facility, the relationship between the
length of salmons (in meters, <span class="math inline">\(y\)</span>)
and the amount of food provided (in grams, <span class="math inline">\(x\)</span>) is studied. The amount of food
administered in 21 salmon pools is meticulously controlled, ranging from
40 to 60 grams per individual salmon in one-gram increments.</p>
<p>The chosen statistical model is linear regression:</p>
<p><span class="math display">\[
y \sim N(a + bx, \sigma^2) \\
a, b \sim N(0, 1) \\
\sigma \sim \Gamma(2, 1) \\
\]</span></p>
<p>Generate the prior predictive distribution, plot it, and visually
assess the validity of the priors.</p>
</div>
<div class="section level2">
<h2 id="posterior-predictive-check">5.2. Posterior predictive check<a class="anchor" aria-label="anchor" href="#posterior-predictive-check"></a>
</h2>
<p>The white noise process is perhaps the simplest non-trivial time
series model. If <span class="math inline">\(i\)</span> is the time
index, then the model can be specified as <span class="math inline">\(x_i ~ N(0, sigma^2)\)</span> for all <span class="math inline">\(i = 1, \ldots ,N\)</span>.</p>
<p>The file <code>data/white_noise.txt</code> contains a time series.
Plot the data.</p>
<p>Build a Stan program for the white noise model and use it on the
provided data.</p>
<p>Do a posterior predictive check by use lag-1 autocorrelation as the
test statistic. Compute the posterior predictive p-value.</p>
<p>Hint: lag-1 autocorrelation is simply the Pearson correlation between
<span class="math inline">\(x_{1:(N-1)}\)</span> and <span class="math inline">\(x_{2:N}\)</span></p>
</div>
</div>
<div class="section level1">
<h1 id="gaussian-processes">6. Gaussian processes<a class="anchor" aria-label="anchor" href="#gaussian-processes"></a>
</h1>
<p>6.1. GP prediction</p>
<p>Consider the following data <span class="math inline">\((x,
y)\)</span>:</p>
<p><span class="math inline">\(x = \{6.86, -7.88, 1.59, 5.95, -2.55,
-1.96, 3.77, -6.74, -6.83, 8.42, -4.95, -6.29, \\ 9.58, -1.95, 7.6,
1.97, 7.75, 8.34, 9.22, -6.31, 1.73, 9.58, 6.86, 1.46, -6.7, -9.9, \\
6.81, -6.38, 3.52, -4.36, -3.46, 7.7, -7.63, 3.51, 5.57, 3.2, 2.04,
6.33, -7.84, -2.85, \\ -7.86, -5.14, -6.18, -9.35, -5.59, -6.86, 4.2,
8.83, 8.04 \}\)</span></p>
<p><span class="math inline">\(y = \{2.95, -6.7, 0.8, -1.47, -0.03,
-0.26, -1.03, -2.8, -3.01, 6.75, 1.68, -0.93, \\ -0.88, -1.03, 6.88,
-0.1, 7.32, 7.59, 3.09, -0.44, 0.69, -0.27, 3.45, 0.46, -2.49, 3.33, \\
3.28, -1.48, -0.11, 1.76, 0.25, 6.01, -6.53, 0.62, -1.01, 0.26, 1.31,
0.47, -6.7, \\ -0.29, -6.09, 2.03, -0.22, -1.24, 1.46, -3.73, -0.87,
5.27, 6.59\}\)</span></p>
<p>Model the data as <span class="math inline">\(y \sim N(f(x),
\sigma^2)\)</span> and give <span class="math inline">\(f(x)\)</span> a
Gaussian process prior. Use the squared exponential covariance function
with hyperparameters <span class="math inline">\(\lambda = 1\)</span>
(length-scale), <span class="math inline">\(\alpha = 1\)</span>
(standard deviation), <span class="math inline">\(\sigma =
0.5\)</span>.</p>
<p>You can use the Stan programs in Episodede XX as a starting
point.</p>
<p>Plot the posterior for <span class="math inline">\(f\)</span> along
with the data, and compute the posterior probability for <span class="math inline">\(f(0) &gt; 0\)</span>.</p>
<div class="section level2">
<h2 id="gp-prediction">6.2. GP prediction<a class="anchor" aria-label="anchor" href="#gp-prediction"></a>
</h2>
<p>The file <code>data/nytemp.txt</code> contains daily maximum
temperatures (<span class="math inline">\(Temp\)</span>) in New York
from May to September 1973 [1]. The column <span class="math inline">\(x\)</span> gives the day number for the date (Jan
1st is day number 1 etc).</p>
<p>Do Gaussian process regression on the data and estimate the
temperature trend for the year 1973. Use a periodic covariance kernel
with <span class="math inline">\(\alpha = 100\)</span> and <span class="math inline">\(\lambda = 10\)</span>, and set a suitable value
for the period. Set <span class="math inline">\(\sigma\)</span>
(deviance from the trend) as an unknown parameter in Stan.</p>
<p>Plot the data with the posterior for temperature. Plot the marginal
posterior for temperature for the last day of the year and compare it
against the true value (which was 38F).</p>
<p>Hint:</p>
<p>See <a href="https://mc-stan.org/docs/functions-reference/gaussian-process-covariance-functions.html" class="external-link uri">https://mc-stan.org/docs/functions-reference/gaussian-process-covariance-functions.html</a>
for info on periodic kernel in Stan.</p>
<p>[1] (Chambers, J. M., Cleveland et al. (1983) Graphical Methods for
Data Analysis)</p>
</div>
<div class="section level2">
<h2 id="gp-prediction-1">6.3. GP prediction<a class="anchor" aria-label="anchor" href="#gp-prediction-1"></a>
</h2>
<p>The data file <code>data/stock.txt</code> contains the (scaled and
transformed) stock price of a company over 250 days.</p>
<p>Implement the following model and use it to predict the stock 150
into the future.</p>
<p><span class="math display">\[
y \sim N(f, \sigma^2) \\
f = f_{trend} + f_{noise} \\
f_{trend} \sim GP(0, K_1(\alpha_1, \lambda_1)) \\
f_{noise} \sim GP(0, K_2(\alpha_2, \lambda_2)) \\
\sigma \sim \Gamma(2, 10)
\]</span></p>
<p>Set <span class="math inline">\(K1\)</span> to squared exponential
covariance function and <span class="math inline">\(K2\)</span> to
exponential covariance. The point of <span class="math inline">\(f_{trend}\)</span> is to capture the longer term
trend in the data, while <span class="math inline">\(f_{noise}\)</span>
target shorter-term variations around the trend.</p>
<p>Set the hyperparameters <span class="math inline">\(\alpha_{1},
\alpha_{2}\)</span> and <span class="math inline">\(\lambda_{1},
\lambda_{1,2}\)</span> appropriately.</p>
<p>Is such a model appropriate for predicting stock prices into the
future?</p>
<p>Hint: Running the Stan program can take quite a long time, so do the
initial testing using only 1 chain and, for example, 500 iterations.</p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:velait@utu.fi">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/aio.html",
  "identifier": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/aio.html",
  "dateCreated": "2024-04-17",
  "dateModified": "2024-04-23",
  "datePublished": "2024-04-23"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

