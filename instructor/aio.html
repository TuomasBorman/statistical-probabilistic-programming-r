<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Probabilistic Programming in R: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Probabilistic Programming in R
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Probabilistic Programming in R
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Probabilistic Programming in R
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="bayesian-statistics.html">1. Short introduction to Bayesian statistics</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="stan.html">2. Stan</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="mcmc.html">3. MCMC</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="hierarchical-models.html">4. Hierarchical Models</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="model-critisism.html">5. Model comparison</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="gaussian-processes.html">6. Gaussian processes</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="other-topics.html">7. Other topics</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="exercises.html">8. Exercises</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-bayesian-statistics"><p>Content from <a href="bayesian-statistics.html">Short introduction to Bayesian statistics</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/bayesian-statistics.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 90 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How are statistical models formulated and fitted within the Bayesian
framework?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Learn to</p>
<ul>
<li>formulate prior, likelihood, posterior distributions.</li>
<li>fit a Bayesian model with the grid approximation.</li>
<li>communicate posterior information.</li>
<li>work with with posterior samples.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="bayes-formula"><h2 class="section-heading">Bayes’ formula<a class="anchor" aria-label="anchor" href="#bayes-formula"></a>
</h2>
<hr class="half-width">
<p>The starting point of Bayesian statistics is Bayes’ theorem,
expressed as:</p>
<p><span class="math display">\[
  p(\theta | X) = \frac{p(X | \theta) p(\theta)  }{p(X)} \\
\]</span></p>
<p>When dealing with a statistical model, this theorem is used to infer
the probability distribution of the model parameters <span class="math inline">\(\theta\)</span>, conditional on the available data
<span class="math inline">\(X\)</span>. These probabilities are
quantified by the <em>posterior distribution</em> <span class="math inline">\(p(\theta | X)\)</span>, which is primary the
target of probabilistic modeling.</p>
<p>On the right-hand side of the formula, the <em>likelihood
function</em> <span class="math inline">\(p(X | \theta)\)</span> gives
plausibility of the data given <span class="math inline">\(\theta\)</span>, and determines the impact of the
data on the posterior.</p>
<p>A defining feature of Bayesian modeling is the second term in the
numerator, the <em>prior distribution</em> <span class="math inline">\(p(\theta)\)</span>. The prior is used to
incorporate beliefs about <span class="math inline">\(\theta\)</span>
before considering the data.</p>
<p>The denominator on the right-hand side <span class="math inline">\(p(X)\)</span> is called the marginal probability,
and is often practically impossible to compute. For this reason the
proportional version of Bayes’ formula is typically employed:</p>
<p><span class="math display">\[
p(\theta | X) \propto p(\theta)  p(X | \theta).
\]</span></p>
<p>The proportional Bayes’ formula yields an unnormalized posterior
distribution, which can subsequently be normalized to obtain the
posterior.</p>
<div class="section level3">
<h3 id="example-handedness">Example: handedness<a class="anchor" aria-label="anchor" href="#example-handedness"></a>
</h3>
<p>Let’s illustrate the use of the Bayes’ theorem with an example.</p>
<p>Assume we are trying to estimate the prevalence of left-handedness in
humans, based on a sample of <span class="math inline">\(N=50\)</span>
students, out of which <span class="math inline">\(x=7\)</span> are
left-handed and 43 right-handed.</p>
<p>The outcome is binary and the students are assumed to be independent
(e.g. no twins), so the binomial distribution is the appropriate choice
for likelihood:</p>
<p><span class="math display">\[
p(X|\theta) = Bin(7 | 50, \theta).
\]</span></p>
<p>Without further justification, we’ll choose <span class="math inline">\(p(\theta) = Beta(\theta |1, 10)\)</span> as the
prior distribution, so the unnormalized posterior distribution is</p>
<p><span class="math display">\[
p(\theta | X) = \text{Bin}(7 | 50, \theta) \cdot \text{Beta}(\theta | 1,
10).
\]</span></p>
<p>Below, we’ll plot these functions. Likelihood (which is not a
distribution!) has been normalized for better illustration.</p>
<figure><img src="../fig/bayesian-statistics-rendered-unnamed-chunk-2-1.png" width="100%" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The figure shows that the majority of the mass in the posterior
distribution is concentrated between 0 and 0.25. This implies that,
given the available data and prior distribution, the model is fairly
confident that the value of <span class="math inline">\(\theta\)</span>
is between these values. The peak of the posterior is at approximately
0.1 representing the most likely value. This aligns well with intuitive
expectations about left-handedness in humans.</p>
<div id="accordionInstructor1" class="accordion instructor-note accordion-flush">
<div class="accordion-item">
<button class="accordion-button instructor-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseInstructor1" aria-expanded="false" aria-controls="collapseInstructor1">
  <h3 class="accordion-header" id="headingInstructor1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="edit-2"></i></div>Instructor Note</h3>
</button>
<div id="collapseInstructor1" class="accordion-collapse collapse" aria-labelledby="headingInstructor1" data-bs-parent="#accordionInstructor1">
<div class="accordion-body">
<p>Actual value from a study from 1975 with 7,688 children in US grades
1-6 was 9.6%</p>
<p>Hardyck, C. et al. (1976), Left-handedness and cognitive deficit <a href="https://en.wikipedia.org/wiki/Handedness" class="external-link uri">https://en.wikipedia.org/wiki/Handedness</a></p>
</div>
</div>
</div>
</div>
</div>
</section><section id="communicating-posterior-information"><h2 class="section-heading">Communicating posterior information<a class="anchor" aria-label="anchor" href="#communicating-posterior-information"></a>
</h2>
<hr class="half-width">
<p>The posterior distribution <span class="math inline">\(p(\theta |
X)\)</span> contains all the information about <span class="math inline">\(\theta\)</span> given the data, chosen model, and
the prior distribution. However, understanding a distribution in itself
can be challenging, especially if it is multidimensional. To effectively
communicate posterior information, methods to quantify the information
contained in the posterior are needed. Two commonly used types of
estimates are point estimates, such as the posterior mean, mode, and
variance, and posterior intervals, which provide probabilities for
ranges of values.</p>
<p>Two specific types of posterior intervals are often of interest:</p>
<ol style="list-style-type: decimal">
<li><p><em>Credible intervals</em> (CIs): These intervals leave equal
posterior mass below and above them, computed as posterior quantiles.
For instance, a 90% CI would span the range between the 5% and 95%
quantiles.</p></li>
<li><p><em>Defined boundary intervals</em>: Computed as the posterior
mass for specific parts of the parameter space, these intervals quantify
the probability for given parameter conditions. For example, we might be
interested in the posterior probability that <span class="math inline">\(\theta &gt; 0\)</span>, <span class="math inline">\(0&lt;\theta&lt;0.5\)</span>, or <span class="math inline">\(\theta&lt;0\)</span> or <span class="math inline">\(\theta &gt; 0.5\)</span>. These probabilities can
be computed by integrating the posterior over the corresponding
sets.</p></li>
</ol>
<p>The following figures illustrate selected posterior intervals for the
previous example along with the posterior mode, or <em>maximum a
posteriori</em> (MAP) estimate.</p>
<figure><img src="../fig/bayesian-statistics-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="grid-approximation"><h2 class="section-heading">Grid approximation<a class="anchor" aria-label="anchor" href="#grid-approximation"></a>
</h2>
<hr class="half-width">
<p>Specifying a probabilistic model can be simple, but a common
bottleneck in Bayesian data analysis is model fitting. Later in the
course, we will begin using Stan, a state-of-the-art method for
approximating the posterior. However, we’ll begin fitting probabilistic
model using the grid approximation. This approach involves computing the
unnormalized posterior distribution at a grid of evenly spaced values in
the parameter space and can be specified as follows:</p>
<ol style="list-style-type: decimal">
<li>Define a grid of parameter values.</li>
<li>Compute the prior and likelihood on the grid.</li>
<li>Multiply to get the unnormalized posterior.</li>
<li>Normalize.</li>
</ol>
<p>Now, we’ll implement the grid approximation for the handedness
example in R.</p>
<div class="section level3">
<h3 id="example-handedness-with-grid-approximation">Example: handedness with grid approximation<a class="anchor" aria-label="anchor" href="#example-handedness-with-grid-approximation"></a>
</h3>
<p>First, we’ll define the data variables and the grid of parameter
values</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span></span>
<span></span>
<span><span class="co"># 7/50 are left-handed</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">7</span></span>
<span></span>
<span><span class="co"># Define a grid of points in the interval [0, 1], with 0.01 interval</span></span>
<span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.01</span></span>
<span><span class="va">theta_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0</span>, to <span class="op">=</span> <span class="fl">1</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span></code></pre>
</div>
<p>Computing the values of the likelihood, prior, and unnormalized
posterior is straightforward. While you can compute these using
for-loops, vectorization as used below, is a more efficient
approach:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="fu">dbinom</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, size <span class="op">=</span> <span class="va">N</span>, prob <span class="op">=</span> <span class="va">theta_grid</span><span class="op">)</span></span>
<span><span class="va">prior</span> <span class="op">&lt;-</span> <span class="fu">dbeta</span><span class="op">(</span><span class="va">theta_grid</span>, <span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">likelihood</span><span class="op">*</span><span class="va">prior</span></span></code></pre>
</div>
<p>Next, the posterior needs to be normalized.</p>
<p>In practice, this means dividing the values by the area under the
unnormalized posterior. The area is computed with the integral <span class="math display">\[\int_0^1 p(\theta | X)_{\text{unnormalized}}
d\theta,\]</span> which is for a grid approximated function is the sum
<span class="math display">\[\sum_{\text{grid}} p(\theta |
X)_{\text{unnormalized}} \cdot \delta,\]</span> where <span class="math inline">\(\delta\)</span> is the grid interval.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># normalize </span></span>
<span><span class="va">posterior</span> <span class="op">&lt;-</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span></span>
<span><span class="co"># likelihood also normalized for better visualization</span></span>
<span><span class="va">likelihood</span> <span class="op">&lt;-</span> <span class="va">likelihood</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">likelihood</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span></span></code></pre>
</div>
<p>Finally, we can plot these functions</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make data frame</span></span>
<span><span class="va">df_hand</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_grid</span>, <span class="va">likelihood</span>, <span class="va">prior</span>, <span class="va">posterior</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># wide to long format</span></span>
<span><span class="va">df_hand_l</span> <span class="op">&lt;-</span> <span class="va">df_hand</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"Function"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">theta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="va">df_hand_l</span>, </span>
<span>       <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">Function</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>size <span class="op">=</span> <span class="fl">2</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">scale_color_grafify</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">theta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p1</span></span></code></pre>
</div>
<figure><img src="../fig/bayesian-statistics-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The points in the figure represent the values of the functions
computed at the grid locations. The lines depict linear interpolations
between these points.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Experiment with different priors and examine their effects on the
posterior. You could try, for example, different Beta distributions, the
normal distribution, or the uniform distribution.</p>
<p>How does the shape of the prior impact the posterior?</p>
<p>What is the relationship between the posterior, data (likelihood) and
the prior?</p>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="grid-approximation-and-posterior-summaries">Grid approximation and posterior summaries<a class="anchor" aria-label="anchor" href="#grid-approximation-and-posterior-summaries"></a>
</h3>
<p>Next, we’ll learn how to compute point estimates and posterior
intervals based on the approximate posterior obtained with the grid
approximation.</p>
<p>Computing the posterior mean and variance is based on the definition
of these statistics for continuous variables. The mean is defined as
<span class="math display">\[\int \theta \cdot p(\theta | X)
d\theta\]</span> and can be computed using discrete integration: <span class="math display">\[\sum_{\text{grid}} \theta \cdot p(\theta | X)
\cdot \delta.\]</span> Similarly, variance can be computed based on the
definition <span class="math display">\[\text{var}(\theta) = \int
(\theta - \text{mean}(\theta))^2p(\theta | X)d\theta\]</span>. Posterior
mode is simply the grid value where the posterior is maximized.</p>
<p>In R, these statistics can be computed as follows:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">data.frame</span><span class="op">(</span>Estimate <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"Mode"</span>, <span class="st">"Mean"</span>, <span class="st">"Variance"</span><span class="op">)</span>, </span>
<span>           Value <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df_hand</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span>, <span class="st">"theta"</span><span class="op">]</span>,</span>
<span>                     <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span>, </span>
<span>                     <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">^</span><span class="fl">2</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span> <span class="op">-</span></span>
<span>                       <span class="fu">sum</span><span class="op">(</span><span class="va">df_hand</span><span class="op">$</span><span class="va">theta</span><span class="op">*</span><span class="va">df_hand</span><span class="op">$</span><span class="va">posterior</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>  Estimate       Value
1     Mode 0.120000000
2     Mean 0.131147540
3 Variance 0.001837869</code></pre>
</div>
<p>Posterior intervals are also relatively easy to compute.</p>
<p>Finding the quantiles used to determine CIs is based on the
cumulative distribution function of the posterior <span class="math inline">\(F(\theta) = \int_{\infty}^{\theta}p(y | X)
dy\)</span>. The locations where the <span class="math inline">\(F(\theta) = 0.05\)</span> and <span class="math inline">\(F(\theta) = 0.95\)</span> define the 90% CIs.</p>
<p>Probabilities for certain parameter ranges are computed simply by
integrating over the appropriate set. For instance, <span class="math inline">\(Pr(\theta &lt; 0.1) = \int_0^{0.1} p(\theta | X)
d\theta\)</span></p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Compute the 90% CIs and the probability <span class="math inline">\(Pr(\theta &lt; 0.1)\)</span> for the handedness
example.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Quantiles</span></span>
<span><span class="va">q5</span> <span class="op">&lt;-</span> <span class="va">theta_grid</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="fu">cumsum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span> <span class="op">&gt;</span> <span class="fl">0.05</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="va">q95</span> <span class="op">&lt;-</span> <span class="va">theta_grid</span><span class="op">[</span><span class="fu">which.min</span><span class="op">(</span><span class="fu">cumsum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span> <span class="op">&lt;</span> <span class="fl">0.95</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># Pr(theta &lt; 0.1)</span></span>
<span><span class="va">Pr_theta_under_0.1</span> <span class="op">&lt;-</span> <span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">[</span><span class="va">theta_grid</span> <span class="op">&lt;</span> <span class="fl">0.1</span><span class="op">]</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="fu">paste0</span><span class="op">(</span><span class="st">"90% CI = ("</span>, <span class="va">q5</span>,<span class="st">","</span>, <span class="va">q95</span>,<span class="st">")"</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "90% CI = (0.07,0.21)"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">print</span><span class="op">(</span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Pr(theta &lt; 0.1) = "</span>,</span>
<span>             <span class="fu">round</span><span class="op">(</span><span class="va">Pr_theta_under_0.1</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Pr(theta &lt; 0.1) = 0.20659"</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="example-gamma-model-with-grid-approximation">Example: Gamma model with grid approximation<a class="anchor" aria-label="anchor" href="#example-gamma-model-with-grid-approximation"></a>
</h3>
<p>Let’s investigate another model and implement a grid approximation to
fit it.</p>
<p>The gamma distribution arises, for example, in applications that
model the waiting time between consecutive events. Let’s model the
following data points as independent realizations from a <span class="math inline">\(\Gamma(\alpha, \beta)\)</span> distribution with
unknown shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span> parameters:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0.34</span>, <span class="fl">0.2</span>, <span class="fl">0.22</span>, <span class="fl">0.77</span>, <span class="fl">0.46</span>, <span class="fl">0.73</span>, <span class="fl">0.24</span>, <span class="fl">0.66</span>, <span class="fl">0.64</span><span class="op">)</span></span></code></pre>
</div>
<p>We’ll estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> using the grid approximation.
Similarly as before, we’ll first need to define a grid. Since there are
two parameters the parameter space is 2-dimensional and the grid needs
to be defined at all pairwise combinations of the points of the
individual grids.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">delta</span> <span class="op">&lt;-</span> <span class="fl">0.1</span></span>
<span><span class="va">alpha_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.01</span>, to <span class="op">=</span> <span class="fl">15</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span>
<span><span class="va">beta_grid</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span>from <span class="op">=</span> <span class="fl">0.01</span>, to <span class="op">=</span> <span class="fl">25</span>, by <span class="op">=</span> <span class="va">delta</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get pairwise combinations</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">expand.grid</span><span class="op">(</span>alpha <span class="op">=</span> <span class="va">alpha_grid</span>, beta <span class="op">=</span> <span class="va">beta_grid</span><span class="op">)</span></span></code></pre>
</div>
<p>Next, we’ll compute the likelihood. As we assumed the data points to
be independently generated from the gamma distribution, the likelihood
is the product of the likelihoods of individual observations.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Loop over all alpha, beta combinations</span></span>
<span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">df</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"likelihood"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu">prod</span><span class="op">(</span></span>
<span>    <span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span>,</span>
<span>           shape <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"alpha"</span><span class="op">]</span>,</span>
<span>           rate <span class="op">=</span> <span class="va">df</span><span class="op">[</span><span class="va">i</span>, <span class="st">"beta"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Next, we’ll define priors for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Only positive values are allowed,
which should be reflected in the prior. We’ll use <span class="math inline">\(\Gamma\)</span> priors with large variances.</p>
<p>Notice, that normalizing the posterior now requires integrating over
both dimensions, hence the <span class="math inline">\(\delta^2\)</span>
below.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Priors: alpha, beta ~ Gamma(2, .1)</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>prior <span class="op">=</span> <span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, <span class="fl">2</span>, <span class="fl">0.1</span><span class="op">)</span><span class="op">*</span><span class="fu">dgamma</span><span class="op">(</span>x <span class="op">=</span> <span class="va">beta</span>, <span class="fl">2</span>, <span class="fl">0.1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Posterior</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">prior</span><span class="op">*</span><span class="va">likelihood</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span> <span class="co"># normalize</span></span>
<span></span>
<span></span>
<span><span class="co"># Plot</span></span>
<span><span class="va">p_joint_posterior</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_tile</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, y <span class="op">=</span> <span class="va">beta</span>, fill <span class="op">=</span> <span class="va">posterior</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">scale_fill_gradientn</span><span class="op">(</span>colours <span class="op">=</span> <span class="fu">rainbow</span><span class="op">(</span><span class="fl">5</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span>, y <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">beta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_joint_posterior</span></span></code></pre>
</div>
<figure><img src="../fig/bayesian-statistics-rendered-unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Next, we’ll compute the posterior mode, which is a point in the
2-dimensional parameter space.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span><span class="op">[</span><span class="fu">which.max</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">posterior</span><span class="op">)</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"alpha"</span>, <span class="st">"beta"</span><span class="op">)</span><span class="op">]</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>      alpha beta
14898  4.71 9.91</code></pre>
</div>
<p>Often, in addition to the parameters of interest, the model contains
parameters we are not interested in. For instance, we might only be
interested in <span class="math inline">\(\alpha\)</span>, in which case
<span class="math inline">\(\beta\)</span> would be a ‘nuisance’
parameter. Nuisance parameters are part of the full (‘joint’) posterior,
but they can be discarded by integrating the joint posterior over these
parameters. A posterior integrated over some parameters is called a
marginal posterior.</p>
<p>Let’s now compute the marginal posterior for <span class="math inline">\(\alpha\)</span> by integrating over <span class="math inline">\(\beta\)</span>. Intuitively, it can be helpful to
think of marginalization as a process where all of the joint posterior
mass is drawn towards the <span class="math inline">\(\alpha\)</span>
axis, as if drawn by a gravitational force.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get marginal posterior for alpha</span></span>
<span><span class="va">alpha_posterior</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>posterior <span class="op">=</span> <span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>posterior <span class="op">=</span> <span class="va">posterior</span><span class="op">/</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="va">posterior</span><span class="op">)</span><span class="op">*</span><span class="va">delta</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_alpha_posterior</span> <span class="op">&lt;-</span> <span class="va">alpha_posterior</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, y <span class="op">=</span> <span class="va">posterior</span><span class="op">)</span>, </span>
<span>            color <span class="op">=</span> <span class="va">posterior_color</span>, </span>
<span>            linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">expression</span><span class="op">(</span><span class="va">alpha</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_alpha_posterior</span></span></code></pre>
</div>
<figure><img src="../fig/bayesian-statistics-rendered-unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Does the MAP of the joint posterior of <span class="math inline">\(\theta = (\alpha, \beta)\)</span> correspond to
the MAPs of the marginal posteriors of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>No. Why?</p>
</div>
</div>
</div>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>The conjugate prior for the Gamma likelihood <a href="https://en.wikipedia.org/wiki/Gamma_distribution#Bayesian_inference" class="external-link">exists</a>,
which means there is a prior that causes the posterior to be of the same
shape.</p>
</div>
</div>
</div>
</div>
</section><section id="working-with-samples"><h2 class="section-heading">Working with samples<a class="anchor" aria-label="anchor" href="#working-with-samples"></a>
</h2>
<hr class="half-width">
<p>The main limitation of the grid approximation is that it becomes
impractical for models with even a moderate number of parameters. The
reason is that the number of computations grows as <span class="math inline">\(O \{ \Delta^p \}\)</span> where <span class="math inline">\(\Delta\)</span> is the number of grid points per
model parameter and <span class="math inline">\(p\)</span> the number of
parameters. This quickly becomes prohibitive, and the grid approximation
is seldom used in practice. The standard approach to fitting Bayesian
models is to draw samples from the posterior with Markov chain Monte
Carlo (MCMC) methods. These methods are the topic of a later episode but
we’ll anticipate this now by studying how posterior summaries can be
computed based on samples.</p>
<div class="section level3">
<h3 id="example-handedness-with-samples">Example: handedness with samples<a class="anchor" aria-label="anchor" href="#example-handedness-with-samples"></a>
</h3>
<p>Let’s take the Beta-binomial model (beta prior, binomial likelihood)
of the handedness analysis as our example. It is an instance of a model
for which the posterior can be computed analytically. Given a prior
<span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span> and
likelihood <span class="math inline">\(\text{Bin}(x | N,
\theta)\)</span>, the posterior is <span class="math display">\[p(\theta
| X) = \text{Beta}(\alpha + x, \beta + N - x).\]</span> Let’s generate
<span class="math inline">\(n = 1000\)</span> samples from this
posterior using the handedness data:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span></span>
<span><span class="va">theta_samples</span> <span class="op">&lt;-</span> <span class="fu">rbeta</span><span class="op">(</span><span class="va">n</span>, <span class="fl">1</span> <span class="op">+</span> <span class="fl">7</span>, <span class="fl">10</span> <span class="op">+</span> <span class="fl">50</span> <span class="op">-</span> <span class="fl">7</span><span class="op">)</span></span></code></pre>
</div>
<p>Plotting a histogram of these samples against the grid approximation
posterior displays that both are indeed approximating the same
distribution
<img src="../fig/bayesian-statistics-rendered-unnamed-chunk-17-1.png" width="100%" style="display: block; margin: auto;" class="figure"></p>
<p>Computing posterior summaries from samples is easy. The posterior
mean and variance are computed simply by taking the mean and variance of
the samples, respectively. Posterior intervals are equally easy to
compute: 90% CI is recovered from the appropriate quantiles and the
probability of a certain parameter interval is the proportion of total
samples within the interval.</p>
<div id="discussion2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion2"></a>
</h3>
<div class="callout-content">
<p>Compute the posterior mean, variance, 90% CI and <span class="math inline">\(Pr(\theta &gt; 0.1)\)</span> using the generated
samples.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Likelihood determines the probability of data conditional on the
model parameters.</li>
<li>Prior encodes beliefs about the model parameters without considering
data.</li>
<li>Posterior quantifies the probability of parameter values conditional
on the data.</li>
<li>The posterior is a compromise between the data and prior. The less
data available, the greater the impact of the prior.</li>
<li>The grid approximation is a method for inferring the (approximate)
posterior distribution.</li>
<li>Posterior information can be summarized with point estimates and
posterior intervals.</li>
<li>The marginal posterior is accessed by integrating over nuisance
parameters.</li>
<li>Usually, Bayesian models are fitted using methods that generate
samples from the posterior.</li>
</ul>
</div>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Bayesian Data Analysis (3rd ed.): Ch. 1-3</li>
<li>Statistical Rethinking (2nd ed.): Ch. 1-3</li>
<li>Bayes Rules!: Ch. 1-6</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-stan"><p>Content from <a href="stan.html">Stan</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/stan.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 60 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can posterior samples be generated using Stan?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Learn how to</p>
<ul>
<li>implement statistical models in Stan.</li>
<li>generate posterior samples with Stan.</li>
<li>extract and process samples generated with Stan.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Stan is a probabilistic programming language that can be used to
specify probabilistic models and to generate samples from posterior
distributions.</p>
<p>The standard steps is using Stan is to first write the statistical
model in a separate text file, then to call Stan from R (or other
supported interface) which performs the sampling. Instead of having to
write formulas the model can be written using built-in functions and
sampling statements similar to written text. The sampling process is
performed with a Markov Chain Monte Carlo (MCMC) algorithm, which we
will study in a later episode. For now, however, our focus is on
understanding how to execute it using Stan.</p>
<p>To get started, follow the instructions provided at <a href="https://mc-stan.org/users/interfaces/" class="external-link uri">https://mc-stan.org/users/interfaces/</a> to install Stan on
your local computer.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>With Stan, you can fit model that have continuous parameters. Models
with discrete parameters such as most classification models are
typically impossible to fit, although some workarounds have been
implemented.</p>
</div>
</div>
</div>
<section id="basic-program-structure"><h2 class="section-heading">Basic program structure<a class="anchor" aria-label="anchor" href="#basic-program-structure"></a>
</h2>
<hr class="half-width">
<p>A Stan program is organized into several blocks that collectively
define the model. Typically, a Stan program includes at least the
following blocks:</p>
<ol style="list-style-type: decimal">
<li><p>Data: This block is used to declare the input data provided to
the model. It specifies the types and dimensions of the data variables
incorporated into the model.</p></li>
<li><p>Parameters: In this block, the model parameters are
declared.</p></li>
<li><p>Model: The likelihood and prior distributions are included here
through sampling statements.</p></li>
</ol>
<p>For best practices, it is recommended to specify Stan programs in
separate text files with a .stan extension, which can then be called
from R.</p>
<div class="section level3">
<h3 id="example-1-beta-binomial-model">Example 1: Beta-binomial model<a class="anchor" aria-label="anchor" href="#example-1-beta-binomial-model"></a>
</h3>
<p>The following Stan program specifies the Beta-binomial model, and
consists of data, parameters, and model blocks.</p>
<p>The data variables are the total sample size <span class="math inline">\(N\)</span> and <span class="math inline">\(x\)</span>, the outcome of a binary variable (coin
flip, handedness etc.). The declared data type is <code>int</code> for
integer, and the variables have a lower bound 1 and 0 for <span class="math inline">\(N\)</span> and <span class="math inline">\(x\)</span>, respectively. Notice that each line
ends with a semicolon.</p>
<p>In the parameters block we declare <span class="math inline">\(\theta\)</span>, the probability for a success.
Since this parameter is a probability, it is a real number restricted
between 0 and 1.</p>
<p>In the model block, the likelihood is specified with the sampling
statement <code>x ~ binomial(N, theta)</code>. This line includes the
binomial distribution <span class="math inline">\(\text{Bin}(x | N,
theta)\)</span> in the target distribution. The prior is set similarly,
and omitting the prior implies a uniform prior. Comments can be included
after two forward slashes.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>  <span class="kw">data</span>{</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N; </span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; x; </span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>  }</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>  <span class="kw">parameters</span>{</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>    <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>, <span class="kw">upper</span>=<span class="dv">1</span>&gt; theta;</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>  }</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>  </span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>  <span class="kw">model</span>{</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>    <span class="co">// Likelihood</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>    x ~ binomial(N, theta);</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>    <span class="co">// Uniform prior</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>  }</span></code></pre>
</div>
<p>Once the Stan program has been saved we need to compile it. In R,
this is done by running the following line, where
<code>"binomial_model.stan"</code> is the path of the program file.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binomial_model</span> <span class="op">&lt;-</span> <span class="fu">stan_model</span><span class="op">(</span><span class="st">"binomial_model.stan"</span><span class="op">)</span></span></code></pre>
</div>
<p>Once the program has been compiled, it can be used to generate the
posterior samples by calling the function <code>sampling()</code>. The
data needs to be input as a list.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binom_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fl">50</span>, x <span class="op">=</span> <span class="fl">7</span><span class="op">)</span></span>
<span></span>
<span><span class="va">binom_samples</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span>object <span class="op">=</span> <span class="va">binomial_model</span>,</span>
<span>                          data <span class="op">=</span> <span class="va">binom_data</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 4e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 1:                0.004 seconds (Sampling)
Chain 1:                0.008 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.003 seconds (Warm-up)
Chain 2:                0.003 seconds (Sampling)
Chain 2:                0.006 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.004 seconds (Warm-up)
Chain 3:                0.003 seconds (Sampling)
Chain 3:                0.007 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 2e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.003 seconds (Warm-up)
Chain 4:                0.003 seconds (Sampling)
Chain 4:                0.006 seconds (Total)
Chain 4: </code></pre>
</div>
<p>With the default settings, Stan executes 4 MCMC chains, each with
2000 iterations (more about this in the next episode). During the run,
Stan provides progress information, aiding in estimating the running
time, particularly for complex models or extensive datasets. In this
case the sampling took only a fraction of a second.</p>
<p>Running <code>binom_samples</code>, a summary for the model parameter
<span class="math inline">\(p\)</span> is printed, facilitating a quick
review of the results.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">binom_samples</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Inference for Stan model: anon_model.
4 chains, each with iter=2000; warmup=1000; thin=1; 
post-warmup draws per chain=1000, total post-warmup draws=4000.

        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
theta   0.16    0.00 0.05   0.07   0.12   0.15   0.19   0.26  1459    1
lp__  -22.84    0.02 0.75 -24.78 -23.00 -22.56 -22.38 -22.33  1468    1

Samples were drawn using NUTS(diag_e) at Tue Jun 18 13:17:46 2024.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).</code></pre>
</div>
<p>This summary can also be accessed as a matrix with
<code>summary(binom_samples)$summary</code>.</p>
<p>Often, however, it is necessary process the individual samples. These
can be extracted as follows:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">theta_samples</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">binom_samples</span>, <span class="st">"theta"</span><span class="op">)</span><span class="op">[[</span><span class="st">"theta"</span><span class="op">]</span><span class="op">]</span></span></code></pre>
</div>
<p>Now we can use the methods presented in the previous Episode to
compute posterior summaries, credible intervals and to generate
figures.</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>Compute the 95% credible intervals for the samples drawn with Stan.
What is the probability that <span class="math inline">\(\theta \in
(0.05, 0.15)\)</span>? Plot a histogram of the posterior samples.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">CI95</span> <span class="op">&lt;-</span> <span class="fu">quantile</span><span class="op">(</span><span class="va">theta_samples</span>, probs <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0.025</span>, <span class="fl">0.975</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">theta_between_0.05_0.15</span> <span class="op">&lt;-</span> <span class="fu">mean</span><span class="op">(</span><span class="va">theta_samples</span><span class="op">&gt;</span><span class="fl">0.05</span> <span class="op">&amp;</span> <span class="va">theta_samples</span><span class="op">&lt;</span><span class="fl">0.15</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span>theta <span class="op">=</span> <span class="va">theta_samples</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">30</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">coord_cartesian</span><span class="op">(</span>xlim <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/stan-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="challenge2" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge2"></a>
</h3>
<div class="callout-content">
<p>Try modifying the Stan program so that you add a <span class="math inline">\(Beta(\alpha, \beta)\)</span> prior for <span class="math inline">\(\theta\)</span>.</p>
<p>Can you modify the Stan program further so that you can set the
hyperparameters <span class="math inline">\(\alpha, \beta\)</span> as
part of the data? What is the benefit of using this approach?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>Modifying the data block so that it declares the hyperparameters as
data (e.g. <code>real&lt;lower=0&gt; alpha;</code>) enables setting the
hyperparameter values as part of data. This makes it possible to change
the hyperparameters without modifying the Stan file.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="additional-stan-blocks"><h2 class="section-heading">Additional Stan blocks<a class="anchor" aria-label="anchor" href="#additional-stan-blocks"></a>
</h2>
<hr class="half-width">
<p>In addition to the data, parameters, and model blocks there are
additional blocks that can be included in the program.</p>
<ol style="list-style-type: decimal">
<li><p>Functions: For user-defined functions. This block must be the
first in the Stan program. It allows users to define custom
functions.</p></li>
<li><p>Transformed data: This block is used for transformations of the
data variables. It is often employed to preprocess or modify the input
data before it is used in the main model. Common tasks include
standardization, scaling, or other data adjustments.</p></li>
<li><p>Transformed parameters: In this block, transformations of the
parameters are defined. If transformed parameters are used on the
left-hand side of sampling statements in the model block, the Jacobian
adjustment for the posterior density needs to be included in the model
block as well.</p></li>
<li><p>Generated quantities: This block is used to define quantities
based on both data and model parameters. These quantities are not part
of the model but are useful for post-processing.</p></li>
</ol>
<p>We will make use of these additional structures in subsequent
illustrations.</p>
</section><section id="example-2-normal-model"><h2 class="section-heading">Example 2: normal model<a class="anchor" aria-label="anchor" href="#example-2-normal-model"></a>
</h2>
<hr class="half-width">
<p>Next, let’s implement the normal model in Stan. First we’ll generate
some data <span class="math inline">\(X\)</span> from a normal model
with unknown mean and standard deviation parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span></p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">99</span></span>
<span></span>
<span><span class="co"># Generate data with unknown parameters</span></span>
<span><span class="va">unknown_sigma</span> <span class="op">&lt;-</span> <span class="fu">runif</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">unknown_mu</span> <span class="op">&lt;-</span> <span class="fu">runif</span><span class="op">(</span><span class="fl">1</span>, <span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span>n <span class="op">=</span> <span class="va">N</span>,</span>
<span>           mean <span class="op">=</span> <span class="va">unknown_mu</span>,</span>
<span>           sd <span class="op">=</span> <span class="va">unknown_sigma</span><span class="op">)</span> </span></code></pre>
</div>
<p>The Stan program for the normal model is specified in the next code
chunk. It introduces a new data type (vector) and leverages
vectorization in the likelihood statement. In the end of the program, a
generated quantities block is included which generates new data
(X_tilde) to estimate what unseen data points might look like. This
resulting distribution is referred to as the <em>posterior predictive
distribution</em>, which is generated by drawing a random realization
from the normal distribution for each posterior sample <span class="math inline">\((\mu, \sigma)\)</span>.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>}</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>}</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>  <span class="co">// Vectorized likelihood</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>  X ~ normal(mu, sigma);</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>  </span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>}</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a>  <span class="dt">real</span> X_tilde;</span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>  X_tilde = normal_rng(mu, sigma);</span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Let’s fit the model to the data</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">normal_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>, </span>
<span>                                  <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">X</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 6e-06 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.007 seconds (Warm-up)
Chain 1:                0.007 seconds (Sampling)
Chain 1:                0.014 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 3e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 2:                0.008 seconds (Sampling)
Chain 2:                0.016 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 2e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 3:                0.006 seconds (Sampling)
Chain 3:                0.014 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 3e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.008 seconds (Warm-up)
Chain 4:                0.006 seconds (Sampling)
Chain 4:                0.014 seconds (Total)
Chain 4: </code></pre>
</div>
<p>Next, we’ll extract posterior samples and generate a plot for the
joint, and marginal posteriors. The true unknown parameter values are
included in the plots in red.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract parameter samples</span></span>
<span><span class="va">par_samples</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">data.frame</span></span>
<span></span>
<span></span>
<span><span class="co"># Full posterior</span></span>
<span><span class="va">p_posterior</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_samples</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">mu</span>, y <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">unknown_mu</span>, y <span class="op">=</span> <span class="va">unknown_sigma</span><span class="op">)</span>,</span>
<span>             color <span class="op">=</span> <span class="st">"red"</span>, size <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Marginal posteriors</span></span>
<span><span class="va">p_marginals</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_samples</span> <span class="op">%&gt;%</span> <span class="va">gather</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span><span class="op">)</span>, bins <span class="op">=</span> <span class="fl">40</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span>key <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span>, </span>
<span>                               value <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">unknown_mu</span>, <span class="va">unknown_sigma</span><span class="op">)</span><span class="op">)</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">value</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">key</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu">cowplot</span><span class="fu">::</span><span class="fu">plot_grid</span><span class="op">(</span><span class="va">p_posterior</span>, <span class="va">p_marginals</span>,</span>
<span>                  ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in geom_point(aes(x = unknown_mu, y = unknown_sigma), color = "red", : All aesthetics have length 1, but the data has 4000 rows.
ℹ Please consider using `annotate()` or provide this layer with data containing
  a single row.</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">print</span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/stan-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s also plot the posterior predictive distribution samples
histogram and compare it to that of the data.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PPD</span> <span class="op">&lt;-</span> <span class="fu">extract</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilde"</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>X_tilde <span class="op">=</span> <span class="va">.</span> <span class="op">)</span></span>
<span></span>
<span><span class="va">p_PPD</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">PPD</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_tilde</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">30</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="fu">data.frame</span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">30</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_PPD</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/stan-rendered-unnamed-chunk-12-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="example-3-linear-regression"><h2 class="section-heading">Example 3: Linear regression<a class="anchor" aria-label="anchor" href="#example-3-linear-regression"></a>
</h2>
<hr class="half-width">
<div id="challenge3" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge3"></a>
</h3>
<div class="callout-content">
<p>Write a Stan program for linear regression with one dependent
variable.</p>
<p>Generate data from the linear model and use the Stan program to
estimate the intercept <span class="math inline">\(\alpha\)</span>,
slope <span class="math inline">\(\beta\)</span> and noise term <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N; <span class="co">// Sample size</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] x; <span class="co">// x-values</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>  <span class="dt">vector</span>[N] y; <span class="co">// y-values</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>}</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>  <span class="dt">real</span> alpha; <span class="co">// intercept</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>  <span class="dt">real</span> beta;  <span class="co">// slope</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma; <span class="co">// noise</span></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a>}</span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>  </span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>  y ~ normal(alpha + beta * x, sigma);</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>  </span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb17-18"><a href="#cb17-18" tabindex="-1"></a>  alpha ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb17-19"><a href="#cb17-19" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb17-20"><a href="#cb17-20" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb17-21"><a href="#cb17-21" tabindex="-1"></a>}</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="challenge4" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge4"></a>
</h3>
<div class="callout-content">
<p>Modify the program for linear regression so it facilitates <span class="math inline">\(M\)</span> dependent variables.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" aria-labelledby="headingSolution4" data-bs-parent="#accordionSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N; <span class="co">// Sample size</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; M; <span class="co">// Number of features</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>  <span class="dt">matrix</span>[N, M] x; <span class="co">// x-values</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>  <span class="dt">vector</span>[N] y; <span class="co">// y-values</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>}</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a>  <span class="dt">real</span> alpha; <span class="co">// intercept</span></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>  <span class="dt">vector</span>[M] beta;  <span class="co">// slopes</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma; <span class="co">// noise</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>}</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>  </span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>  y ~ normal(alpha + x * beta, sigma);</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>  </span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>  alpha ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>  beta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a>  sigma ~ inv_gamma(<span class="dv">1</span>, <span class="dv">1</span>);</span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a>}</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Stan is a tool for generating posterior distribution samples.</li>
<li>A Stan program is specified in a separate text file that consists of
code blocks, with the data, parameters, and model blocks being the most
crucial ones.</li>
</ul>
</div>
</div>
</div>
</section><section id="resources"><h2 class="section-heading">Resources<a class="anchor" aria-label="anchor" href="#resources"></a>
</h2>
<hr class="half-width">
<ul>
<li>Official release paper <a href="https://www.jstatsoft.org/article/view/v076i01" class="external-link uri">https://www.jstatsoft.org/article/view/v076i01</a>
</li>
<li>User’s guide <a href="https://mc-stan.org/docs/2_18/stan-users-guide/" class="external-link uri">https://mc-stan.org/docs/2_18/stan-users-guide/</a>
</li>
<li>Function’s reference <a href="https://mc-stan.org/docs/functions-reference/" class="external-link uri">https://mc-stan.org/docs/functions-reference/</a>
</li>
<li>Reference manual <a href="https://mc-stan.org/docs/reference-manual/" class="external-link uri">https://mc-stan.org/docs/reference-manual/</a>
</li>
<li>Stan forum <a href="https://discourse.mc-stan.org" class="external-link uri">https://discourse.mc-stan.org</a>
</li>
<li>Case studies <a href="https://mc-stan.org/users/documentation/case-studies" class="external-link uri">https://mc-stan.org/users/documentation/case-studies</a>
</li>
</ul></section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>BDA3: Ch. 12.6, Appendix C</li>
<li>Bayes Rules!: Ch. 6.2</li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-mcmc"><p>Content from <a href="mcmc.html">MCMC</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/mcmc.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 62 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does Stan generate the posterior samples?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Learn</p>
<ul>
<li>the basic idea of the Metropolis-Hasting algorithm.</li>
<li>how to assess Markov chain Monte Carlo convergence.</li>
<li>how to implement a random walk Metropolis-Hasting algorithm.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>The standard solution to fitting probabilistic models is to generate
random samples from the posterior distribution. In the previous episode,
we learned how to generate posterior samples using Stan without knowing
how the samples are generated. In this episode, we’ll study Markov chain
Monte Carlo (MCMC) methods, which are the class of algorithms Stan uses
under the hood.</p>
<section id="metropolis-hastings-algorithm"><h2 class="section-heading">Metropolis-Hastings algorithm<a class="anchor" aria-label="anchor" href="#metropolis-hastings-algorithm"></a>
</h2>
<hr class="half-width">
<p>MCMC methods draw samples from the posterior distribution by
constructing sequences (chains) of values in the parameter space that
ultimately converge to the posterior. While there are other variants of
MCMC, on this course we will mainly focus on the Metropolis-Hasting (MH)
algorithm outlined below. As this algorithm is ran long enough,
convergence to posterior (or to other specified target density) is
guaranteed. This means that that if the chain is run long enough, the
samples will eventually start approximating the posterior
distribution.</p>
<p>A chain starts is initialized at value <span class="math inline">\(\theta^{0}\)</span>, which can be manually set or
random. The only precondition is that the target distribution has
positive mass at the location, <span class="math inline">\(p(\theta^{0}
| X) &gt; 0\)</span>. Then, a proposal <span class="math inline">\(\theta^*\)</span> for the next value is generated
from a transition distribution <span class="math inline">\(T_i\)</span>.
An often-used solution is the normal distribution centered at the
current value, <span class="math inline">\(\theta^* \sim N(\theta^{i},
\sigma^2)\)</span>. This is where the term “Markov chain” comes from,
the value of each element depends only on the previous one.</p>
<p>Next, the proposal <span class="math inline">\(\theta^*\)</span> is
either accepted or rejected. If each proposal was accepted, the sequence
would simply be a random walk in the parameter space and would not
approximate the posterior to any degree. The rule that determines the
acceptance should reflect this; proposals towards higher posterior
densities should be favored over proposals toward low density areas. The
solution is to compute the ratio</p>
<p><span class="math display">\[r = \frac{p(\theta^* | X) / T_i(\theta^*
| \theta^{i})}{p(\theta^i | X) / T_i(\theta^{i} | \theta^{*})},\]</span>
and use is as the probability to move to the proposed value. In other
words, the next element in the chain is <span class="math inline">\(\theta^{i+1} = \theta^*\)</span> with probability
<span class="math inline">\(\min(r, 1)\)</span>. If the proposal is not
accepted the chain stays at the current value, <span class="math inline">\(\theta^{i+1} = \theta^{i}.\)</span> This approach
induces directional randomness in the chain; proposals towards higher
density areas are generally accepted but transitions away from it are
also possible. In situations where the transition density is symmetric,
such as with the normal distribution, <span class="math inline">\(r\)</span> reduces simply to the ratio of the
posterior values, and all proposals toward higher posterior density
areas are accepted.</p>
<div class="section level3">
<h3 id="example-banana-distribution">Example: Banana distribution<a class="anchor" aria-label="anchor" href="#example-banana-distribution"></a>
</h3>
<p>Let’s implement the MH algorithm and use it to generate posterior
samples from the following statistical model:</p>
<p><span class="math display">\[X \sim N(\theta_1 + \theta_2^2, 1) \\
\theta_1, \theta_2 \sim N(0, 1),\]</span></p>
<div class="section level4">
<h4 id="helper-functions">Helper functions<a class="anchor" aria-label="anchor" href="#helper-functions"></a>
</h4>
<p>We begin by writing some helper functions that carry out the
incremental steps of the MH algorithm.</p>
<p>First, we need to be able to generate the proposals. Let’s use the
two-dimensional normal with identity covariance scaled by a scalar
<code>jump_scale</code>.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">generate_proposal</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">pars_now</span>, <span class="va">jump_scale</span> <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  <span class="va">n_pars</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">pars_now</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Random draw from multivariate normal</span></span>
<span>  <span class="va">theta_star</span> <span class="op">&lt;-</span> <span class="fu">mvtnorm</span><span class="fu">::</span><span class="fu">rmvnorm</span><span class="op">(</span><span class="fl">1</span>,</span>
<span>                                 mean <span class="op">=</span> <span class="va">pars_now</span>,</span>
<span>                                 sigma <span class="op">=</span> <span class="va">jump_scale</span><span class="op">*</span><span class="fu">diag</span><span class="op">(</span><span class="va">n_pars</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">theta_star</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Running MH also requires computing the (unnormalized) posterior
density at the proposed parameter values. This function returns the log
posterior value at the location <code>pars</code>. The density is
computed on log scale to avoid issues with numerical precision.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">get_log_target_value</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># log(likelihood)</span></span>
<span>  <span class="fu">sum</span><span class="op">(</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">X</span>,</span>
<span>          mean <span class="op">=</span> <span class="va">pars</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span> <span class="op">+</span> <span class="va">pars</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">^</span><span class="fl">2</span>, </span>
<span>          sd <span class="op">=</span> <span class="fl">1</span>,</span>
<span>          log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>      <span class="op">)</span> <span class="op">+</span></span>
<span>    </span>
<span>    <span class="co"># log(prior)</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">pars</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, <span class="fl">0</span>, <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">dnorm</span><span class="op">(</span><span class="va">pars</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>, <span class="fl">0</span>, <span class="fl">1</span>, log <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Then, we’ll write a function that computes the acceptance ratio <span class="math inline">\(r\)</span>. Since the proposal is symmetric, the
expression reduces to the ratio of the posterior densities of the
proposed and current parameter values. Notice that a ratio on a log
scale is equal to the difference of logarithms.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute ratio</span></span>
<span><span class="va">get_ratio</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span>, <span class="va">pars_proposal</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">exp</span><span class="op">(</span></span>
<span>    <span class="fu">get_log_target_value</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_proposal</span><span class="op">)</span> <span class="op">-</span> </span>
<span>      <span class="fu">get_log_target_value</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span><span class="op">)</span></span>
<span>    <span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">r</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Finally, we can wrap the helpers in a function that loops over the
algorithm steps.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sampler</span></span>
<span><span class="va">MH_sampler</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">X</span>, <span class="co"># Data</span></span>
<span>                       <span class="va">inits</span>, <span class="co"># Initial values</span></span>
<span>                       <span class="va">n_samples</span> <span class="op">=</span> <span class="fl">1000</span>, <span class="co"># Number of iterations</span></span>
<span>                       <span class="va">jump_scale</span> <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># Proposal jump variance</span></span>
<span>                       <span class="op">)</span> <span class="op">{</span></span>
<span></span>
<span>  </span>
<span>  <span class="co"># Matrix for samples</span></span>
<span>  <span class="va">pars</span> <span class="op">&lt;-</span> <span class="fu">matrix</span><span class="op">(</span>nrow <span class="op">=</span> <span class="va">n_samples</span>, ncol <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">inits</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Set initial values</span></span>
<span>  <span class="va">pars</span><span class="op">[</span><span class="fl">1</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">inits</span></span>
<span></span>
<span>  <span class="co"># Generate samples </span></span>
<span>  <span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="va">n_samples</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="co"># Current parameters</span></span>
<span>    <span class="va">pars_now</span> <span class="op">&lt;-</span> <span class="va">pars</span><span class="op">[</span><span class="va">i</span><span class="op">-</span><span class="fl">1</span>, <span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Proposal</span></span>
<span>    <span class="va">pars_proposal</span> <span class="op">&lt;-</span> <span class="fu">generate_proposal</span><span class="op">(</span><span class="va">pars_now</span>, <span class="va">jump_scale</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Ratio</span></span>
<span>    <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">get_ratio</span><span class="op">(</span><span class="va">X</span>, <span class="va">pars_now</span>, <span class="va">pars_proposal</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="va">r</span> <span class="op">&lt;-</span> <span class="fu">min</span><span class="op">(</span><span class="fl">1</span>, <span class="va">r</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># Does the sampler move?</span></span>
<span>    <span class="va">move</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="cn">TRUE</span>, <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>                   size <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                   prob <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">r</span>, <span class="fl">1</span><span class="op">-</span><span class="va">r</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="co"># OR: </span></span>
<span>    <span class="co"># move &lt;- runif(n = 1, min = 0, max = 1) &lt;= r</span></span>
<span>    </span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">move</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">pars</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pars_proposal</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="va">pars</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="va">pars_now</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="va">pars</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span><span class="va">pars</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">pars</span><span class="op">)</span> </span>
<span><span class="op">}</span></span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="run-mh">Run MH<a class="anchor" aria-label="anchor" href="#run-mh"></a>
</h4>
<p>Now we can try out our MH implementation. Let’s use the simulated
data points stored in vector <code>X</code>:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="fl">3.78</span>, <span class="fl">2.76</span>, <span class="fl">2.84</span>, <span class="fl">2.92</span>, <span class="fl">1.3</span>, <span class="fl">3.93</span>, <span class="fl">3.69</span>, <span class="fl">2.28</span>, <span class="fl">2.81</span>, <span class="fl">0.71</span><span class="op">)</span></span></code></pre>
</div>
<p>We’ll generate 1000 samples with initial value (0, 5) and jump scale
0.01. The trajectory of samples is plotted over the posterior density
computed with the grid approximation.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">12</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Draw samples</span></span>
<span><span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu">MH_sampler</span><span class="op">(</span><span class="va">X</span>,</span>
<span>                      inits <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span>,</span>
<span>                      n_samples <span class="op">=</span> <span class="fl">1000</span>, </span>
<span>                      jump_scale <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">colnames</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"theta1"</span>, <span class="st">"theta2"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Add column for sample index</span></span>
<span><span class="va">samples</span><span class="op">$</span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot joint posterior samples</span></span>
<span><span class="va">p_MH1</span> <span class="op">&lt;-</span> <span class="va">p_grid</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta1</span>, y <span class="op">=</span> <span class="va">theta2</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_MH1</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/mcmc-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Looking at the figure, a few observations become evident. Firstly,
despite the chosen initial value being moderately far from the
high-density areas of the posterior, the algorithm quickly converges to
the target region. This rapid convergence is due to the fact that
proposals toward higher density areas are favored, in fact they are
always accepted when using normal density proposals. However, it’s
important to note that such swift convergence is not guaranteed in all
scenarios. In cases with a high number of model parameters, there’s an
increased likelihood of the sampler taking ‘wrong’ directions. The
samples before convergence introduce bias to the posterior
approximation.</p>
<p>Secondly, the posterior is not fully explored; no samples are
generated from the lower mode in the figure. This highlights a crucial
point: even if the sampler has converged, it doesn’t guarantee that the
drawn samples provide a representative picture of the target.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Consider how you could address the two issues raised above:</p>
<ol style="list-style-type: decimal">
<li>Initial unconverged samples introduce a bias.</li>
<li>The sampler may not have explored the target distribution
properly.</li>
</ol>
<p>Try different proposal distributions variances in the MCMC example
above by changing <code>jump_scale</code>. How does this affect the
inference and convergence? Why?</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="assessing-convergence"><h2 class="section-heading">Assessing convergence<a class="anchor" aria-label="anchor" href="#assessing-convergence"></a>
</h2>
<hr class="half-width">
<p>Although convergence of MCMC is theoretically guaranteed, in
practice, this is not always the case. Monitoring convergence is crucial
whenever MCMC is utilized to ensure the reliability of recovered
results.</p>
<p>Depending on the model used, initial values, amount of data, among
other factors, can cause convergence issues. Earlier, we mentioned two
common complications, and here we will list a few more, along with
actions that can alleviate the issues.</p>
<ol style="list-style-type: decimal">
<li><p>Slow convergence can occur when initial values of the chain are
far from most of the target mass, resulting in early iterations biasing
the approximation. Another cause for slow convergence is that the
proposals are not far enough from the current value, and the sampler
moves too slowly.</p></li>
<li><p>Incomplete exploration: This means that the sampler doesn’t spend
enough time in all significant posterior areas.</p></li>
<li><p>A large proportion of the proposals is rejected. When the
proposal distribution generates proposals too far from the current
value, the proposals are rejected and the sampler stands still for many
iterations. This leads to inefficiency.</p></li>
<li><p>Sample autocorrelation: Consecutive samples are close to each
other. Ideally, we’d like to generate independent samples from the
target. High sample autocorrelation can be caused by several factors,
including the ones mentioned in the previous points.</p></li>
</ol>
<p>These issues can be remedied with:</p>
<ol style="list-style-type: decimal">
<li><p>Running multiple long chains with distinct or random initial
values.</p></li>
<li><p>Discarding the early proportion of the chain as warm-up.</p></li>
<li><p>Setting an appropriate proposal distribution.</p></li>
</ol>
<p>It also important to be able to monitor whether the sampler has
converged. This can be done with statistics, such as <em>effective
sample size</em> and <span class="math inline">\(\hat{R}\)</span>.
Effective sample size estimates how many independent samples have been
generated. Ideally, this number should be close to the total number of
iterations the sampler has been ran for. <span class="math inline">\(\hat{R}\)</span> on the other hand measures chain
mixing, that is, how well the chains agree with each other. It is
computed by comparing the variance within each chain to the total
variance of all samples. Usually, values of <span class="math inline">\(\hat{R} &gt; 1.1\)</span> are considered as
signaling convergence issues.</p>
<p>Besides statistics, visually evaluating the samples can be useful.
<em>Trace plots</em> refer to graphs where the marginal posterior
samples are plotted against sample index. Trace plots can be used to
investigate convergence and mixing properties, and can reveal, for
example, multimodality.</p>
<p>In Stan, many of the above-mentioned points have been automatized. By
default, Stan runs 4 chains with 2000 iterations each, and discards the
initial 50% as warm-up. Moreover, it computes <span class="math inline">\(\hat{R}\)</span>, effective sample size, and other
statistics and throws warnings in case of issues.</p>
<div class="section level3">
<h3 id="example-continued">Example continued<a class="anchor" aria-label="anchor" href="#example-continued"></a>
</h3>
<p>In light of the above information, let’s re-fit the model of the
previous example. Now, we’ll run 4 chains with random initial values,
10000 samples each, and discard the first 50% of each chain as warm-up.
We’ll use 0.1 as the proposal variance.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Number of chains</span></span>
<span><span class="va">n_chains</span> <span class="op">&lt;-</span> <span class="fl">4</span></span>
<span></span>
<span><span class="co"># Number of samples</span></span>
<span><span class="va">n_samples</span> <span class="op">&lt;-</span> <span class="fl">10000</span></span>
<span></span>
<span><span class="co"># Consider first p% samples as warmup</span></span>
<span><span class="va">warmup</span> <span class="op">&lt;-</span> <span class="fl">0.5</span></span>
<span></span>
<span><span class="va">samples</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n_chains</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Use random initial values</span></span>
<span>  <span class="va">inits</span> <span class="op">&lt;-</span> <span class="fu">rnorm</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">5</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">chain</span> <span class="op">&lt;-</span> <span class="fu">MH_sampler</span><span class="op">(</span><span class="va">X</span>, inits <span class="op">=</span> <span class="va">inits</span>,</span>
<span>                        n_samples <span class="op">=</span> <span class="va">n_samples</span>, </span>
<span>                        jump_scale <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Wrangle</span></span>
<span>  <span class="fu">colnames</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu">c</span><span class="op">(</span><span class="st">"theta1"</span>, <span class="st">"theta2"</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">$</span><span class="va">sample</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">$</span><span class="va">chain</span> <span class="op">&lt;-</span> <span class="fu">as.factor</span><span class="op">(</span><span class="va">i</span><span class="op">)</span></span>
<span>  <span class="va">chain</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fu">round</span><span class="op">(</span><span class="va">warmup</span><span class="op">*</span><span class="va">n_samples</span><span class="op">)</span>, <span class="st">"warmup"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">TRUE</span></span>
<span>  <span class="va">chain</span><span class="op">[</span><span class="op">(</span><span class="fu">round</span><span class="op">(</span><span class="va">warmup</span><span class="op">*</span><span class="va">n_samples</span><span class="op">)</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="va">n_samples</span>, <span class="st">"warmup"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="cn">FALSE</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">chain</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<p>Now it’s evident that the sample trajectories explore the entire
posterior distribution:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot</span></span>
<span><span class="va">p_joint_2</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># warmup samples</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span></span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span><span class="va">theta1</span>, <span class="va">theta2</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span>,</span>
<span>            alpha <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="co"># post-warmup samples</span></span>
<span>  <span class="fu">geom_path</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span></span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">FALSE</span><span class="op">)</span>,</span>
<span>            <span class="fu">aes</span><span class="op">(</span><span class="va">theta1</span>, <span class="va">theta2</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_joint_2</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/mcmc-rendered-unnamed-chunk-9-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s see what the trace plots look like. Generally, we’d want to see
the samples randomly scattered around a mean value. For <span class="math inline">\(\theta_1\)</span> this is more or less the case,
although there some autocorrelation is apparent. With <span class="math inline">\(\theta_2\)</span> we can see that the chains have
mixed as they explore the same parameter ranges. However, the bimodality
of the posterior is quite apparent.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Trace plots</span></span>
<span><span class="va">p_trace_2</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"parameter"</span>,</span>
<span>                     value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                     <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="st">"sample"</span>, <span class="st">"chain"</span>, <span class="st">"warmup"</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sample</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span>, </span>
<span>            alpha <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">samples</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">filter</span><span class="op">(</span><span class="va">warmup</span> <span class="op">==</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>              <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"parameter"</span>,</span>
<span>                     value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                     <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="st">"sample"</span>, <span class="st">"chain"</span>, <span class="st">"warmup"</span><span class="op">)</span><span class="op">)</span>, </span>
<span>            <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">sample</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">chain</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">parameter</span>,</span>
<span>             ncol <span class="op">=</span> <span class="fl">1</span>,</span>
<span>             scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_trace_2</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/mcmc-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</section><section id="hamiltonian-monte-carlo"><h2 class="section-heading">Hamiltonian Monte Carlo<a class="anchor" aria-label="anchor" href="#hamiltonian-monte-carlo"></a>
</h2>
<hr class="half-width">
<p>Stan uses a variant of the Metropolis-Hastings algorithm called
Hamiltonian Monte Carlo (HMC; or actually a variant HMC). The defining
feature is the elaborate scheme it uses to generate proposals. Briefly,
the idea is to simulate the dynamics of a particle moving in a potential
landscape defined by the posterior. At each iteration, the particle is
given a random momentum vector and then its dynamics are simulated
forward for some time. The end of the trajectory is then taken as the
proposal value.</p>
<p>Compared to the random walk Metropolis-Hastings we implemented in
this episode, HMC is very efficient. The main advantages of HMC is its
ability to explore high-dimensional spaces more effectively, making it
especially useful in complex models with many parameters.</p>
<p>A type of convergence criterion exclusive to HMC is the divergent
transition. In a region of the parameter space where the posterior has
high curvature, the simulated particle dynamics can produce spurious
transitions which do not represent the posterior accurately. Such
transitions are called divergent and signal that the particular area of
parameter space is not explored accurately. Stan provides information
about divergent transitions automatically.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Markov chain Monte Carlo methods can be used to generate samples
from a posterior distribution.</li>
<li>MCMC convergence should always be monitored.</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li><p>See interactive visualization of different MCMC algorithms: <a href="https://chi-feng.github.io/mcmc-demo/app.html" class="external-link uri">https://chi-feng.github.io/mcmc-demo/app.html</a></p></li>
<li><p>Bayesian Data Analysis (3rd ed.): Ch. 11-12</p></li>
<li><p>Statistical Rethinking (2nd ed.): Ch. 9</p></li>
<li><p>Bayes Rules!: Ch. 6-7</p></li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-hierarchical-models"><p>Content from <a href="hierarchical-models.html">Hierarchical Models</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/hierarchical-models.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How does Bayesian modeling accommodate group structure?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn to construct and fit hierarchical models.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p><em>Hierarchical</em> (or multi-level) models are a class of models
suited for situations where the study population comprises distinct but
interconnected groups. For example, analyzing student performance across
different schools, income disparities among various regions, or studying
animal behavior within different populations are scenarios where such
models would be appropriate.</p>
<p>Incorporating group-wise parameters allows us to model each group
separately, and a model becomes hierarchical when we treat the
parameters of the prior distribution as unknown. These parameters, known
as hyperparameters, are assigned their own prior distribution, referred
to as a hyperprior, and are learned during the model fitting process.
Conceptually, these hyperparameters and hyperpriors operate at a higher
level of hierarchy, hence the name.</p>
<p>For example, let’s consider the beta-binomial model discussed in
Episode 1. We used it to estimate the prevalence of left-handedness
based on a sample of 50 students. If we were to include additional
information, such as the students’ majors, we could extend the model as
follows:</p>
<p><span class="math display">\[X_g \sim \text{Bin}(N_g, \theta_g) \\
\theta_g \sim \text{Beta}(\alpha, \beta) \\
\alpha, \beta \sim \Gamma(2, 0.1).\]</span></p>
<p>Here, the subscript <span class="math inline">\(g\)</span> indexes
the groups based on majors. The group-specific prevalences for
left-handedness <span class="math inline">\(\theta_g\)</span> are
assigned a beta prior with hyperparameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> which are treated as random
variables. The final line indicates the hyperprior <span class="math inline">\(\Gamma(2, 0.1)\)</span> governing the prior
beliefs about the hyperparameters.</p>
<p>In this hierarchical beta-binomial model, students are considered
exchangeable within their majors but no longer across the entire
population. However, an underlying assumption of similarity exists
between the groups since they share a common prior, that is learned.
This way the groups are not entirely independent but are not treated as
equal either.</p>
<p>One of the key advantages of Bayesian hierarchical models is their
capacity to leverage information across groups. By pooling information
from various groups, these models can yield more robust estimates,
particularly when data availability is limited.</p>
<p>Another distinction from non-hierarchical models is that the prior,
or the <em>population distribution</em>, of the parameters is learned in
the process. This population distribution can provide information into
parameter variability on a broader scale, even for groups where data is
scarce or completely missing. For instance, if we had data on the
handedness of students majoring in natural sciences, the population
distribution can give insights into students in humanities and social
sciences as well.</p>
<p>In the following example, we will perform a hierarchical analysis of
human heights across different countries.</p>
<section id="example-human-height"><h2 class="section-heading">Example: human height<a class="anchor" aria-label="anchor" href="#example-human-height"></a>
</h2>
<hr class="half-width">
<p>Let’s examine the heights of adults in various countries. In [1],
averages and standard errors of adult heights in centimeters across
different countries and age groups were provided. We’ll utilize this
dataset to generate a sample of hypothetical individual heights and then
assess our ability to reproduce these measured height statistics.</p>
<p>This approach is commonly employed when building and testing models:
we generate simulated data with known parameters and then compare the
inferred results to these known parameters. In our case, the true
parameters are derived from real-world data.</p>
<p>We’ll employ a normal model with unknown mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> as our generative model, and treat
these parameters hierarchically.</p>
<p>First, let’s load the data and examine its structure.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">height</span> <span class="op">&lt;-</span> <span class="fu">read.csv</span><span class="op">(</span><span class="st">"data/height_data.csv"</span><span class="op">)</span></span>
<span><span class="fu">str</span><span class="op">(</span><span class="va">height</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>'data.frame':	210000 obs. of  8 variables:
 $ Country                                   : chr  "Afghanistan" "Afghanistan" "Afghanistan" "Afghanistan" ...
 $ Sex                                       : chr  "Boys" "Boys" "Boys" "Boys" ...
 $ Year                                      : int  1985 1985 1985 1985 1985 1985 1985 1985 1985 1985 ...
 $ Age.group                                 : int  5 6 7 8 9 10 11 12 13 14 ...
 $ Mean.height                               : num  103 109 115 120 125 ...
 $ Mean.height.lower.95..uncertainty.interval: num  92.9 99.9 106.3 112.2 117.9 ...
 $ Mean.height.upper.95..uncertainty.interval: num  114 118 123 128 132 ...
 $ Mean.height.standard.error                : num  5.3 4.72 4.27 3.92 3.66 ...</code></pre>
</div>
<p>Let’s subset this data to simplify the analysis and focus on the
height of adult women measured in 2019.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">height_women</span> <span class="op">&lt;-</span> <span class="va">height</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">filter</span><span class="op">(</span></span>
<span>    <span class="va">Age.group</span> <span class="op">==</span> <span class="fl">19</span>, </span>
<span>    <span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span>,</span>
<span>    <span class="va">Year</span> <span class="op">==</span> <span class="fl">2019</span></span>
<span>    <span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="co"># Select variables of interest</span></span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span>, <span class="va">Mean.height</span>, <span class="va">Mean.height.standard.error</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s select 10 countries randomly</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">set.seed</span><span class="op">(</span><span class="fl">5431</span><span class="op">)</span></span>
<span><span class="co"># Select countries</span></span>
<span><span class="va">N_countries</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span><span class="va">Countries</span> <span class="op">&lt;-</span> <span class="fu">sample</span><span class="op">(</span><span class="fu">unique</span><span class="op">(</span><span class="va">height_women</span><span class="op">$</span><span class="va">Country</span><span class="op">)</span>,</span>
<span>                    size <span class="op">=</span> <span class="va">N_countries</span>,</span>
<span>                    replace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">sort</span></span>
<span></span>
<span><span class="va">height_women10</span> <span class="op">&lt;-</span> <span class="va">height_women</span> <span class="op">%&gt;%</span> <span class="fu">filter</span><span class="op">(</span><span class="va">Country</span> <span class="op">%in%</span> <span class="va">Countries</span><span class="op">)</span></span>
<span></span>
<span><span class="va">height_women10</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>         Country   Sex Mean.height Mean.height.standard.error
1     Bangladesh Girls    152.3776                  0.4966124
2         Belize Girls    158.1201                  1.4026324
3       Cameroon Girls    160.4112                  0.6146315
4           Chad Girls    162.1242                  0.8894219
5  Cote d'Ivoire Girls    158.6524                  0.9254438
6          Ghana Girls    158.8551                  0.8002225
7          Kenya Girls    159.4338                  0.6680202
8     Luxembourg Girls    165.0690                  1.3778094
9         Taiwan Girls    160.6953                  0.7307839
10     Venezuela Girls    160.0370                  1.0813162</code></pre>
</div>
<div class="section level3">
<h3 id="simulate-data">Simulate data<a class="anchor" aria-label="anchor" href="#simulate-data"></a>
</h3>
<p>Now, we can treat the values in the table above as ground truth and
simulate some data based on them. Let’s generate <span class="math inline">\(N=10\)</span> samples for each country from the
normal model with <span class="math inline">\(\mu =
\text{Mean.height}\)</span> and <span class="math inline">\(\sigma =
\text{Mean.height.standard.error}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Sample size per group </span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">10</span></span>
<span></span>
<span><span class="co"># For each country, generate heights</span></span>
<span><span class="va">height_sim</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N_countries</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">my_df</span> <span class="op">&lt;-</span> <span class="va">height_women10</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>Country <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Country</span>, </span>
<span>             <span class="co"># Random values from normal</span></span>
<span>             Height <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="va">N</span>,</span>
<span>                            mean <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Mean.height</span>,</span>
<span>                            sd <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Mean.height.standard.error</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s plot the data</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Plot</span></span>
<span><span class="va">height_sim</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_point</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Height</span>, y <span class="op">=</span> <span class="va">Country</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>    <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Simulated data"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/hierarchical-models-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="modeling">Modeling<a class="anchor" aria-label="anchor" href="#modeling"></a>
</h3>
<p>Let’s build a normal model that uses partial pooling for the country
means and standard deviations. The model can be stated as follows:</p>
<p><span class="math display">\[\begin{align}
X_{gi} &amp;\sim \text{N}(\mu_g, \sigma_g) \\
\mu_g &amp;\sim \text{N}(\mu_\mu, \sigma_\mu) \\
\sigma_g &amp;\sim \Gamma(\alpha_\sigma, \beta_\sigma) \\
\mu_\mu &amp;\sim \text{N}(0, 100)\\
\sigma_\mu &amp;\sim \Gamma(2, 0.1) \\
\alpha_\sigma, \beta_\sigma  &amp;\sim \Gamma(2, 0.01).
\end{align}\]</span></p>
<p>Above, <span class="math inline">\(X_{gi}\)</span> denotes the height
for individual <span class="math inline">\(i\)</span> in country <span class="math inline">\(g\)</span>. The country specific parameters <span class="math inline">\(\mu_g\)</span> and <span class="math inline">\(\sigma_g\)</span> are given normal and gamma
priors, respectively, with unknown hyperparameters that, in turn, are
given hyperpriors on the last two lines.</p>
<p>Below is the Stan program for this model. The data points are input
as a concatenated vector <code>X</code>. The country-specific start and
end indices are computed in the transformed data block. This approach
accommodates uneven sample sizes between groups, although in our data
these are equal.</p>
<p>The parameters block contains the declarations of mean and standard
deviation vectors, along with the hyperparameters. The hyperparameter
subscripts denote the parameter they are assigned to so, for instance,
<span class="math inline">\(\sigma_{\mu}\)</span> is the standard
deviation of the mean parameter <span class="math inline">\(\mu\)</span>. The generated quantities block
generates samples from the population distributions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> and a country-agnostic posterior
predictive distribution <span class="math inline">\(\tilde{X}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; G; <span class="co">// number of groups</span></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N[G]; <span class="co">// sample size within each group</span></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>  <span class="dt">vector</span>[sum(N)] X; <span class="co">// concatenated observations</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>}</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  <span class="co">// get first and last index for each group in X</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>  <span class="dt">int</span> start_i[G];</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>  <span class="dt">int</span> end_i[G];</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>  </span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a>  <span class="cf">for</span>(g <span class="cf">in</span> <span class="dv">1</span>:G) {</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    <span class="cf">if</span>(g == <span class="dv">1</span>) {</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>      start_i[<span class="dv">1</span>] = <span class="dv">1</span>;</span>
<span id="cb8-16"><a href="#cb8-16" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb8-17"><a href="#cb8-17" tabindex="-1"></a>      start_i[g] = start_i[g<span class="dv">-1</span>] + N[g<span class="dv">-1</span>];</span>
<span id="cb8-18"><a href="#cb8-18" tabindex="-1"></a>    }</span>
<span id="cb8-19"><a href="#cb8-19" tabindex="-1"></a>    </span>
<span id="cb8-20"><a href="#cb8-20" tabindex="-1"></a>    end_i[g] = start_i[g] + N[g]-<span class="dv">1</span>;</span>
<span id="cb8-21"><a href="#cb8-21" tabindex="-1"></a>  }</span>
<span id="cb8-22"><a href="#cb8-22" tabindex="-1"></a>}</span>
<span id="cb8-23"><a href="#cb8-23" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb8-25"><a href="#cb8-25" tabindex="-1"></a>  </span>
<span id="cb8-26"><a href="#cb8-26" tabindex="-1"></a>  <span class="co">// parameters</span></span>
<span id="cb8-27"><a href="#cb8-27" tabindex="-1"></a>  <span class="dt">vector</span>[G] mu;</span>
<span id="cb8-28"><a href="#cb8-28" tabindex="-1"></a>  <span class="dt">vector</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt;[G] sigma;</span>
<span id="cb8-29"><a href="#cb8-29" tabindex="-1"></a>  </span>
<span id="cb8-30"><a href="#cb8-30" tabindex="-1"></a>  <span class="co">// hyperparameters</span></span>
<span id="cb8-31"><a href="#cb8-31" tabindex="-1"></a>  <span class="dt">real</span> mu_mu;</span>
<span id="cb8-32"><a href="#cb8-32" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_mu;</span>
<span id="cb8-33"><a href="#cb8-33" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha_sigma;</span>
<span id="cb8-34"><a href="#cb8-34" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; beta_sigma;</span>
<span id="cb8-35"><a href="#cb8-35" tabindex="-1"></a>}</span>
<span id="cb8-36"><a href="#cb8-36" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb8-38"><a href="#cb8-38" tabindex="-1"></a>  </span>
<span id="cb8-39"><a href="#cb8-39" tabindex="-1"></a>  <span class="co">// Likelihood for each group</span></span>
<span id="cb8-40"><a href="#cb8-40" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:G) {</span>
<span id="cb8-41"><a href="#cb8-41" tabindex="-1"></a>    X[start_i[i]:end_i[i]] ~ normal(mu[i], sigma[i]);</span>
<span id="cb8-42"><a href="#cb8-42" tabindex="-1"></a>  }</span>
<span id="cb8-43"><a href="#cb8-43" tabindex="-1"></a>  </span>
<span id="cb8-44"><a href="#cb8-44" tabindex="-1"></a>  <span class="co">// Priors</span></span>
<span id="cb8-45"><a href="#cb8-45" tabindex="-1"></a>  mu ~ normal(mu_mu, sigma_mu);</span>
<span id="cb8-46"><a href="#cb8-46" tabindex="-1"></a>  sigma ~ gamma(alpha_sigma, beta_sigma);</span>
<span id="cb8-47"><a href="#cb8-47" tabindex="-1"></a>  </span>
<span id="cb8-48"><a href="#cb8-48" tabindex="-1"></a>  <span class="co">// Hyperpriors</span></span>
<span id="cb8-49"><a href="#cb8-49" tabindex="-1"></a>  mu_mu ~ normal(<span class="dv">0</span>, <span class="dv">100</span>);</span>
<span id="cb8-50"><a href="#cb8-50" tabindex="-1"></a>  sigma_mu ~ inv_gamma(<span class="dv">2</span>, <span class="fl">0.1</span>);</span>
<span id="cb8-51"><a href="#cb8-51" tabindex="-1"></a>  alpha_sigma ~ gamma(<span class="dv">2</span>, <span class="fl">0.01</span>);</span>
<span id="cb8-52"><a href="#cb8-52" tabindex="-1"></a>  beta_sigma ~ gamma(<span class="dv">2</span>, <span class="fl">0.01</span>);</span>
<span id="cb8-53"><a href="#cb8-53" tabindex="-1"></a>}</span>
<span id="cb8-54"><a href="#cb8-54" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb8-56"><a href="#cb8-56" tabindex="-1"></a>  </span>
<span id="cb8-57"><a href="#cb8-57" tabindex="-1"></a>  <span class="dt">real</span> mu_tilda;</span>
<span id="cb8-58"><a href="#cb8-58" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma_tilda;</span>
<span id="cb8-59"><a href="#cb8-59" tabindex="-1"></a>  <span class="dt">real</span> X_tilda; </span>
<span id="cb8-60"><a href="#cb8-60" tabindex="-1"></a>  </span>
<span id="cb8-61"><a href="#cb8-61" tabindex="-1"></a>  <span class="co">// Population distributions</span></span>
<span id="cb8-62"><a href="#cb8-62" tabindex="-1"></a>  mu_tilda = normal_rng(mu_mu, sigma_mu);</span>
<span id="cb8-63"><a href="#cb8-63" tabindex="-1"></a>  sigma_tilda = gamma_rng(alpha_sigma, beta_sigma);</span>
<span id="cb8-64"><a href="#cb8-64" tabindex="-1"></a>  </span>
<span id="cb8-65"><a href="#cb8-65" tabindex="-1"></a>  <span class="co">// Posterior predictive distribution</span></span>
<span id="cb8-66"><a href="#cb8-66" tabindex="-1"></a>  X_tilda = normal_rng(mu_tilda, sigma_tilda);</span>
<span id="cb8-67"><a href="#cb8-67" tabindex="-1"></a>  </span>
<span id="cb8-68"><a href="#cb8-68" tabindex="-1"></a>} </span></code></pre>
</div>
<p>Now we can call Stan and fit the model. Hierarchical models can
encounter convergence issues and for this reason, we’ll use 10000
iterations and set <code>adapt_delta = 0.99</code>. Moreover, we’ll
speed up the inference by running 2 chains in parallel by setting
<code>cores = 2</code>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">stan_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>G <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="fu">unique</span><span class="op">(</span><span class="va">height_sim</span><span class="op">$</span><span class="va">Country</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                  N <span class="op">=</span> <span class="fu">rep</span><span class="op">(</span><span class="va">N</span>, <span class="fu">length</span><span class="op">(</span><span class="va">Countries</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                  X <span class="op">=</span> <span class="va">height_sim</span><span class="op">$</span><span class="va">Height</span><span class="op">)</span></span>
<span></span>
<span><span class="va">normal_hier_fit</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">normal_hier_model</span>,</span>
<span>                                   <span class="va">stan_data</span>, </span>
<span>                                   iter <span class="op">=</span> <span class="fl">10000</span>,</span>
<span>                                   chains <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                                   <span class="co"># Use to avoid divergent transitions:</span></span>
<span>                                   control <span class="op">=</span> <span class="fu">list</span><span class="op">(</span>adapt_delta <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span>, </span>
<span>                                   cores <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: There were 1279 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<!-- Non-pooled analysis -->
<!-- Fit -->
</div>
</section><section id="results"><h2 class="section-heading">Results<a class="anchor" aria-label="anchor" href="#results"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="country-specific-estimates">Country-specific estimates<a class="anchor" aria-label="anchor" href="#country-specific-estimates"></a>
</h3>
<p>Let’s first compare the marginal posteriors for the country-specific
estimates from the hierarchical model (blue) and an unpooled model
(brown) that treats the parameters separately.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">par_summary</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">summary</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span><span class="op">$</span><span class="va">summary</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rownames_to_column</span><span class="op">(</span>var <span class="op">=</span> <span class="st">"par"</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">separate</span><span class="op">(</span><span class="va">par</span>, into <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="st">"par"</span>, <span class="st">"country"</span><span class="op">)</span>, sep <span class="op">=</span> <span class="st">"\\["</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>country <span class="op">=</span> <span class="fu">gsub</span><span class="op">(</span><span class="st">"\\]"</span>, <span class="st">""</span>, <span class="va">country</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>country <span class="op">=</span> <span class="va">Countries</span><span class="op">[</span><span class="fu">as.integer</span><span class="op">(</span><span class="va">country</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span><span class="co"># geom_point(data = par_summary, aes(x = country, y = mean),</span></span>
<span><span class="co">#            color = posterior_color) +</span></span>
<span><span class="fu">geom_errorbar</span><span class="op">(</span>data <span class="op">=</span> <span class="va">par_summary</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">country</span>, ymin <span class="op">=</span> <span class="va">X2.5.</span>, ymax <span class="op">=</span> <span class="va">X97.5.</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span> </span>
<span><span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women10</span> <span class="op">%&gt;%</span> </span>
<span>             <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>             <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>,</span>
<span>                    value <span class="op">=</span> <span class="st">"value"</span>,</span>
<span>                    <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span><span class="op">)</span><span class="op">)</span>, </span>
<span>           <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Country</span>, y <span class="op">=</span> <span class="va">value</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span><span class="fu">geom_errorbar</span><span class="op">(</span>data <span class="op">=</span> <span class="va">unpooled_summaries</span>,</span>
<span>              <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">country</span>, ymin <span class="op">=</span> <span class="va">X2.5.</span>, ymax <span class="op">=</span> <span class="va">X97.5.</span><span class="op">)</span>,</span>
<span>              color <span class="op">=</span> <span class="st">"brown"</span><span class="op">)</span> <span class="op">+</span></span>
<span><span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span> <span class="va">par</span>, scales <span class="op">=</span> <span class="st">"free"</span>, ncol <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span><span class="fu">coord_flip</span><span class="op">(</span><span class="op">)</span> </span></code></pre>
</div>
<figure><img src="../fig/hierarchical-models-rendered-unnamed-chunk-11-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Above, the black points represent the true values, and the intervals
are the 95% CIs for a hierarchical and non-hierarchical models,
respectively. As apparent, the CIs from the hierarchical model are more
concentrated and better capture the true values.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Experiment with the data and fit. Explore the effect of sample size,
unequal sample sizes between countries, and the amount of countries, for
example.</p>
</div>
</div>
</div>
</div>
</section><section id="hyperparameters"><h2 class="section-heading">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h2>
<hr class="half-width">
<p>Let’s then plot the population distribution’s parameters, that is,
the hyperparameters. The sample-based values are included in the plots
of <span class="math inline">\(\mu_\mu\)</span> and <span class="math inline">\(\sigma_\mu\)</span> (why not for the other two
hyperparameters?). It seems that the model has slightly underestimated
the overall average and variance of the mean parameter <span class="math inline">\(\mu\)</span>, which is not suprising given the low
number of data points.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Population distributions:</span></span>
<span><span class="va">population_samples_l</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>,</span>
<span>                                       <span class="fu">c</span><span class="op">(</span><span class="st">"mu_mu"</span>, <span class="st">"sigma_mu"</span>, <span class="st">"alpha_sigma"</span>, <span class="st">"beta_sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu_mu"</span>, <span class="st">"sigma_mu"</span>, <span class="st">"alpha_sigma"</span>, <span class="st">"beta_sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"hyperpar"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">sample</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">population_samples_l</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">posterior_color</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_vline</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>               <span class="fu">filter</span><span class="op">(</span><span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">summarise</span><span class="op">(</span>mu_mu <span class="op">=</span> <span class="fu">mean</span><span class="op">(</span><span class="va">mu</span><span class="op">)</span>, sigma_mu <span class="op">=</span> <span class="fu">sd</span><span class="op">(</span><span class="va">mu</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>               <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"hyperpar"</span>, value <span class="op">=</span> <span class="st">"value"</span><span class="op">)</span>,</span>
<span>             <span class="fu">aes</span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">value</span><span class="op">)</span></span>
<span>             <span class="op">)</span><span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">hyperpar</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/hierarchical-models-rendered-unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="population-distributions"><h2 class="section-heading">Population distributions<a class="anchor" aria-label="anchor" href="#population-distributions"></a>
</h2>
<hr class="half-width">
<p>Let’s then plot the population distributions and compare to the true
sample means and standard errors.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">population_l</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu_tilda"</span>, <span class="st">"sigma_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span> <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="va">sample</span><span class="op">)</span></span>
<span></span>
<span></span>
<span> </span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">population_l</span>,</span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span>, fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>    <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">height_women</span> <span class="op">%&gt;%</span></span>
<span>                     <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>                     <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"par"</span>, value <span class="op">=</span> <span class="st">"value"</span>, <span class="op">-</span><span class="fu">c</span><span class="op">(</span><span class="va">Country</span>, <span class="va">Sex</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>                     <span class="fu">filter</span><span class="op">(</span><span class="va">Sex</span> <span class="op">==</span> <span class="st">"Girls"</span><span class="op">)</span>, </span>
<span>                   <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                   alpha <span class="op">=</span> <span class="fl">0.75</span>, bins <span class="op">=</span> <span class="fl">30</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">facet_wrap</span><span class="op">(</span><span class="op">~</span><span class="va">par</span>, scales <span class="op">=</span> <span class="st">"free"</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">labs</span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Blue = posterior; black = sample"</span><span class="op">)</span></span></code></pre>
</div>
<p><img src="../fig/hierarchical-models-rendered-unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" class="figure">
We can see that the population distribution is able to capture the
measured average heights and standard deviations relatively well,
although the within-country variation is estimated to be too
concentrated. However, remember that these estimates are based on a
limited sample: 10 out of 200 countries with 10 individuals in each
group.</p>
</section><section id="posterior-predictive-distribution"><h2 class="section-heading">Posterior predictive distribution<a class="anchor" aria-label="anchor" href="#posterior-predictive-distribution"></a>
</h2>
<hr class="half-width">
<p>Finally, let’s plot the posterior predictive distribution and overlay
it with the simulated data based on all countries.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># For each country, generate some random girl's heights</span></span>
<span><span class="va">Height_all</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">height_women</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">my_df</span> <span class="op">&lt;-</span> <span class="va">height_women</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">rename_with</span><span class="op">(</span><span class="op">~</span> <span class="fu">c</span><span class="op">(</span><span class="st">'mu'</span>, <span class="st">'sigma'</span><span class="op">)</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span>Country <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Country</span>, </span>
<span>             Sex <span class="op">=</span> <span class="va">my_df</span><span class="op">$</span><span class="va">Sex</span>, </span>
<span>             <span class="co"># Random normal values based on sample mu and sd</span></span>
<span>             Height <span class="op">=</span> <span class="fu">rnorm</span><span class="op">(</span><span class="va">N</span>, <span class="va">my_df</span><span class="op">$</span><span class="va">mu</span>, <span class="va">my_df</span><span class="op">$</span><span class="va">sigma</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Extract the posterior predictive distribution</span></span>
<span><span class="va">PPD</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_hier_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">set_colnames</span><span class="op">(</span> <span class="fu">c</span><span class="op">(</span><span class="st">"X_tilda"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">PPD</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">X_tilda</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                 fill <span class="op">=</span> <span class="va">posterior_color</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span>data <span class="op">=</span> <span class="va">Height_all</span>, </span>
<span>                 <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Height</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                 alpha <span class="op">=</span> <span class="fl">0.75</span>, </span>
<span>                 bins <span class="op">=</span> <span class="fl">100</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/hierarchical-models-rendered-unnamed-chunk-15-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="extensions"><h2 class="section-heading">Extensions<a class="anchor" aria-label="anchor" href="#extensions"></a>
</h2>
<hr class="half-width">
<p>Here, we analyzed the height for women in a randomly chosen countries
using a hierarchical model. The model could be extended further, for
instance, by adding hierarchy between sexes, continents,
developed/developing countries etc.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Hierarchical models are appropriate for scenarios where the study
population naturally divides into subgroups.</li>
<li>Hierarchical models borrow statistical strength across the
population groups.</li>
<li>Population distributions hold information about the variation of the
model parameters over the whole population.</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li>Bayesian Data Analysis (3rd ed.): Ch. 5</li>
<li>Statistical Rethinking (2nd ed.): Ch. 13</li>
<li>Bayes Rules!: Ch. 15-19</li>
</ul></section><section id="references"><h2 class="section-heading">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<hr class="half-width">
<p>[1] Height: Height and body-mass index trajectories of school-aged
children and adolescents from 1985 to 2019 in 200 countries and
territories: a pooled analysis of 2181 population-based studies with 65
million participants. Lancet 2020, 396:1511-1524</p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-model-critisism"><p>Content from <a href="model-critisism.html">Model comparison</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/model-critisism.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning in check_dep_version(): ABI version mismatch: 
lme4 was built with Matrix ABI version 1
Current Matrix ABI version is 2
Please re-install lme4 from source or restore original 'Matrix' package</code></pre>
</div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can competing models be compared?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>Get a basic understanding of</p>
<ul>
<li><p>Posterior predictive check</p></li>
<li><p>Model comparison with information criteria</p></li>
<li><p>Bayesian cross-validation</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>A common scenario in life is having data but being uncertain about
which model would be the most appropriate choice. The aim of this
chapter is to introduce some tools for systematic model comparison. We
will explore three different approaches for this purpose.</p>
<p>Firstly, we will learn how to conduct a posterior predictive check, a
method that involves comparing a fitted model’s predictions with the
observed data.</p>
<p>Next, we will examine information criteria, which measure the balance
between model complexity and goodness-of-fit.</p>
<p>Finally, we will conclude the chapter with Bayesian
cross-validation.</p>
<section id="data"><h2 class="section-heading">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h2>
<hr class="half-width">
<p>Throughout the chapter, we will use the same simulated data set in
the examples, a set of <span class="math inline">\(N=88\)</span>
univariate numerical data points.</p>
<p>Looking at a histogram, it’s evident that the data is approximately
symmetrically distributed around 0. However, there is some dispersion in
the values, and an extreme positive value, suggesting that the tails
might be longer than those of the normal distribution. The Cauchy
distribution is a potential alternative and below we will compare the
suitability of these two distributions on this data.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code> [1]  -2.270   1.941   0.502  -0.378  -0.226  -0.786  -0.209  -0.637   0.814
[10]   0.566  -1.901  -2.047  -0.689  -3.509   0.133  -4.353   1.067   0.722
[19]   0.861   0.523   0.681   2.982   0.429  -0.539  -0.512  -1.090  -8.044
[28]  -0.387  -0.007 -11.126   1.036   1.734  -0.203   1.036   0.582  -2.922
[37]  -0.543  -6.120  -0.649   4.547  -0.867   1.942   7.148  -0.044  -0.681
[46]  -3.461  -0.142   0.678   0.644  -0.039   0.354   1.783   0.369   0.175
[55]   0.980  -0.097  -4.408   0.442   0.158   0.255   0.084   0.775   2.786
[64]   0.008  -0.664  43.481   1.943   0.334  -0.118   3.901   1.736  -0.665
[73]   2.695   0.002  -1.904  -2.194  -4.015   0.329   1.140  -3.816 -14.788
[82]   0.047   6.205   1.119  -0.003   3.618   1.666 -10.845</code></pre>
</div>
<figure><img src="../fig/model-critisism-rendered-unnamed-chunk-1-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure></section><section id="posterior-predictive-check"><h2 class="section-heading">Posterior predictive check<a class="anchor" aria-label="anchor" href="#posterior-predictive-check"></a>
</h2>
<hr class="half-width">
<p>The idea of posterior predictive checking is to use the posterior
predictive distribution to simulate a replicate data set and compare it
to the observed data. The reasoning behind this approach is that if the
model is a good fit, then data generated from the model should look
similar the observed data. Any qualitative discrepancies between the
simulated and observed data can imply shortcomings in the model that do
not match the properties of the data or the domain.</p>
<p>Comparison between simulated and actual data can be done in different
ways. Visual comparison is an option but a more rigorous approach is to
compute the posterior predictive p-value (<span class="math inline">\(ppp\)</span>), which measures how well the model
can reproduce the observed data. Computing the <span class="math inline">\(ppp\)</span> requires specifying a statistic whose
value is compared between the posterior predictive and the observed
data.</p>
<p>The posterior predictive check, utilizing <span class="math inline">\(ppp\)</span>, can be formulated in the following
points:</p>
<ol style="list-style-type: decimal">
<li>Generate replicate data: Use the posterior predictive distribution
to simulate new datasets <span class="math inline">\(X^{rep}\)</span>
with characteristics matching the observed data. In our example, this
amounts to generating a replication of data with sample size <span class="math inline">\(N=88\)</span> for each posterior sample.</li>
<li>Choose test quantity <span class="math inline">\(T(X)\)</span>:
Choose an aspect of the data that you wish to check. We’ll use the
maximum value of the data as the test quantity and compute it for the
observed data and for each replication: <span class="math inline">\(X^{rep}\)</span>.</li>
<li>Compute <span class="math inline">\(ppp\)</span>: The posterior
predictive p-value is defined as the probability <span class="math inline">\(Pr(T(X^{rep}) \geq T(X) | X)\)</span>, that is the
probability that the predictions produce test quantities at least as
extreme as those found in the data. Using samples, it is computed as the
proportion of replicate data sets with <span class="math inline">\(T\)</span> not smaller than that of <span class="math inline">\(T(X)\)</span>.</li>
</ol>
<p>The smaller the <span class="math inline">\(ppp\)</span>-value, the
bigger the evidence that the model doesn’t capture the properties of the
data.</p>
<p>Next, we will perform a posterior predictive check on the example
data and compare the results for the normal and Cauchy models.</p>
<div class="section level3">
<h3 id="normal-model">Normal model<a class="anchor" aria-label="anchor" href="#normal-model"></a>
</h3>
<p>We’ll use a basic Stan program for the normal model and produce the
replicate data in the generated quantities block. Notice that
<code>X_rep</code> is a vector with length equal to the sample size
<span class="math inline">\(N\)</span>. The values of <code>X_rep</code>
are generated in a loop using the random number generator
<code>normal_rng</code>. Notice that a single posterior value of <span class="math inline">\((\mu, \sigma)\)</span> is used for each evaluation
of the generated quantities block; one posterior value is used to
generate one realization of <span class="math inline">\(X^{rep}\)</span>.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>}</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>}</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>  X ~ normal(mu, sigma);</span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>  </span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>  sigma ~ gamma(<span class="dv">2</span>, <span class="dv">1</span>);</span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a>}</span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a>  <span class="dt">vector</span>[N] X_rep;</span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a>  </span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>    X_rep[i] = normal_rng(mu, sigma);</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a>  }</span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Let’s fit model and extract the replicates.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit</span></span>
<span><span class="va">normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract </span></span>
<span><span class="va">X_rep</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_fit</span>, <span class="st">"X_rep"</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>sample <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<p>Below is a comparison of 9 realizations of <span class="math inline">\(X^{rep}\)</span> (blue) against the data (grey;
the panel titles correspond to MCMC sample numbers). It is evident that
the tail properties are different between <span class="math inline">\(X^{rep}\)</span> and <span class="math inline">\(X\)</span>, and this discrepancy indicates an
issue with the model choice.</p>
<figure><img src="../fig/model-critisism-rendered-unnamed-chunk-4-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>Let’s quantify this discrepancy by computing the <span class="math inline">\(ppp\)</span> using the maximum of the data as a
test statistic. The maximum of the original data is max(<span class="math inline">\(X\)</span>) = 43.481. The <span class="math inline">\(ppp\)</span>, or the proportion of posterior
prediction with a maximal value at least as large as this, is <span class="math inline">\(ppp =\)</span> 1.</p>
<p>This means that the chosen statistic <span class="math inline">\(T\)</span> is at least as large as in the data in
0% of the replications. This indicates strong evidence that the normal
model is a poor choice for the data.</p>
<p>The following histogram displays <span class="math inline">\(T(X) =
\max(X)\)</span> (vertical line) against the distribution of <span class="math inline">\(T(X^{rep})\)</span>.</p>
<figure><img src="../fig/model-critisism-rendered-unnamed-chunk-6-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="cauchy-model">Cauchy model<a class="anchor" aria-label="anchor" href="#cauchy-model"></a>
</h3>
<p>Let’s do an identical analysis using the Cauchy model.</p>
<p>The results are generated with code that is essentially copy-pasted
from above, with a minor distinction in the Stan program.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; N;</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="dt">vector</span>[N] X;</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>}</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>  <span class="co">// Scale</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>  <span class="co">// location</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  <span class="dt">real</span> mu;</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>}</span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>  <span class="co">// location = mu and scale = sigma</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>  X ~ cauchy(mu, sigma);</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>  </span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>  mu ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>  sigma ~ gamma(<span class="dv">2</span>, <span class="dv">1</span>);</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>}</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a>  <span class="dt">vector</span>[N] X_rep;</span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a>  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a>    X_rep[i] = cauchy_rng(mu, sigma);</span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a>  }</span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a>}</span></code></pre>
</div>
<p>A comparison of data <span class="math inline">\(X\)</span> and <span class="math inline">\(X^{rep}\)</span> from the Cauchy model reveals
little discrepancy between the posterior predictions and the data. The
distributions appear to closely match around 0, and the replicates
contain extreme values similarly to the data.</p>
<figure><img src="../fig/model-critisism-rendered-unnamed-chunk-8-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The maximum value observed in the data is similar to those from
replicate sets. Additionally, <span class="math inline">\(ppp\)</span>
is large, indicating no issues with the suitability of the model for the
data. The conclusion drawn from this posterior predictive analysis is
that the Cauchy distribution provides a better description of the data
compared to the normal distribution.</p>
<figure><img src="../fig/model-critisism-rendered-unnamed-chunk-9-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure>
</div>
</section><section id="information-criteria"><h2 class="section-heading">Information criteria<a class="anchor" aria-label="anchor" href="#information-criteria"></a>
</h2>
<hr class="half-width">
<p>Information criteria are statistics used for model comparison within
both Bayesian and classical frequentist frameworks. These criteria
provide a means to compare the relative suitability of a model to data
by estimating out-of-sample predictive accuracy while simultaneously
taking model complexity into account.</p>
<p>The Widely Applicable Information Criterion (WAIC) is an example of
an information criteria developed within the Bayesian framework. WAIC is
computed using the log pointwise predictive density (lppd) and the
available data. Since the same data is used in both tasks, lppd may be
an overly confident estimate of the predictive capability. To take this
into account, a penalization term <span class="math inline">\(p_{WAIC}\)</span> is included:</p>
<p><span class="math display">\[WAIC = -2(\text{lppd} -
p_{WAIC}).\]</span></p>
<p>The log pointwise predictive density is computed as $<em>{i=1}^N (
</em>{s=1}^S p(X_i | ^s)), $, where <span class="math inline">\(X_i,
\,i=1,\ldots,N\)</span> are data points and <span class="math inline">\(S\)</span> the number of posterior samples. The
penalization term <span class="math inline">\(p_{WAIC} = \sum_{i=1}^N
\text{Var}(\log p(y_i | \theta^s))\)</span> measures the effective
number of parameters (although this may not be apparent from the
formula). Because the definition contains a negative of the difference
<span class="math inline">\(\text{lppd} - p_{WAIC}\)</span>, lower WAIC
values imply better fit.</p>
<p>Let’s use the WAIC to compare the normal and Cauchy models. First
we’ll need to fit both models on the data using the Stan programs
presented above.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">stan_data</span> <span class="op">&lt;-</span> <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="va">N</span>, X <span class="op">=</span> <span class="va">df</span><span class="op">$</span><span class="va">X</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Fit</span></span>
<span><span class="va">normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>, <span class="va">stan_data</span>,</span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">cauchy_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>, <span class="va">stan_data</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Extract samples</span></span>
<span><span class="va">normal_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">data.frame</span></span>
<span><span class="va">cauchy_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">cauchy_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">data.frame</span></span></code></pre>
</div>
<p>Then we can write a function to compute lppd and the penalization,
and combine these into WAIC</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">WAIC</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">samples</span>, <span class="va">data</span>, <span class="va">model</span><span class="op">)</span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Loop over data points</span></span>
<span>  <span class="va">pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">length</span><span class="op">(</span><span class="va">data</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">data</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>    </span>
<span>    <span class="co"># Loop over posterior samples  </span></span>
<span>    <span class="va">point_pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op">{</span></span>
<span>      </span>
<span>      <span class="va">my_mu</span> <span class="op">&lt;-</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span></span>
<span>      <span class="va">my_sigma</span> <span class="op">&lt;-</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span></span>
<span>      </span>
<span>      <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="co"># Model: y ~ normal(mu, sigma)</span></span>
<span>        <span class="fu">dnorm</span><span class="op">(</span>x <span class="op">=</span> <span class="va">my_x</span>,</span>
<span>              mean <span class="op">=</span> <span class="va">my_mu</span>,</span>
<span>              sd <span class="op">=</span> <span class="va">my_sigma</span><span class="op">)</span></span>
<span>      <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span><span class="op">)</span> <span class="op">{</span></span>
<span>        <span class="co"># Model: y ~ Cauchy(mu, sigma)</span></span>
<span>        <span class="fu">dcauchy</span><span class="op">(</span>x <span class="op">=</span> <span class="va">my_x</span>,</span>
<span>                location <span class="op">=</span> <span class="va">my_mu</span>,</span>
<span>                scale <span class="op">=</span> <span class="va">my_sigma</span><span class="op">)</span></span>
<span>      <span class="op">}</span></span>
<span>      </span>
<span>    <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>      <span class="fu">unlist</span><span class="op">(</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="kw">return</span><span class="op">(</span><span class="va">point_pp_dens</span><span class="op">)</span></span>
<span>    </span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  </span>
<span>  <span class="co"># See BDA3 p.169</span></span>
<span>  <span class="va">lppd</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span>X <span class="op">=</span> <span class="va">pp_dens</span>,</span>
<span>                MARGIN <span class="op">=</span> <span class="fl">1</span>, </span>
<span>                FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">log</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="va">sum</span></span>
<span>  </span>
<span>  <span class="co"># See BDA3 p.173</span></span>
<span>  <span class="va">bias</span> <span class="op">&lt;-</span> <span class="fu">apply</span><span class="op">(</span>X <span class="op">=</span> <span class="va">pp_dens</span>,</span>
<span>                MARGIN <span class="op">=</span> <span class="fl">1</span>, </span>
<span>                FUN <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">var</span><span class="op">(</span><span class="fu">log</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="va">sum</span></span>
<span>  </span>
<span>  <span class="co"># WAIC</span></span>
<span>  <span class="va">waic</span> <span class="op">=</span> <span class="op">-</span><span class="fl">2</span><span class="op">*</span><span class="op">(</span><span class="va">lppd</span> <span class="op">-</span> <span class="va">bias</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">waic</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Applying this function to the posterior samples, we’ll obtain a lower
value for the Cauchy model, implying a better fit to the data. This is
in line with the posterior predictive check performed above.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">WAIC</span><span class="op">(</span><span class="va">normal_samples</span>, <span class="va">df</span><span class="op">$</span><span class="va">X</span>, model <span class="op">=</span> <span class="st">"normal"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 582.6821</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">WAIC</span><span class="op">(</span><span class="va">cauchy_samples</span>, <span class="va">df</span><span class="op">$</span><span class="va">X</span>, model <span class="op">=</span> <span class="st">"cauchy"</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 413.7336</code></pre>
</div>
</section><section id="bayesian-cross-validation"><h2 class="section-heading">Bayesian cross-validation<a class="anchor" aria-label="anchor" href="#bayesian-cross-validation"></a>
</h2>
<hr class="half-width">
<p>The final approach we take to model comparison in
cross-validation.</p>
<p>Cross-validation is a technique that estimates how well a model
predicts previously unseen data by using fits of the model to a subset
of the data to predict the rest of the data.</p>
<p>Performing cross-validation entails defining the amount of leave-out
data. The larger the proportion of the data used for model training, the
better the accuracy. However, increasing size of training data leads to
having to fit a larger number of models. In the extreme case, when each
data point is left out individually, we talk about leave-one-out
cross-validation and need <span class="math inline">\(n\)</span> data
partition, and fits.</p>
<p>In general in machine learning and statistics, the prediction made on
the test set need to be evaluated. There are different metrics. Here, we
will use log predictive density as a metric and take the sum over the
different fits as the representation of predictive accuracy. Then we
will compare this to the predictive densities computed using the model
fit with all the data. This difference represents the effective number
of parameters that can be used for comparing models.</p>
<p><span class="math display">\[p_{\text{loo-cv}} = \text{lppd} -
\text{lppd}_\text{loo-cv},\]</span> where <span class="math inline">\(\text{lppd}_\text{loo-cv}\)</span> is the sum of
the log predictive densities of individual data points evaluated based
on the model trained without the particular data point.</p>
<p>Let’s first write a helper function that computes the log predictive
density for a point, given posterior samples and the model.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Get log predictive density for a point x,</span></span>
<span><span class="co"># given data X and posterior samples</span></span>
<span><span class="co"># See BDA3 p.175</span></span>
<span><span class="va">get_lpd</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">samples</span>, <span class="va">model</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Loop over posterior samples  </span></span>
<span>  <span class="va">pp_dens</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu">nrow</span><span class="op">(</span><span class="va">samples</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">S</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># Normal(x | mu, sigma^2)</span></span>
<span>      <span class="fu">dnorm</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>,</span>
<span>            mean <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span>,</span>
<span>            sd <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="kw">if</span> <span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="co"># Cauchy(x | location = mu, scale = sigma^2)</span></span>
<span>      <span class="fu">dcauchy</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>,</span>
<span>              location <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"mu"</span><span class="op">]</span>,</span>
<span>              scale <span class="op">=</span> <span class="va">samples</span><span class="op">[</span><span class="va">S</span>, <span class="st">"sigma"</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    </span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>    <span class="fu">unlist</span><span class="op">(</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">lpd</span> <span class="op">&lt;-</span> <span class="fu">log</span><span class="op">(</span><span class="fu">mean</span><span class="op">(</span><span class="va">pp_dens</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpd</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre>
</div>
<p>Now we can perform cross-validation for the normal and Cauchy
models:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Loop over data partitions</span></span>
<span><span class="va">normal_loo_lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Training set</span></span>
<span>  <span class="va">my_X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Test set</span></span>
<span>  <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">my_X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">my_X</span><span class="op">)</span>,</span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span> <span class="co"># omit output</span></span>
<span>                            <span class="op">)</span> </span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get lpd</span></span>
<span>  <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">my_x</span>, <span class="va">my_samples</span>, <span class="st">"normal"</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"normal_loo"</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Predictive density for data points using full data in training</span></span>
<span><span class="va">normal_full_lpd</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">dummy</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">normal_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">X</span><span class="op">)</span>, </span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Compute lpds</span></span>
<span>  <span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">my_samples</span>, <span class="st">"normal"</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"normal"</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpds</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="co"># Same for Cauchy:</span></span>
<span><span class="va">cauchy_loo_lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Subset data</span></span>
<span>  <span class="va">my_X</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="op">-</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">my_x</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_normal_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>,</span>
<span>                            <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">my_X</span><span class="op">)</span>,</span>
<span>                                 X <span class="op">=</span> <span class="va">my_X</span><span class="op">)</span>, </span>
<span>                            refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_normal_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get lpd</span></span>
<span>  <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">my_x</span>, <span class="va">my_samples</span>, <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"cauchy_loo"</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span></span>
<span><span class="va">cauchy_full_lpd</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span>, <span class="kw">function</span><span class="op">(</span><span class="va">dummy</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># Fit model</span></span>
<span>  <span class="va">my_cachy_fit</span> <span class="op">&lt;-</span> <span class="fu">sampling</span><span class="op">(</span><span class="va">cauchy_model</span>,</span>
<span>                           <span class="fu">list</span><span class="op">(</span>N <span class="op">=</span> <span class="fu">length</span><span class="op">(</span><span class="va">X</span><span class="op">)</span>,</span>
<span>                                X <span class="op">=</span> <span class="va">X</span><span class="op">)</span>, </span>
<span>                           refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Get data</span></span>
<span>  <span class="va">my_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">my_cachy_fit</span>, <span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">do.call</span><span class="op">(</span><span class="va">cbind</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">set_colnames</span><span class="op">(</span><span class="fu">c</span><span class="op">(</span><span class="st">"mu"</span>, <span class="st">"sigma"</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># Compute lpds</span></span>
<span>  <span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">lapply</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">N</span>, <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">my_lpd</span> <span class="op">&lt;-</span> <span class="fu">get_lpd</span><span class="op">(</span><span class="va">X</span><span class="op">[</span><span class="va">i</span><span class="op">]</span>, <span class="va">my_samples</span>, <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="fu">data.frame</span><span class="op">(</span><span class="va">i</span>, lpd <span class="op">=</span> <span class="va">my_lpd</span>, model <span class="op">=</span> <span class="st">"cauchy"</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">lpds</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">do.call</span><span class="op">(</span><span class="va">rbind</span>, <span class="va">.</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s combine the computed log densities, and compute model-wise
sums</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Combine</span></span>
<span><span class="va">lpds</span> <span class="op">&lt;-</span> <span class="fu">rbind</span><span class="op">(</span><span class="va">normal_loo_lpds</span>, </span>
<span>              <span class="va">normal_full_lpd</span>, </span>
<span>              <span class="va">cauchy_loo_lpds</span>,</span>
<span>              <span class="va">cauchy_full_lpd</span><span class="op">)</span></span>
<span></span>
<span><span class="va">lpd_summary</span> <span class="op">&lt;-</span> <span class="va">lpds</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>lppd <span class="op">=</span> <span class="fu">sum</span><span class="op">(</span><span class="va">lpd</span><span class="op">)</span><span class="op">)</span></span></code></pre>
</div>
<p>Finally, we can compute the estimated of the effective number of
parameters. As with WAIC, smaller values imply better suitability. In
line with the posterior predictive check and WAIC, we see that, again,
the Cauchy distribution gives a better description of the data that the
normal model.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Effective number of parameters</span></span>
<span><span class="va">p_loo_cv_normal</span> <span class="op">&lt;-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal"</span>, <span class="st">"lppd"</span><span class="op">]</span> <span class="op">-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"normal_loo"</span>, <span class="st">"lppd"</span><span class="op">]</span></span>
<span><span class="va">p_loo_cv_cauchy</span> <span class="op">&lt;-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy"</span>, <span class="st">"lppd"</span><span class="op">]</span> <span class="op">-</span> <span class="va">lpd_summary</span><span class="op">[</span><span class="va">lpd_summary</span><span class="op">$</span><span class="va">model</span> <span class="op">==</span> <span class="st">"cauchy_loo"</span>, <span class="st">"lppd"</span><span class="op">]</span></span>
<span></span>
<span></span>
<span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Effective number of parameters, normal = "</span>, <span class="va">p_loo_cv_normal</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Effective number of parameters, normal = 33.7059542565819"</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">paste0</span><span class="op">(</span><span class="st">"Effective number of parameters, cauchy = "</span>, <span class="va">p_loo_cv_cauchy</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] "Effective number of parameters, cauchy = 1.903135942788"</code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div>
</section><section id="reading"><h2 class="section-heading">Reading<a class="anchor" aria-label="anchor" href="#reading"></a>
</h2>
<hr class="half-width">
<ul>
<li><p>Statistical Rethinking: Ch. 7</p></li>
<li><p>BDA3: p.143: 6.3 Posterior predictive checking</p></li>
<li><p>PSIS-loo</p></li>
<li><p><a href="https://mc-stan.org/loo/articles/online-only/faq.html" class="external-link uri">https://mc-stan.org/loo/articles/online-only/faq.html</a></p></li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 --></section></section><section id="aio-gaussian-processes"><p>Content from <a href="gaussian-processes.html">Gaussian processes</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/gaussian-processes.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 61 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How to do probabilistic non-parameteric regression?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn to perform Gaussian process regression with Stan</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Gaussian processes (GPs) are a class of stochastic (random)
processes. They are widely used for non-parametric regression, that is,
when the functional form of the predictor is not predetermined but
learned from the data. Formally, a Gaussian process <span class="math inline">\(GP(\mu, K)\)</span> is defined as a collection of
random variables <span class="math inline">\(X\)</span> with the
property that any finite subset <span class="math inline">\(X_I \subset
X\)</span> follows a multivariate normal distribution with mean <span class="math inline">\(\mu\)</span> and covariance <span class="math inline">\(K\)</span>.</p>
<p>This definition implies a distribution over functions, meaning that
generating a sample from a GP produces a function. This in turn implies
that GPs can be used as priors for unknown functional forms between
variables.</p>
<p>As an example, consider modeling crop yields as a function of
fertilizer use. Presumably, there exists a non-linear trend between
these variables, as insufficient or excessive fertilizer use will lead
to suboptimal yields. In the absence of a parametric model, GPs can
function as a prior for the relationship <span class="math inline">\(f\)</span> between fertilizer and yield. In its
simplest form, measured yields <span class="math inline">\(y\)</span>
could be modeled as noisy observations from <span class="math inline">\(f(x)\)</span>, where <span class="math inline">\(x\)</span> is the amount of fertilizer used:</p>
<p><span class="math display">\[\begin{align}
y &amp;\sim N(f(x), \sigma^2) \\
f(x) &amp;\sim GP(\mu, K).
\end{align} \]</span></p>
<p>As with all priors, the chosen hyperparameters (here <span class="math inline">\(\mu, \, K\)</span>) influence the inference. The
mean parameter <span class="math inline">\(\mu\)</span> defines the
average level of the process, while the covariance function <span class="math inline">\(K\)</span> exerts a more defining effect on the
process characteristics.</p>
<p>Perhaps the most frequently used covariance function is the squared
exponential kernel <span class="math inline">\(K_{SE}(x, x’) = \alpha^2
\exp{ \frac{(x - x’)^2}{2 \lambda} }\)</span>. The parameter <span class="math inline">\(\alpha^2\)</span> sets the variance of the
process, and <span class="math inline">\(\lambda\)</span> determines the
scale of the correlation; a large <span class="math inline">\(\lambda\)</span> implies large correlation between
<span class="math inline">\(x\)</span> and <span class="math inline">\(x’\)</span>. In the figure below, we’ve plotted
some realizations from a GP with <span class="math inline">\(\mu = (0,
0, \ldots, 0)\)</span> and squared exponential covariance function with
<span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\lambda = 25\)</span>. The input space <span class="math inline">\(X\)</span> is the integers between 0 and 100.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sq_exp_cov</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">lambda</span>, <span class="va">alpha</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="va">K</span> <span class="op">&lt;-</span> <span class="fu">matrix</span><span class="op">(</span><span class="fl">0</span>, <span class="va">n</span>, <span class="va">n</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">diff</span> <span class="op">&lt;-</span> <span class="fu">sqrt</span><span class="op">(</span><span class="fu">sum</span><span class="op">(</span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">-</span> <span class="va">x</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>      <span class="va">K</span><span class="op">[</span><span class="va">i</span>,<span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">alpha</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="fu">exp</span><span class="op">(</span><span class="op">-</span><span class="va">diff</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">lambda</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="kw">return</span><span class="op">(</span><span class="va">K</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="fl">100</span></span>
<span><span class="va">alpha</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">lambda</span> <span class="op">&lt;-</span> <span class="fl">25</span></span>
<span></span>
<span><span class="co"># Multivariate normal</span></span>
<span><span class="va">gp_samples</span> <span class="op">&lt;-</span> <span class="fu">mvtnorm</span><span class="fu">::</span><span class="fu">rmvnorm</span><span class="op">(</span><span class="fl">10</span>, sigma <span class="op">=</span> <span class="fu">sq_exp_cov</span><span class="op">(</span><span class="va">x</span>, <span class="va">lambda</span>, <span class="va">alpha</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">gp_samples_l</span> <span class="op">&lt;-</span> <span class="va">gp_samples</span> <span class="op">%&gt;%</span></span>
<span>  <span class="va">t</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">data.frame</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">seq</span><span class="op">(</span><span class="fl">0</span>, <span class="fl">100</span>, by <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"sample"</span>, value <span class="op">=</span> <span class="st">"y"</span>, <span class="op">-</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="va">gp_sample_p</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span>data <span class="op">=</span> <span class="va">gp_samples_l</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, group <span class="op">=</span> <span class="va">sample</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">gp_sample_p</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/gaussian-processes-rendered-unnamed-chunk-2-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Generate samples from the GP above with different values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> to get intuition about the role
of these hyperparameters.</p>
<p>Implement the exponential covariance kernel defined as <span class="math inline">\(K_{SE}(x, x’) = \alpha^2 \exp{ \frac{|x -
x’|}{\lambda} }\)</span> and generate samples using this kernel. What is
the qualitative difference to samples generated with squared exponential
covariance?</p>
</div>
</div>
</div>
<p>Next, we’ll explore some examples that make use of Gaussian
processes.</p>
<section id="gaussian-process-regression"><h2 class="section-heading">Gaussian process regression<a class="anchor" aria-label="anchor" href="#gaussian-process-regression"></a>
</h2>
<hr class="half-width">
<p>Assume we’d like to estimate the relationship between variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> based on the following 5 data
points.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">data.frame</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">2.76</span>, <span class="fl">2.46</span>, <span class="op">-</span><span class="fl">1.52</span>, <span class="op">-</span><span class="fl">4.34</span>, <span class="fl">4.54</span>,  <span class="fl">1</span><span class="op">)</span>,</span>
<span>                 y <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="op">-</span><span class="fl">0.81</span>, <span class="op">-</span><span class="fl">0.85</span>, <span class="fl">0.76</span>, <span class="op">-</span><span class="fl">0.41</span>, <span class="op">-</span><span class="fl">1.48</span>,  <span class="fl">0.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot </span></span>
<span><span class="va">p_data</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span><span class="va">x</span>,<span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_point</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_data</span></span></code></pre>
</div>
<figure><img src="../fig/gaussian-processes-rendered-unnamed-chunk-3-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>As explained above, we’ll assume <span class="math inline">\(y\)</span> are noisy observations from some
unknown function <span class="math inline">\(f(x)\)</span> for which
we’ll give a GP prior. Because we will not recover any functional (such
as polynomial) form for <span class="math inline">\(f\)</span>, we will
only learn the value of <span class="math inline">\(f\)</span> at
separate predetermined locations <span class="math inline">\(x\)</span>.
The covariance function needs to be computed in all those points, where
the value <span class="math inline">\(f(x)\)</span> is of interest.
Let’s predict the <span class="math inline">\(f\)</span> on a grid of
points spanning the interval (-5, 5), stored in vector
<code>x_pred</code>:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x_pred</span> <span class="op">&lt;-</span> <span class="fu">seq</span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span>, by <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">N_pred</span> <span class="op">&lt;-</span> <span class="fu">length</span><span class="op">(</span><span class="va">x_pred</span><span class="op">)</span></span></code></pre>
</div>
<p>Next we’ll build the Stan program. The model structure is simple; the
model block defines the likelihood as the normal distribution with mean
<span class="math inline">\(f(x)\)</span>:
<code>y ~ normal(f[1:N_data], sigma);</code>. Notice that this is a
vectorized statement so the mean of each <span class="math inline">\(y_i\)</span> equals <span class="math inline">\(f(x_i)\)</span>.</p>
<p>The parameter vector <code>f</code> contains the values of <span class="math inline">\(f\)</span> corresponding to the data points, in
addition to the locations where we want interpolate. The covariance
function is computed in the transformed data block, where first a vector
of concatenated data and prediction locations is build. For
computational stability, it is customary to add a small value on the
diagonal of the covariance matrix. This ensures that the matrix is
positive semi-definite.</p>
<p>Take a moment to digest the structure of the Stan program.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="co">// Data</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_data;</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>  <span class="dt">real</span> y[N_data];</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>  <span class="dt">real</span> x_data[N_data];</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>  </span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>  <span class="co">// GP hyperparameters</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; lambda;</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>  </span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>  <span class="co">// Observation error</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>  </span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>  <span class="co">// Prediction points</span></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_pred;</span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a>  <span class="dt">real</span> x_pred[N_pred];</span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a>}</span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a>  <span class="co">// Number of data and prediction points</span></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N_data + N_pred;</span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a>  </span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a>  <span class="dt">real</span> x[N];</span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a>  </span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a>  x[<span class="dv">1</span>:N_data] = x_data;</span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a>  x[(N_data+<span class="dv">1</span>):N] = x_pred;</span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>  <span class="co">// Covariance function</span></span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a>  K = gp_exp_quad_cov(x, alpha, lambda);</span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>  <span class="co">// Add nugget on diagonal for numerical stability</span></span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a>    K[n, n] = K[n, n] + <span class="fl">1e-6</span>;</span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>  }</span>
<span id="cb4-35"><a href="#cb4-35" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" tabindex="-1"></a>}</span>
<span id="cb4-37"><a href="#cb4-37" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb4-38"><a href="#cb4-38" tabindex="-1"></a>  <span class="dt">vector</span>[N] f;</span>
<span id="cb4-39"><a href="#cb4-39" tabindex="-1"></a>}</span>
<span id="cb4-40"><a href="#cb4-40" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb4-41"><a href="#cb4-41" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb4-42"><a href="#cb4-42" tabindex="-1"></a>  y ~ normal(f[<span class="dv">1</span>:N_data], sigma);</span>
<span id="cb4-43"><a href="#cb4-43" tabindex="-1"></a>  <span class="co">// GP prior</span></span>
<span id="cb4-44"><a href="#cb4-44" tabindex="-1"></a>  f ~ multi_normal(rep_vector(<span class="dv">0</span>, N), K);</span>
<span id="cb4-45"><a href="#cb4-45" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Let’s fit the model. We’ll use hyperparameter values <span class="math inline">\(\lambda = 1\)</span>, <span class="math inline">\(\alpha = 1\)</span> and set the observation error
standard deviation to <span class="math inline">\(\sigma =
0.1\)</span>.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Fit</span></span>
<span><span class="va">gp_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">gp_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N_data <span class="op">=</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>,</span>
<span>                            x_data <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>,</span>
<span>                            y <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                            lambda <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            alpha <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            sigma <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                            N_pred <span class="op">=</span> <span class="va">N_pred</span>,</span>
<span>                            x_pred <span class="op">=</span> <span class="va">x_pred</span><span class="op">)</span>,</span>
<span>                       chains <span class="op">=</span> <span class="fl">1</span>, iter <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 8.9e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)
Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)
Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)
Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)
Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)
Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)
Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)
Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)
Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)
Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)
Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)
Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 31.364 seconds (Warm-up)
Chain 1:                35.946 seconds (Sampling)
Chain 1:                67.31 seconds (Total)
Chain 1: </code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: There were 488 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: The largest R-hat is 1.45, indicating chains have not mixed.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#r-hat</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">WARNING<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="warning" tabindex="0"><code>Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
</div>
<p>The inference takes some time (minutes on a standard laptop) even
though we only use (an insufficient) single chain and 1000 iterations.
There are also some convergence issues. Let’s ignore these at this
point, and look at the output.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">gp_samples</span>, <span class="st">"f"</span><span class="op">)</span><span class="op">[[</span><span class="st">"f"</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">t</span> <span class="op">%&gt;%</span> <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span>, <span class="va">x_pred</span><span class="op">)</span><span class="op">)</span> <span class="co"># data and prediction locations</span></span>
<span></span>
<span><span class="va">f_samples_l</span> <span class="op">&lt;-</span> <span class="va">f_samples</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"sample"</span>, value <span class="op">=</span> <span class="st">"f"</span>, <span class="op">-</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_f</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">f_samples_l</span>,</span>
<span>    <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">f</span>, group <span class="op">=</span> <span class="va">sample</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">df</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span><span class="st">"red"</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_f</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/gaussian-processes-rendered-unnamed-chunk-7-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><p>The figure contains the data points in red and samples from the
posterior distribution of <span class="math inline">\(f\)</span> in
black. Note that each posterior sample corresponds to a function. This
distribution essentially captures the model’s interpretation of the
underlying trend within the data. The estimate for the trend seems
plausible. However, in certain regions, the posterior seems a bit
strange. For example, the posterior between the farthest two data points
on the right contains most of the mass above the line connecting the
points, and hardly any below it. This is likely a symptom of the
convergence issues.</p>
<div id="discussion2" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion2"></a>
</h3>
<div class="callout-content">
<p>In the figure above, where is the posterior uncertainty the highest
and why? What controls the uncertainty at the locations of the data? If
we made the prediction range wider, say, from -10 to 10, what would the
posterior look like at the extremes? ::::::::::::::::::::::: solution
Uncertainty grows at locations away from the data points and starts to
resemble the prior. At the data locations, the uncertainty is controlled
by the parameter <span class="math inline">\(\sigma\)</span>. Far from
the data, the posterior would be centered around 0 and have variance
<span class="math inline">\(\alpha^2\)</span>.</p>
</div>
</div>
</div>
<p>::::::::::::::::::::::::::::::::::::::::</p>
</section><section id="cholesky-parameterization"><h2 class="section-heading">Cholesky parameterization<a class="anchor" aria-label="anchor" href="#cholesky-parameterization"></a>
</h2>
<hr class="half-width">
<p>Running the previous example look a few minutes even though the
amount data and number of prediction locations was modest. Scalability
is a weak point of Gaussian processes but luckily there is a trick that
can be used to speed up the inference and to improve convergence.</p>
<p>The idea is to use the Cholesky decomposition of the covariance
function <span class="math inline">\(K = LL^T\)</span> to reparameterize
the target function <span class="math inline">\(f\)</span> as <span class="math inline">\(f = \mu + L\eta\)</span>. Now, if the random
variable <span class="math inline">\(\eta\)</span> is distributed as
multivariate normal with mean 0 and identity covariance matrix, it
implies that <span class="math inline">\(f \sim GP(\mu, K)\)</span>.</p>
<p>The Stan program below implements this parameterization. The Cholesky
decomposition is performed at the end of the transformed data block and
the reparameterization in the transformed parameters block. The
likelihood statement is unchanged but prior is now given to <span class="math inline">\(\eta\)</span>. Other parts of the program are
identical to the previous example.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">STAN<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode stan" tabindex="0"><code class="sourceCode stan"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>  <span class="co">// Data</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_data;</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>  <span class="dt">real</span> y[N_data];</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>  <span class="dt">real</span> x_data[N_data];</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>  </span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>  <span class="co">// GP hyperparameters</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; alpha;</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; lambda;</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>  </span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; sigma;</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>  </span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>  <span class="co">// Prediction points</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N_pred;</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>  <span class="dt">real</span> x_pred[N_pred];</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>}</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a><span class="kw">transformed data</span> {</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N = N_data + N_pred;</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>  </span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>  <span class="dt">real</span> x[N];</span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] K;</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a>  <span class="dt">matrix</span>[N, N] L;</span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a>  </span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a>  x[<span class="dv">1</span>:N_data] = x_data;</span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>  x[(N_data+<span class="dv">1</span>):N] = x_pred;</span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a>  <span class="co">// Covariance function</span></span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a>  K = gp_exp_quad_cov(x, alpha, lambda);</span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a>  </span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a>  <span class="co">// Add nugget on diagonal for numerical stability</span></span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb13-32"><a href="#cb13-32" tabindex="-1"></a>    K[n, n] = K[n, n] + <span class="fl">1e-6</span>;</span>
<span id="cb13-33"><a href="#cb13-33" tabindex="-1"></a>  }</span>
<span id="cb13-34"><a href="#cb13-34" tabindex="-1"></a></span>
<span id="cb13-35"><a href="#cb13-35" tabindex="-1"></a>  L = cholesky_decompose(K);</span>
<span id="cb13-36"><a href="#cb13-36" tabindex="-1"></a>}</span>
<span id="cb13-37"><a href="#cb13-37" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb13-38"><a href="#cb13-38" tabindex="-1"></a>  <span class="dt">vector</span>[N] eta;</span>
<span id="cb13-39"><a href="#cb13-39" tabindex="-1"></a>}</span>
<span id="cb13-40"><a href="#cb13-40" tabindex="-1"></a><span class="kw">transformed parameters</span> {</span>
<span id="cb13-41"><a href="#cb13-41" tabindex="-1"></a>  <span class="co">// mu = (0, 0, ..., 0)</span></span>
<span id="cb13-42"><a href="#cb13-42" tabindex="-1"></a>  <span class="dt">vector</span>[N] f = L*eta;</span>
<span id="cb13-43"><a href="#cb13-43" tabindex="-1"></a>}</span>
<span id="cb13-44"><a href="#cb13-44" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb13-45"><a href="#cb13-45" tabindex="-1"></a>  <span class="co">// Likelihood</span></span>
<span id="cb13-46"><a href="#cb13-46" tabindex="-1"></a>  y ~ normal(f[<span class="dv">1</span>:N_data], sigma);</span>
<span id="cb13-47"><a href="#cb13-47" tabindex="-1"></a>  <span class="co">// GP</span></span>
<span id="cb13-48"><a href="#cb13-48" tabindex="-1"></a>  eta ~ normal(<span class="dv">0</span>, <span class="dv">1</span>);</span>
<span id="cb13-49"><a href="#cb13-49" tabindex="-1"></a>}</span></code></pre>
</div>
<p>Let’s compile and fit this model using the same data. Fitting is
completed in a few seconds with no convergence issues:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">gp_cholesky_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">sampling</span><span class="op">(</span><span class="va">gp_cholesky_model</span>,</span>
<span>                       <span class="fu">list</span><span class="op">(</span>N_data <span class="op">=</span> <span class="fu">nrow</span><span class="op">(</span><span class="va">df</span><span class="op">)</span>,</span>
<span>                            x_data <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>,</span>
<span>                            y <span class="op">=</span> <span class="fu">as.array</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">y</span><span class="op">)</span>,</span>
<span>                            lambda <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            alpha <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            sigma <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>                            N_pred <span class="op">=</span> <span class="va">N_pred</span>,</span>
<span>                            x_pred <span class="op">=</span> <span class="va">x_pred</span><span class="op">)</span>,</span>
<span>                       chains <span class="op">=</span> <span class="fl">1</span>, iter <span class="op">=</span> <span class="fl">2000</span>, </span>
<span>                       refresh <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<p>Let’s examine the results. How is the posterior different from the
one recovered without the Cholesky parameterization?</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">f_cholesky_samples</span> <span class="op">&lt;-</span> <span class="fu">rstan</span><span class="fu">::</span><span class="fu">extract</span><span class="op">(</span><span class="va">gp_cholesky_samples</span>, <span class="st">"f"</span><span class="op">)</span><span class="op">[[</span><span class="st">"f"</span><span class="op">]</span><span class="op">]</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">t</span> <span class="op">%&gt;%</span> <span class="fu">data.frame</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>x <span class="op">=</span> <span class="fu">c</span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">x</span>, <span class="va">x_pred</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">f_cholesky_samples_l</span> <span class="op">&lt;-</span> <span class="va">f_cholesky_samples</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">gather</span><span class="op">(</span>key <span class="op">=</span> <span class="st">"sample"</span>, value <span class="op">=</span> <span class="st">"f"</span>, <span class="op">-</span><span class="va">x</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p_cholesky_f</span> <span class="op">&lt;-</span> <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_line</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">f_cholesky_samples_l</span>,</span>
<span>    <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">f</span>, group <span class="op">=</span> <span class="va">sample</span><span class="op">)</span>,</span>
<span>    alpha <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">df</span>, </span>
<span>             <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, color <span class="op">=</span><span class="st">"red"</span><span class="op">)</span> </span>
<span></span>
<span><span class="fu">print</span><span class="op">(</span><span class="va">p_cholesky_f</span><span class="op">)</span></span></code></pre>
</div>
<figure><img src="../fig/gaussian-processes-rendered-unnamed-chunk-10-1.png" style="display: block; margin: auto;" class="figure mx-auto d-block"></figure><div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>For the previous example, compute the posterior probability for <span class="math inline">\(f(0) &gt; 0\)</span>.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">R<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Marginal posterior at x == 0</span></span>
<span><span class="va">posterior_at_0</span> <span class="op">&lt;-</span> <span class="va">f_cholesky_samples_l</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">filter</span><span class="op">(</span><span class="va">x</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">mean</span><span class="op">(</span><span class="va">posterior_at_0</span> <span class="op">&gt;</span> <span class="fl">0</span><span class="op">)</span></span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[1] 0.609</code></pre>
</div>
</div>
</div>
</div>
</div>
<!-- ## Logistic Gaussian process regression -->
<!-- Gaussian processes can also be used as priors in models where the relationship between the explanatory and response variables is more complex.  -->
<!-- Consider, for example, predicting predicting the presence of some insect species in different regions based on average annual temperature. Now the response variable $y$ is binary with $y=0$ and $y=1$ corresponding to absence and presence, respectfully. -->
<!-- ```{r, include = FALSE} -->
<!-- N <- 50 -->
<!-- alpha <- -5 -->
<!-- beta <- 1.5 -->
<!-- x <- runif(N, 0, 25) %>% round(2) -->
<!-- theta <- 0.75*exp(-0.06*(x- 15)^2) -->
<!-- y <- rbinom(N, 1, theta) -->
<!-- df <- data.frame(x, y) -->
<!-- ``` -->
<!-- The following simulated data contains observations from 40 locations including presence/absence observations for a species and yearly average temperature. Plotting the data suggests there is an optimal range where the species can exists. -->
<!-- ```{r} -->
<!-- df <- data.frame( -->
<!--   x = c(1.74, 13.46, 3.69, 16.09, 8.52, 11.11, 19.32, 5.79, 11.44, 2.32, 0.67, 23.29, 14.1, 16.96, 16.29, 20.16, 12.68, 3.61, 14.22, 11.1, 8.02, 13.35, 24.48, 4.04, 21.41, 6.64, 1.36, 8.97, 17.87, 3.51, 15.68, 8.12, 1.38, 13.39, 3.01, 13.84, 5.29, 20.13, 5.57, 24.51, 3.94, 17.53, 10.62, 3.26, 19.78, 21.93, 21.47, 18.3, 15.91, 8.51),  -->
<!--   y = c(0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0) -->
<!-- ) -->
<!-- p <- df %>% -->
<!--   ggplot() + -->
<!--   geom_point(aes(x, y)) -->
<!-- print(p) -->
<!-- ``` -->
<!-- One way of modeling presence/absence data is with logistic regression: -->
<!-- $$ y \sim \text{Bernoulli}(\theta) \\  -->
<!-- \theta = \frac{1}{1 + e^{-(\alpha + \beta x)}},$$ where $\alpha, \beta$ are real numbers and $\theta$ is the probability of $y = 1$. -->
<!-- However, in this standard form, the relationship between temperature and presence is monotonous: assuming $\beta > 0$, higher temperatures imply higher probability of presence. This is in disagreement with the data and, of course, with reality. For this reason, we will modify the model so that the term $\beta x$ is replaced with a non-parametric function $f(x)$. We'll give $f$ a GP prior and use a normal prior $N(0, 10)$ for the baseline parameter $\alpha$.  -->
<!-- In the model block, we utilize the built-in Stan function `bernoulli_logit` to write the likelihood statement. In the generated quantities block, we'll generate $\theta$, the probability of presence as a function of temperature.  -->
<!-- ```{stan output.var="logistic_gp_model"} -->
<!-- data { -->
<!--   // Data -->
<!--   int<lower=1> N_data; -->
<!--   int<lower=0, upper=1> y[N_data]; -->
<!--   real x_data[N_data]; -->
<!--   // Prediction points -->
<!--   int<lower=1> N_pred; -->
<!--   real x_pred[N_pred]; -->
<!--   // GP hyperparameters -->
<!--   real<lower=0> alpha; -->
<!--   real<lower=0> lambda; -->
<!-- } -->
<!-- transformed data { -->
<!--   int<lower=1> N = N_data + N_pred; -->
<!--   real x[N]; -->
<!--   matrix[N, N] K; -->
<!--   matrix[N, N] L; -->
<!--   x[1:N_data] = x_data; -->
<!--   x[(N_data+1):N] = x_pred; -->
<!--   // Covariance function -->
<!--   K = gp_exp_quad_cov(x, alpha, lambda); -->
<!--   // Add nugget on diagonal for numerical stability -->
<!--   for (n in 1:N) { -->
<!--     K[n, n] = K[n, n] + 1e-6; -->
<!--   } -->
<!--   L = cholesky_decompose(K); -->
<!-- } -->
<!-- parameters { -->
<!--   real a; -->
<!--   vector[N] eta; -->
<!-- } -->
<!-- transformed parameters { -->
<!--   // mu = (0, 0, ..., 0) -->
<!--   vector[N] f = L*eta; -->
<!-- } -->
<!-- model { -->
<!--   // Likelihood -->
<!--   y ~ bernoulli_logit(a + f[1:N_data]); -->
<!--   // Priors -->
<!--   a ~ normal(0, 10); -->
<!--   eta ~ normal(0, 1); -->
<!-- } -->
<!-- generated quantities{ -->
<!--   vector[N] theta = 1 / (1 + exp(-(alpha + f))); -->
<!-- } -->
<!-- ``` -->
<!-- Let's fit the model and extract posterior summary of $\theta$.   -->
<!-- ```{r} -->
<!-- x_pred <- seq(min(df$x), max(df$x), length.out = 100) -->
<!-- N_pred <- length(x_pred) -->
<!-- logistic_gp_fit <- rstan::sampling(logistic_gp_model,  -->
<!--                                    list(N_data = nrow(df),  -->
<!--                                         y = df$y,  -->
<!--                                         x_data = df$x,  -->
<!--                                         alpha = 1,  -->
<!--                                         lambda = 3,  -->
<!--                                         N_pred = N_pred,  -->
<!--                                         x_pred = x_pred),  -->
<!--                                    refresh = 0) -->
<!-- theta_summary <- rstan::summary(logistic_gp_fit, "theta")$summary %>% -->
<!--   data.frame() %>% -->
<!--   select(lower_2.5 = X2.5., mean, upper_97.5 = X97.5.) %>% -->
<!--   mutate(x = c(df$x, x_pred)) -->
<!-- ``` -->
<!-- Then we'll look at the posterior of $\theta$, the probability of presence of the species and overlay it with the data. The posterior looks reasonable in the sense that the posterior of $\theta$ is higher in the temperature range where presence was observed. However, the posterior values seem too high across the temperature range and, moreover, start veering up at the ends. Why might this be? -->
<!-- ```{r} -->
<!-- p_theta <-  -->
<!--   ggplot() +  -->
<!--   geom_ribbon(data = theta_summary,  -->
<!--               aes(x = x, ymin = lower_2.5, ymax = upper_97.5),  -->
<!--               fill = posterior_color, alpha = 0.5) +  -->
<!--   geom_line(data = theta_summary,  -->
<!--             aes(x = x, y = mean), color = posterior_color) +  -->
<!--   geom_point(data = df,  -->
<!--              aes(x = x, y = y)) -->
<!-- p_theta -->
<!-- ``` -->
<!-- ::::::::::::::::::::::::::::::::::: challenge -->
<!-- Think of ways to modify the Stan program for the logistic GP regression so that the posterior behavior is more reasonable in the prediction range.  -->
<!-- ::::::::::::::::: solution -->
<!-- Let's modify the program by setting the GP mean to a negative value and treating the length scale as a parameter.  -->
<!-- ```{stan output.var="logistic_gp_model2"} -->
<!-- data { -->
<!--   // Data -->
<!--   int<lower=1> N_data; -->
<!--   int<lower=0, upper=1> y[N_data]; -->
<!--   real x_data[N_data]; -->
<!--   // Prediction points -->
<!--   int<lower=1> N_pred; -->
<!--   real x_pred[N_pred]; -->
<!--   real<lower=0> alpha; -->
<!-- } -->
<!-- transformed data { -->
<!--   int<lower=1> N = N_data + N_pred; -->
<!--   real x[N]; -->
<!--   x[1:N_data] = x_data; -->
<!--   x[(N_data+1):N] = x_pred; -->
<!-- } -->
<!-- parameters { -->
<!--   real a; -->
<!--   vector[N] eta; -->
<!--   real<lower=0> lambda; -->
<!-- } -->
<!-- transformed parameters { -->
<!--   matrix[N, N] K; -->
<!--   matrix[N, N] L; -->
<!--   vector[N] f; -->
<!--   // Covariance function -->
<!--   K = gp_exp_quad_cov(x, alpha, lambda); -->
<!--   // Add nugget on diagonal for numerical stability -->
<!--   for (n in 1:N) { -->
<!--     K[n, n] = K[n, n] + 1e-6; -->
<!--   } -->
<!--   L = cholesky_decompose(K); -->
<!--   f = rep_vector(-3, N) + L*eta; -->
<!-- } -->
<!-- model { -->
<!--   // Likelihood -->
<!--   y ~ bernoulli_logit(a + f[1:N_data]); -->
<!--   // Priors -->
<!--   a ~ normal(0, 10); -->
<!--   eta ~ normal(0, 1); -->
<!--   lambda ~ gamma(2, 1); -->
<!-- } -->
<!-- generated quantities{ -->
<!--   vector[N] theta = 1 / (1 + exp(-(alpha + f))); -->
<!-- } -->
<!-- ``` -->
<!-- Refit the model and check posterior -->
<!-- ```{r} -->
<!-- logistic_gp_fit2 <- rstan::sampling(logistic_gp_model2,  -->
<!--                                    list(N_data = nrow(df),  -->
<!--                                         y = df$y,  -->
<!--                                         x_data = df$x,  -->
<!--                                         alpha = 1,  -->
<!--                                         N_pred = N_pred,  -->
<!--                                         x_pred = x_pred),  -->
<!--                                    chains = 1) -->
<!-- theta_summary2 <- rstan::summary(logistic_gp_fit2, "theta")$summary %>% -->
<!--   data.frame() %>% -->
<!--   select(lower_2.5 = X2.5., mean, upper_97.5 = X97.5.) %>% -->
<!--   mutate(x = c(df$x, x_pred)) -->
<!-- p_theta2 <-  -->
<!--   ggplot() +  -->
<!--   geom_ribbon(data = theta_summary2,  -->
<!--               aes(x = x, ymin = lower_2.5, ymax = upper_97.5),  -->
<!--               fill = posterior_color, alpha = 0.5) +  -->
<!--   geom_line(data = theta_summary2,  -->
<!--             aes(x = x, y = mean), color = posterior_color) +  -->
<!--   geom_point(data = df,  -->
<!--              aes(x = x, y = y)) -->
<!-- p_theta2 -->
<!-- ``` -->
<!-- :::::::::::::::::::::::::: -->
<!-- ::::::::::::::::::::::::::::::::::::::::::::: -->
<div id="discussion3" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#discussion3"></a>
</h3>
<div class="callout-content">
<p>Generate a posterior for the optimal temperature.</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-other-topics"><p>Content from <a href="other-topics.html">Other topics</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/other-topics.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What next?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Learn more</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Add text</p>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge<a class="anchor" aria-label="anchor" href="#challenge1"></a>
</h3>
<div class="callout-content">
<p>You can define a challenge here</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>… and a solution here</p>
</div>
</div>
</div>
</div>
<p>More text</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Discussion poitns</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>point 1</li>
</ul>
</div>
</div>
</div></section><section id="aio-exercises"><p>Content from <a href="exercises.html">Exercises</a></p>
<hr>
<p>Last updated on 2024-06-18 |
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/episodes/exercises.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I get routine in probabilistic programming?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<p>The purpose of this Episodede is to provide material for practicing
probabilistic programming. The exercises are listed approximately based
on the Episodede they refer to.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="basics">1. Basics<a class="anchor" aria-label="anchor" href="#basics"></a>
</h1>
<div class="section level2">
<h2 id="grid-approximation-normal-model-with-unknown-mean">1.1 Grid approximation: normal model with unknown mean<a class="anchor" aria-label="anchor" href="#grid-approximation-normal-model-with-unknown-mean"></a>
</h2>
<p>Generate 1000 data points from the normal model. Use a randomly
generated mean parameter, <span class="math inline">\(\mu \sim
N(0,1)\)</span> and set the standard deviation <span class="math inline">\(\sigma=1\)</span>.</p>
<p>The grid approximation for this model was introduced in Episodede XX
but the implementation doesn’t work for the generated data. Locate the
source of error and make the necessary modifications to get the program
working.</p>
<p>Plot the posterior of <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="section level2">
<h2 id="grid-approximation-gamma-poisson">1.2 Grid approximation: Gamma-Poisson<a class="anchor" aria-label="anchor" href="#grid-approximation-gamma-poisson"></a>
</h2>
<p>The Gamma-Poisson model can be stated as:</p>
<p><span class="math display">\[y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, 1), \]</span></p>
<p>where <span class="math inline">\(y_i\)</span> are non-negative data
points, and Pois is the Poisson distribution with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span>. Implement a grid
approximation for this model.</p>
<p>Apparently (<a href="https://en.wikipedia.org/wiki/Poisson_distribution" class="external-link uri">https://en.wikipedia.org/wiki/Poisson_distribution</a>), the
number of chewing gum on a sidewalk tile is approximately Poisson
distributed.</p>
<p>Estimate the average number of gum on a Reykjavik side walk tile
(lambda), using the data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
</div>
<div class="section level2">
<h2 id="less-data-means-bigger-prior-effect">1.3 Less data means bigger prior effect<a class="anchor" aria-label="anchor" href="#less-data-means-bigger-prior-effect"></a>
</h2>
<p>Show that as the amount of available data increases, the effect of
the prior decreases.</p>
<p>Instructions: - Simulate a series of coin tosses: - Generate <span class="math inline">\(p \sim \text{Uniform}(0, 1)\)</span>. - Simulate a
sequence of 50 tosses with Pr(heads) = p.  - Fit the grid approximation
using the first 1, 5, 10, 15,…, 50 tosses. - Use a Beta prior for p. -
Compare the posteriors the prior.</p>
</div>
<div class="section level2">
<h2 id="grid-approximation-for-a-normal-model-with-unknown-mean-and-standard-deviation">1.4 Grid approximation: for a normal model with unknown mean and
standard deviation<a class="anchor" aria-label="anchor" href="#grid-approximation-for-a-normal-model-with-unknown-mean-and-standard-deviation"></a>
</h2>
<p>The following data is a collection of daily milk yield (in liters)
for dairy cows.</p>
<p><span class="math inline">\(X = {30.25, 34.98, 29.66, 20.14, 23.92,
38.61, 36.89, 34.68, 25.83, 29.93}.\)</span></p>
<p>Using the grid approximation for the normal model, estimate the
average daily yield <span class="math inline">\(\mu\)</span>.</p>
<p>Use some non-uniform priors.</p>
<p>Plot the marginal posterior for <span class="math inline">\(\mu\)</span> and compute the 90% credible interval
for the marginal.</p>
<p>What is the probability that the average daily milk yield is more
than 30 liters?</p>
</div>
<div class="section level2">
<h2 id="sampling-the-gamma">1.5 Sampling the Gamma<a class="anchor" aria-label="anchor" href="#sampling-the-gamma"></a>
</h2>
<p>Let’s model the following observations X with the exponential
likelihood, <span class="math inline">\(\text{Exp}(\lambda)\)</span>:</p>
<p><span class="math display">\[X = \{0.166, 1.08, 1.875, 0.413, 1.369,
0.463, 0.735,
       0.24, 0.774, 1.09, 0.463, 0.916, 0.225, \\
        0.889, 0.051, 0.688, 0.119, 0.078, 1.624, 0.553, 0.523,
       0.644, 0.284, 1.744, 1.468\}.\]</span></p>
<p>If we use a <span class="math inline">\(\Gamma(2, 1)\)</span> prior,
the posterior distribution can be shown to be</p>
<p><span class="math inline">\(p(\lambda | X) = \Gamma(2 + n, 1 + X_1 +
X_2 + ... + X_n),\)</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of
observations.</p>
<p>Generate 5000 samples from the posterior and compute 1. the posterior
mean and mode 2. the 50% and 95% credible intervals 3. the probabilities
<span class="math inline">\(Pr(\lambda &gt; 1), Pr( 1 &lt; \lambda &lt;
1.5), Pr(\lambda &lt; 1 \text{  or } \lambda &gt; 1.5)\)</span></p>
</div>
<div class="section level2">
<h2 id="grid-approximation-cauchy-distribution">1.6 Grid approximation: Cauchy distribution<a class="anchor" aria-label="anchor" href="#grid-approximation-cauchy-distribution"></a>
</h2>
<p>(Emulated from BDA3: p59. Ex.11)</p>
<p>Suppose y1,…,y5 are independent samples from a Cauchy distribution
with scale 1 and unknown location <span class="math inline">\(\theta\)</span>. Given the observations <span class="math inline">\((y_1, . . . , y_5) = (43, 44, 45, 46.5,
47.5)\)</span>:</p>
<ol style="list-style-type: lower-alpha">
<li>Compute the unnormalized posterior density function, <span class="math inline">\(p(\theta)p(y|\theta)\)</span>, on a grid of points
<span class="math inline">\(\theta = 0, 1/m , 2/m ,..., 100\)</span>,
for some large integer <span class="math inline">\(m\)</span>. Using the
grid approximation, compute and plot the normalized posterior density
function as a function of <span class="math inline">\(\theta\)</span>.
Assume a uniform prior for over [0, 100].</li>
<li>Generate 1000 samples of <span class="math inline">\(\theta\)</span>
from the posterior and plot a histogram of the samples.</li>
<li>Use each of the samples generated in b) to generate a new data point
<span class="math inline">\(y_6\)</span> from the likelihood function
and plot a histogram of these draws.</li>
</ol>
</div>
<div class="section level2">
<h2 id="sampling-the-normal">1.7 Sampling the Normal<a class="anchor" aria-label="anchor" href="#sampling-the-normal"></a>
</h2>
<p>The posterior for the normal model with unknown <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> can be sampled as follows
(BDA3:p.65):</p>
<ol style="list-style-type: decimal">
<li>Sample the variance from <span class="math inline">\(\sigma^2 \sim
Inv- \chi^2(n-1, \text{Var}(X))\)</span>
</li>
<li>Sample the mean from <span class="math inline">\(\mu | \sigma^2 ~
N(\bar{X}, \frac{\sigma^2}{n})\)</span>
</li>
</ol>
<p>Here <span class="math inline">\(\bar{X}\)</span> is the mean of the
data points <span class="math inline">\(X = \{X_1, X_2, ...,
X_n\}\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Generate 5000 samples from the posterior using the data</li>
</ol>
<p><span class="math inline">\(X = \{21.1, 20.8, 21.9, 20.5, 18.7, 24.1,
18.6, 15.4, 16.9, 20.8\}\)</span></p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compute the posterior for the coefficient of variation <span class="math inline">\(CV = \frac{\sigma}{\mu}\)</span>.</li>
<li>Generate samples for an unseen data point <span class="math inline">\(\tilde{X}\)</span>.</li>
<li>Compare the distribution in c) to one generated using only the MAP
estimate. Is there a discrepancy and why?</li>
</ol>
</div>
<div class="section level2">
<h2 id="hpdi">1.8 HPDI<a class="anchor" aria-label="anchor" href="#hpdi"></a>
</h2>
<p>Another approach to summarizing the posterior is to compute the
<em>shortest</em> interval that contains <span class="math inline">\(p\%\)</span> of the posterior. Such interval is
called highest posterior density interval (HPDI).</p>
<p>Write a function that returns the highest posterior density interval,
given a set of posterior samples.</p>
<p>Assume that the analytical form of a posterior is known to be
Gamma(3, 20). Generate samples from this distribution and compute the
90% HPDI. Compare it to the 90% credible interval.</p>
<p>Hint: If you sort the posterior samples in order, each set of n
consecutive samples contains <span class="math inline">\(100 \cdot
\frac{n}{N} \%\)</span> of the posterior, where <span class="math inline">\(N\)</span> is the total number of samples.</p>
<!-- ************************************************************************ -->
</div>
</div>
<div class="section level1">
<h1 id="stan">2. Stan<a class="anchor" aria-label="anchor" href="#stan"></a>
</h1>
<p>2.1 Gamma-Poisson model</p>
<p>The Gamma-Poisson model can be stated as:</p>
<p><span class="math display">\[y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, 1), \]</span></p>
<p>where <span class="math inline">\(y_i\)</span> are non-negative data
points, and Pois is the Poisson distribution with rate parameter <span class="math inline">\(\lambda &gt; 0\)</span>.</p>
<p>Implement the model in Stan. Do it so that the parameter beta is
input as part of data. Heuristically, what effect does altering beta
have on the prior shape?</p>
<p>Fit the model using the data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
<p>Compute the MAP, mean and 90% CIs and include them in a figure with a
histogram of posterior samples.</p>
<p>2.2 Dice</p>
<p>Write a Stan program that implements the following statistical
model:</p>
<p><span class="math display">\[y \sim \text{categorial}(\theta) \\
\theta \sim \text{Dir}(1, 1, ..., 1), \]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is a 6-dimensional
probability vector and Dir is the Dirichlet distribution.</p>
<p>Use the program to assess the fairness of a 6 sided dice. In other
words, estimate the probabilities of each side on a roll using the
following data.</p>
<p>$X = {3, 2, 6, 3, 6, 2, 5, 6, 5, 6, 4, 1, 4, 2, \ 5, 4, 6, 6, 5, 4,
1, 3, 3, 4, 2, 3, 4, 4, 4, 1, 1, \ 3, 4, 4, 1, 6, 4, 6, 5, 5, 2, 6, 1,
1, 4, 4, 1, 6, \ 6, 1, 6, 4, 5, 5, 3, 4, 2, 6, 6, 5, 2, 6, 1, 1, 4, \ 4,
4, 6, 3, 5, 3, 6, 5, 3, 3, 2, 3, 3, 5, 3, 3, 4, \ 6, 4, 3, 6, 6, 4, 4,
6, 5, 1, 3, 5, 1, 2, 4, 4, 1 } $</p>
<ol style="list-style-type: decimal">
<li><p>Plot the marginal posteriors for each <span class="math inline">\(\theta_i\)</span>.</p></li>
<li><p>Is the dice fair? Quantify this somehow</p></li>
</ol>
<p>Hint: You can e.g. compute a posterior probability difference of some
of the dice faces.</p>
<p>2.3 Normal model</p>
<p>Implement the Normal model in Stan.</p>
<p>In the generated quantities block: - Generate a posterior predictive
distribution - Generate posterior for <span class="math inline">\(CV =
\frac{\mu}{\sigma}.\)</span></p>
<p>Fit the model using the data <span class="math inline">\(X = \{3.7,
2.8, 4.03, 2.11, 2.58, 0.96, 1.74, 0.34, 0.75, 2.07\}\)</span>. Plot the
posterior distribution and color the points according to the condition
<span class="math inline">\(CV &lt; 1\)</span>.</p>
<p>2.4. Time series modeling</p>
<p>The AR(1) process is defined by the recursion: <span class="math display">\[x_i \sim N(\phi \cdot x_{i-1},
\sigma^2),\]</span> where <span class="math inline">\(i\)</span> is a
time index. In other words, given some initial value <span class="math inline">\(x_1\)</span>, the next value <span class="math inline">\(x_2\)</span> is generated from a normal
distribution with mean <span class="math inline">\(\phi\cdot
x_1\)</span> and variance <span class="math inline">\(sigma^2\)</span>.
This pattern continues for the successive values.</p>
<p>Write a Stan program that estimates the parameters <span class="math inline">\(\phi\)</span> and <span class="math inline">\(\sigma\)</span> of the AR(1) process.</p>
<p>Using the data in <code>data/time_series.txt</code>, do the
following:</p>
<ol style="list-style-type: decimal">
<li>Estimate the parameters <span class="math inline">\(\phi\)</span>
and <span class="math inline">\(\sigma\)</span>.</li>
<li>Starting from <span class="math inline">\(x_{new} = 5\)</span>
predict the process 50 values into the future and plot the
predictions.</li>
</ol>
</div>
<div class="section level1">
<h1 id="mcmc">3. MCMC<a class="anchor" aria-label="anchor" href="#mcmc"></a>
</h1>
<div class="section level2">
<h2 id="binomial-model">3.1 Binomial model<a class="anchor" aria-label="anchor" href="#binomial-model"></a>
</h2>
<p>Implement the Metropolis-Hastings algorithm for the beta-binomial
model.</p>
<p>Assume there are 50 people of whom 7 are left-handed. Estimate the
probability of being left-handed in the wider population.</p>
<p>Use <span class="math inline">\(p(\theta^* | \theta_{now}) =
\text{Beta}(\theta^* | 2, 2)\)</span> as your proposal distribution,
where <span class="math inline">\(\theta^*\)</span> is the proposal and
<span class="math inline">\(\theta_{now}\)</span> the current
sample.</p>
<p>Compute the proportion of accepted proposals for the sampler.</p>
</div>
<div class="section level2">
<h2 id="gibbs-sampler">3.2. Gibbs sampler<a class="anchor" aria-label="anchor" href="#gibbs-sampler"></a>
</h2>
<p>Consider the distribution <span class="math display">\[ p(x, y) = C
\cdot \exp^{-(x^2y^2 + x^2 + y^2 -8x -8y)/2}\]</span>, where <span class="math inline">\(C\)</span> is a normalizing constant. It is known
that the conditional distribution of x given y is the normal
distribution <span class="math inline">\(p(x|y) = N(\mu,
\sigma^2)\)</span>, where the mean <span class="math inline">\(\mu =
4/(1 + y^2)\)</span> and the standard deviation <span class="math inline">\(\sigma = \sqrt{1/(1 + y^2)}\)</span>. Due to
symmetry, <span class="math inline">\(p(y|x)\)</span> can be recovered
simply by changing <span class="math inline">\(y\)</span>’s to <span class="math inline">\(x\)</span>’s in <span class="math inline">\(p(x|y)\)</span>.</p>
<p>The Gibbs sampler is a special case of the Metropolis-Hastings
algorithm. It can be stated as follows:</p>
<ol style="list-style-type: decimal">
<li>Choose some initial values for the parameters, <span class="math inline">\(x_0\)</span> and <span class="math inline">\(y_0\)</span> in our case.</li>
<li>For <span class="math inline">\(i = 1, ..., N\)</span> do:
<ol style="list-style-type: lower-alpha">
<li>Draw <span class="math inline">\(x_i\)</span> from <span class="math inline">\(p(x_i|y_{i-1})\)</span>
</li>
<li>Draw <span class="math inline">\(y_i\)</span> from <span class="math inline">\(p(y_i|x_i)\)</span>
</li>
</ol>
</li>
</ol>
<p>(In this notation <span class="math inline">\(x_i\)</span> refers to
the <span class="math inline">\(x\)</span> sample at step <span class="math inline">\(i\)</span>, <span class="math inline">\(y_{i-1}\)</span> to the y sample at step <span class="math inline">\(i-1\)</span> and so on)</p>
<p>Build a Gibbs sampler that draws samples from <span class="math inline">\(p(x, y)\)</span>. Visualize the resulting
distribution.</p>
</div>
<div class="section level2">
<h2 id="gamma-with-discrete-rate">3.3. Gamma with discrete rate<a class="anchor" aria-label="anchor" href="#gamma-with-discrete-rate"></a>
</h2>
<p>The data <span class="math inline">\(X = \{10.61, 4.76, 11.25, 5.55,
23.81, 7.45, 17.31, 15.08, \\8.19, 15, 4.29, 10.95, 15.45, 10.09, 7.96,
12.35, \\ 11.43, 7.33, 8.17, 21\}\)</span></p>
<p>is modelled with a <span class="math inline">\(\Gamma(\alpha,
\beta)\)</span> distribution, where the shape <span class="math inline">\(\alpha = 5\)</span>, but the rate <span class="math inline">\(\beta\)</span> is unknown. However, it is known
that beta can only take values <span class="math inline">\(1.1^N\)</span>, where <span class="math inline">\(N\)</span> is an integer.</p>
<p>Implement a Metropolis-Hastings sampler for this model.</p>
<p>Use a proposal distribution that gives equal probability for moving
one step up <span class="math inline">\((N^* = N+1)\)</span> or down
<span class="math inline">\((N^* = N-1)\)</span>, and some positive
probability for staying still <span class="math inline">\((N^*=N)\)</span>.</p>
<p>Generate the initial value randomly: <span class="math inline">\(N_0
\sim \text{Uniform}(-100, 100)\)</span> and compute an estimate for
<span class="math inline">\(\beta\)</span>.</p>
</div>
</div>
<div class="section level1">
<h1 id="hierarchical-models">4. Hierarchical models<a class="anchor" aria-label="anchor" href="#hierarchical-models"></a>
</h1>
<div class="section level2">
<h2 id="model-analysis">4.1. Model analysis<a class="anchor" aria-label="anchor" href="#model-analysis"></a>
</h2>
<p>Examine the following statistical models. Determine if they exhibit a
hierarchical structure. If not, introduce modifications to make them
hierarchical.</p>
<p>Below, the subscript <span class="math inline">\(i\)</span> denotes a
data point, while <span class="math inline">\(g\)</span> designates a
population subgroup.</p>
<ol style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
y_{g, i} \sim N(a_g + b \cdot x_{g,i}, \sigma^2) \\
a_g \sim N(0, 1) \\
b \sim N(0, 1)  \\
\sigma \sim \text{Exponential}(1) \\
\]</span></p>
<ol start="2" style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
y_{g,i} \sim \text{Binom}(N, p_{g,i})  \\
\text{logit}(p_{g,i}) = a_g + b \cdot x_i  \\
a_{g} \sim N(\alpha, 1)   \\
\alpha \sim N(0, 10)   \\
b \sim N(0, 1)
\]</span></p>
<ol start="3" style="list-style-type: lower-alpha"><li>
</li></ol>
<p><span class="math display">\[
X_i \sim \Gamma(\alpha, \beta)   \\
\alpha \sim \Gamma(1, 1)   \\
\beta \sim \Gamma(1, 1)
\]</span></p>
<p>4.2. Hierarchical Gamma-Poisson</p>
<p>A hierarchical Gamma-Poisson model can be stated as follows:</p>
<p><span class="math display">\[
y_i \sim \text{Pois}(\lambda) \\
\lambda \sim \Gamma(1, \beta) \\
\beta \sim \Gamma(2, 1) \\
\]</span></p>
<p>Use data <span class="math inline">\(y =
\{2,7,4,3,5,2,7,5,5,5\}\)</span>.</p>
<p>Implement the model in Stan.</p>
<p>Identify the data subgroups. Visualize the posterior distribution of
beta. Generate samples from the population distribution of <span class="math inline">\(\lambda\)</span> in the generated quantities block
and plot them in a histogram.</p>
<p>4.3. Hierarchical Poisson regression</p>
<p>A Poisson regression model can be specified as <span class="math inline">\(y_i \sim \text{Pois}(\exp^{\alpha + \beta
x_i})\)</span>, where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span> are corresponding data points.</p>
<p>Apply Poisson regression to model the annual rental count <span class="math inline">\((y_i)\)</span> for homestay apartments located at
a distance <span class="math inline">\(x_i\)</span> from the city
center.</p>
<p>The data comprises 25 randomly selected apartments in 10 different
cities. The file <code>data/apartment_x.txt</code> contains the
distances of apartments to the center, with rows representing cities and
columns representing apartments. The file
<code>data/apartment_y.txt</code> contains the number of rentals during
the surveyed year for the same apartments.</p>
<p>Build a Stan program for hierarchical Poisson regression, with
hierarchical structure for both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p>Generate and visualize posteriors for the population distributions of
<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. What is the probability that <span class="math inline">\(\beta &gt; 0\)</span> in the population? What
implications does <span class="math inline">\(\beta &gt; 0\)</span> have
in terms of the application?</p>
</div>
<div class="section level2">
<h2 id="hierarchical-binomial-model">4.4. Hierarchical binomial model<a class="anchor" aria-label="anchor" href="#hierarchical-binomial-model"></a>
</h2>
<p>As commonly acknowledged, multiple humanoid species inhabit various
solar systems within the Milky Way galaxy.</p>
<p>The file, <code>data/handedness.txt</code>, contains data on the
handedness of some of these species, with <span class="math inline">\(N\)</span> representing the sample size and <span class="math inline">\(x\)</span> denoting the count of left-handed
specimens. The objective is to estimate the prevalence of
left-handedness, <span class="math inline">\(\theta\)</span>.</p>
<p>Develop Stan programs for both unpooled and partially pooled binomial
models. Utilize the unpooled model to fit the completely pooled
model.</p>
<p>Incorporate Beta priors for <span class="math inline">\(\theta\)</span>. Compare the estimates, such as
means, derived from the distinct pooling strategies.</p>
</div>
</div>
<div class="section level1">
<h1 id="model-critisism">5. Model critisism<a class="anchor" aria-label="anchor" href="#model-critisism"></a>
</h1>
<div class="section level2">
<h2 id="prior-predictive-check">5.1 Prior predictive check<a class="anchor" aria-label="anchor" href="#prior-predictive-check"></a>
</h2>
<p>In a salmon farm research facility, the relationship between the
length of salmons (in meters, <span class="math inline">\(y\)</span>)
and the amount of food provided (in grams, <span class="math inline">\(x\)</span>) is studied. The amount of food
administered in 21 salmon pools is meticulously controlled, ranging from
40 to 60 grams per individual salmon in one-gram increments.</p>
<p>The chosen statistical model is linear regression:</p>
<p><span class="math display">\[
y \sim N(a + bx, \sigma^2) \\
a, b \sim N(0, 1) \\
\sigma \sim \Gamma(2, 1) \\
\]</span></p>
<p>Generate the prior predictive distribution, plot it, and visually
assess the validity of the priors.</p>
</div>
<div class="section level2">
<h2 id="posterior-predictive-check">5.2. Posterior predictive check<a class="anchor" aria-label="anchor" href="#posterior-predictive-check"></a>
</h2>
<p>The white noise process is perhaps the simplest non-trivial time
series model. If <span class="math inline">\(i\)</span> is the time
index, then the model can be specified as <span class="math inline">\(x_i ~ N(0, sigma^2)\)</span> for all <span class="math inline">\(i = 1, \ldots ,N\)</span>.</p>
<p>The file <code>data/white_noise.txt</code> contains a time series.
Plot the data.</p>
<p>Build a Stan program for the white noise model and use it on the
provided data.</p>
<p>Do a posterior predictive check by use lag-1 autocorrelation as the
test statistic. Compute the posterior predictive p-value.</p>
<p>Hint: lag-1 autocorrelation is simply the Pearson correlation between
<span class="math inline">\(x_{1:(N-1)}\)</span> and <span class="math inline">\(x_{2:N}\)</span></p>
</div>
</div>
<div class="section level1">
<h1 id="gaussian-processes">6. Gaussian processes<a class="anchor" aria-label="anchor" href="#gaussian-processes"></a>
</h1>
<p>6.1. GP prediction</p>
<p>Consider the following data <span class="math inline">\((x,
y)\)</span>:</p>
<p><span class="math inline">\(x = \{6.86, -7.88, 1.59, 5.95, -2.55,
-1.96, 3.77, -6.74, -6.83, 8.42, -4.95, -6.29, \\ 9.58, -1.95, 7.6,
1.97, 7.75, 8.34, 9.22, -6.31, 1.73, 9.58, 6.86, 1.46, -6.7, -9.9, \\
6.81, -6.38, 3.52, -4.36, -3.46, 7.7, -7.63, 3.51, 5.57, 3.2, 2.04,
6.33, -7.84, -2.85, \\ -7.86, -5.14, -6.18, -9.35, -5.59, -6.86, 4.2,
8.83, 8.04 \}\)</span></p>
<p><span class="math inline">\(y = \{2.95, -6.7, 0.8, -1.47, -0.03,
-0.26, -1.03, -2.8, -3.01, 6.75, 1.68, -0.93, \\ -0.88, -1.03, 6.88,
-0.1, 7.32, 7.59, 3.09, -0.44, 0.69, -0.27, 3.45, 0.46, -2.49, 3.33, \\
3.28, -1.48, -0.11, 1.76, 0.25, 6.01, -6.53, 0.62, -1.01, 0.26, 1.31,
0.47, -6.7, \\ -0.29, -6.09, 2.03, -0.22, -1.24, 1.46, -3.73, -0.87,
5.27, 6.59\}\)</span></p>
<p>Model the data as <span class="math inline">\(y \sim N(f(x),
\sigma^2)\)</span> and give <span class="math inline">\(f(x)\)</span> a
Gaussian process prior. Use the squared exponential covariance function
with hyperparameters <span class="math inline">\(\lambda = 1\)</span>
(length-scale), <span class="math inline">\(\alpha = 1\)</span>
(standard deviation), <span class="math inline">\(\sigma =
0.5\)</span>.</p>
<p>You can use the Stan programs in Episodede XX as a starting
point.</p>
<p>Plot the posterior for <span class="math inline">\(f\)</span> along
with the data, and compute the posterior probability for <span class="math inline">\(f(0) &gt; 0\)</span>.</p>
<div class="section level2">
<h2 id="gp-prediction">6.2. GP prediction<a class="anchor" aria-label="anchor" href="#gp-prediction"></a>
</h2>
<p>The file <code>data/nytemp.txt</code> contains daily maximum
temperatures (<span class="math inline">\(Temp\)</span>) in New York
from May to September 1973 [1]. The column <span class="math inline">\(x\)</span> gives the day number for the date (Jan
1st is day number 1 etc).</p>
<p>Do Gaussian process regression on the data and estimate the
temperature trend for the year 1973. Use a periodic covariance kernel
with <span class="math inline">\(\alpha = 100\)</span> and <span class="math inline">\(\lambda = 10\)</span>, and set a suitable value
for the period. Set <span class="math inline">\(\sigma\)</span>
(deviance from the trend) as an unknown parameter in Stan.</p>
<p>Plot the data with the posterior for temperature. Plot the marginal
posterior for temperature for the last day of the year and compare it
against the true value (which was 38F).</p>
<p>Hint:</p>
<p>See <a href="https://mc-stan.org/docs/functions-reference/gaussian-process-covariance-functions.html" class="external-link uri">https://mc-stan.org/docs/functions-reference/gaussian-process-covariance-functions.html</a>
for info on periodic kernel in Stan.</p>
<p>[1] (Chambers, J. M., Cleveland et al. (1983) Graphical Methods for
Data Analysis)</p>
</div>
<div class="section level2">
<h2 id="gp-prediction-1">6.3. GP prediction<a class="anchor" aria-label="anchor" href="#gp-prediction-1"></a>
</h2>
<p>The data file <code>data/stock.txt</code> contains the (scaled and
transformed) stock price of a company over 250 days.</p>
<p>Implement the following model and use it to predict the stock 150
into the future.</p>
<p><span class="math display">\[
y \sim N(f, \sigma^2) \\
f = f_{trend} + f_{noise} \\
f_{trend} \sim GP(0, K_1(\alpha_1, \lambda_1)) \\
f_{noise} \sim GP(0, K_2(\alpha_2, \lambda_2)) \\
\sigma \sim \Gamma(2, 10)
\]</span></p>
<p>Set <span class="math inline">\(K1\)</span> to squared exponential
covariance function and <span class="math inline">\(K2\)</span> to
exponential covariance. The point of <span class="math inline">\(f_{trend}\)</span> is to capture the longer term
trend in the data, while <span class="math inline">\(f_{noise}\)</span>
target shorter-term variations around the trend.</p>
<p>Set the hyperparameters <span class="math inline">\(\alpha_{1},
\alpha_{2}\)</span> and <span class="math inline">\(\lambda_{1},
\lambda_{1,2}\)</span> appropriately.</p>
<p>Is such a model appropriate for predicting stock prices into the
future?</p>
<p>Hint: Running the Stan program can take quite a long time, so do the
initial testing using only 1 chain and, for example, 500 iterations.</p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/statistical-probabilistic-programming-r/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:velait@utu.fi">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/aio.html",
  "identifier": "https://carpentries-incubator.github.io/statistical-probabilistic-programming-r/instructor/aio.html",
  "dateCreated": "2024-06-18",
  "dateModified": "2024-06-18",
  "datePublished": "2024-06-18"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

